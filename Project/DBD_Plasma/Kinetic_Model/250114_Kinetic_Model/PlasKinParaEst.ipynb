{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import subprocess\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "import ast\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "step = 0.01\n",
    "max_error = 1000\n",
    "pre_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile zdplaskin\n",
    "def run_prep(inp_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            'preprocessor.exe',\n",
    "            stdout = subprocess.DEVNULL,        # ignore outputs\n",
    "            stderr = subprocess.DEVNULL,        # ignore errors\n",
    "            stdin = subprocess.PIPE,            # recognize input\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        process.stdin.write(inp_path)\n",
    "        process.stdin.flush()                   # send a data\n",
    "\n",
    "        while process.poll() is None:           # check the program state, if None, program is still in the run\n",
    "            process.stdin.write('.\\n')\n",
    "            process.stdin.flush()\n",
    "    except:\n",
    "        pass\n",
    "    print('check the run of preprocessor')\n",
    "    return process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile exe\n",
    "def compile_zdp1(name):\n",
    "    compile_command = [\n",
    "        'gfortran', '-o', name, 'dvode_f90_m.F90', 'zdplaskin_m.F90',\n",
    "        'run_plasRxn1.F90', 'bolsig_x86_64_g.dll'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(compile_command)\n",
    "    except:\n",
    "        pass\n",
    "    print('check the compiler_run1')\n",
    "\n",
    "def compile_zdp2(name):\n",
    "    compile_command = [\n",
    "        'gfortran', '-o', name, 'dvode_f90_m.F90', 'zdplaskin_m.F90',\n",
    "        'run_plasRxn2.F90', 'bolsig_x86_64_g.dll'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(compile_command)\n",
    "    except:\n",
    "        pass\n",
    "    print('check the compiler_run2')\n",
    "\n",
    "# Compile exe\n",
    "def compile_zdp3(name):\n",
    "    compile_command = [\n",
    "        'gfortran', '-o', name, 'dvode_f90_m.F90', 'zdplaskin_m.F90',\n",
    "        'run_plasRxn3.F90', 'bolsig_x86_64_g.dll'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(compile_command)\n",
    "    except:\n",
    "        pass\n",
    "    print('check the compiler_run3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exe\n",
    "def run_exe(exe_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            exe_path,\n",
    "            stdout = subprocess.PIPE, # Read standard outputs\n",
    "            stderr = subprocess.PIPE, # Read standard errors\n",
    "            universal_newlines=True,  # outputs to str variables\n",
    "            bufsize = 1               # control the size of buffer\n",
    "        )\n",
    "\n",
    "        log_flag = False             # The flag for starting log after \"Caculation Start!!\"\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if not output:\n",
    "                break\n",
    "            if \"Calculation Start\" in output:\n",
    "                log_flag = True\n",
    "\n",
    "            if log_flag:\n",
    "                print(f'\\r{output.strip()}           ',end='',flush=True)\n",
    "\n",
    "            if \"PRESS ENTER TO EXIT\" in output:\n",
    "                process.kill()        # forced shutdown\n",
    "                break\n",
    "            if \"WARNING: BOLSIG+ convergence failed\" in output:\n",
    "                process.kill()        # forced shutdown\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    return process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error calculation\n",
    "def cal_error(exp_result):\n",
    "    # Read a result\n",
    "    conditions = []\n",
    "    with open('qt_conditions_list.txt','r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            line = line[2:]\n",
    "            conditions.append(line)\n",
    "        file.close()\n",
    "\n",
    "    species = []\n",
    "    with open('qt_species_list.txt','r') as file:\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            line = line[3:]\n",
    "            species.append(line)\n",
    "        file.close()\n",
    "\n",
    "    reactions = []\n",
    "    reaction_list = pd.read_csv('parameter_set.csv')\n",
    "    reactions = reaction_list['Reaction'].to_list()\n",
    "    df_cd = pd.read_csv('qt_conditions.txt', sep=r'\\s+', header=0, names=['Time [s]']+conditions)\n",
    "    df_sp = pd.read_csv('qt_densities.txt', sep=r'\\s+', header=0, names=['Time [s]']+species)\n",
    "    df_rx = pd.read_csv('qt_rates.txt', sep=r'\\s+', header=0, names=['Time [s]']+reactions)\n",
    "    top_rate = df_rx.iloc[:,1:].sum().sort_values(ascending=False)\n",
    "\n",
    "    CH4 = (df_sp['CH4'] + df_sp['CH4(V13)'] + df_sp['CH4(V24)'])\n",
    "    C2H2 = (df_sp['C2H2'] + df_sp['C2H2(V13)']+ df_sp['C2H2(V2)']+ df_sp['C2H2(V5)'])\n",
    "    C2H4 = (df_sp['C2H4'] + df_sp['C2H4(V1)']+ df_sp['C2H4(V2)'])\n",
    "    C2H6 = (df_sp['C2H6'] + df_sp['C2H6(V13)']+ df_sp['C2H6(V24)'])\n",
    "    C3H6 = (df_sp['C3H6'] + df_sp['C3H6(V)'])\n",
    "    C3H8 = (df_sp['C3H8'] + df_sp['C3H8(V1)'] + df_sp['C3H8(V2)'])\n",
    "    C4H10 = (df_sp['C4H9H'])\n",
    "    C5H12 = (df_sp['C5H12'])\n",
    "    H2 = df_sp['H2']\n",
    "    C = df_sp['C']\n",
    "    H = df_sp['H']\n",
    "    CH = df_sp['CH']\n",
    "    CH2 = df_sp['CH2']\n",
    "    CH3 = df_sp['CH3']\n",
    "    C2H3 = df_sp['C2H3']\n",
    "    C2H5 = df_sp['C2H5']\n",
    "    C3H5 = df_sp['C3H5']\n",
    "    C3H7 = df_sp['C3H7']\n",
    "    C4H9 = df_sp['C4H9']\n",
    "\n",
    "    exp = exp_result\n",
    "    \n",
    "    sim_XCH4 = (CH4.iloc[0] - CH4.iloc[-1])/CH4.iloc[0]*100\n",
    "    sim_SH2 = 0.5*H2.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC2H6 = 2*C2H6.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC2H4 = 2*C2H4.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC2H2 = 2*C2H2.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC3H8 = 3*C3H8.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC3H6 = 3*C3H6.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC4H10 = 4*C4H10.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC5H12 = 5*C5H12.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SH = 0.25*H.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SCH = CH.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SCH2 = CH2.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SCH3 = CH3.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC2H3 = 2*C2H3.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC2H5 = 2*C2H5.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC3H5 = 3*C3H5.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC3H7 = 3*C3H7.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "    sim_SC4H9 = 4*C4H9.iloc[-1]/(CH4.iloc[0] - CH4.iloc[-1])*100\n",
    "\n",
    "    sim = []\n",
    "    sim.append(sim_XCH4)\n",
    "    sim.append(sim_SH2)\n",
    "    sim.append(sim_SC2H6)\n",
    "    sim.append(sim_SC2H4)\n",
    "    sim.append(sim_SC2H2)\n",
    "    sim.append(sim_SC3H8)\n",
    "    sim.append(sim_SC3H6)\n",
    "    sim.append(sim_SC4H10)\n",
    "    sim.append(sim_SH)\n",
    "    sim.append(sim_SCH)\n",
    "    sim.append(sim_SCH2)\n",
    "    sim.append(sim_SCH3)\n",
    "    sim.append(sim_SC2H3)\n",
    "    sim.append(sim_SC2H5)\n",
    "    sim.append(sim_SC3H5)\n",
    "    sim.append(sim_SC3H7)\n",
    "    sim.append(sim_SC4H9)\n",
    "    \n",
    "    err = 0\n",
    "    \n",
    "    for i in range(len(sim)):\n",
    "        if i < 8:\n",
    "            err += ((exp[i] - sim[i])/exp[i])**2\n",
    "        else:\n",
    "            err += (exp[i] - sim[i])**2\n",
    "\n",
    "    err =  float(np.sum((np.asarray(exp) - np.asarray(sim))**2))\n",
    "    \n",
    "    return err, top_rate, sim, df_cd['Time [s]'].iloc[-1], sim_XCH4, sim_SC2H6, sim_SC2H4, sim_SC2H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial variable sset sampling: latin_hypercube sampling\n",
    "def latin_hypercube_sampling(bounds, n_samples):\n",
    "    return np.random.uniform(\n",
    "        low = np.array([b[0] for b in bounds]),\n",
    "        high = np.array([b[1] for b in bounds]),\n",
    "        size = (n_samples, len(bounds))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update kinet.inp\n",
    "def update_kinet1(samples,index):\n",
    "    with open(f'kinet_blank.inp','r') as file:\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "    new_lines=[]\n",
    "    for i in range(316):\n",
    "        new_value = 10**samples[i]\n",
    "        new_value2 = f'{new_value:.4e}'.replace('e-0','e-').replace('e+00','e0').replace('e+0','e+').replace('e','d')\n",
    "        new_line = f'$ double precision, parameter :: f{i} = {new_value2}\\n'\n",
    "        new_lines.append(new_line)\n",
    "    new_inp = lines[:18] + new_lines + lines[334:]\n",
    "    with open(f'./kinet1_{index}.inp', 'w') as file:\n",
    "        file.writelines(new_inp)\n",
    "\n",
    "def update_kinet2(samples,index):\n",
    "    with open(f'kinet_blank.inp','r') as file:\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "    new_lines=[]\n",
    "    for i in range(316):\n",
    "        new_value = 10**samples[i]\n",
    "        new_value2 = f'{new_value:.4e}'.replace('e-0','e-').replace('e+00','e0').replace('e+0','e+').replace('e','d')\n",
    "        new_line = f'$ double precision, parameter :: f{i} = {new_value2}\\n'\n",
    "        new_lines.append(new_line)\n",
    "    new_inp = lines[:18] + new_lines + lines[334:]\n",
    "    with open(f'./kinet2_{index}.inp', 'w') as file:\n",
    "        file.writelines(new_inp)\n",
    "\n",
    "def update_kinet3(samples,index):\n",
    "    with open(f'kinet_blank.inp','r') as file:\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "    new_lines=[]\n",
    "    for i in range(316):\n",
    "        new_value = 10**samples[i]\n",
    "        new_value2 = f'{new_value:.4e}'.replace('e-0','e-').replace('e+00','e0').replace('e+0','e+').replace('e','d')\n",
    "        new_line = f'$ double precision, parameter :: f{i} = {new_value2}\\n'\n",
    "        new_lines.append(new_line)\n",
    "    new_inp = lines[:18] + new_lines + lines[334:]\n",
    "    with open(f'./kinet3_{index}.inp', 'w') as file:\n",
    "        file.writelines(new_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_Model(X,y):\n",
    "    avg_pre_Y = sum(y)/len(y)\n",
    "    X_range = X.max(axis=0) - X.min(axis=0)\n",
    "    # Gaussian Process Surrogate Model\n",
    "    constant_kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-5,1e5))\n",
    "    RBF_kernel = RBF(1.0, length_scale_bounds = (1e-5,1e5))\n",
    "    kernel = constant_kernel * RBF_kernel\n",
    "\n",
    "    gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-6)\n",
    "    gp_model.fit(X,y)\n",
    "    return gp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Improvement\n",
    "def EI(X, gp, y_best, xi):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma\n",
    "    #improvement = mu - y_best - xi\n",
    "    improvement = y_best - mu - xi\n",
    "    Z = improvement / sigma\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization for minimization\n",
    "def BO(gp, bounds, n_iter, X, y, exp_result):\n",
    "    \"\"\"Perform Bayesian Optimization using EI for minimization.\"\"\"\n",
    "    for i in range(n_iter):\n",
    "        y_best = np.min(y)  # 현재까지의 최소값\n",
    "        \n",
    "        def random_search(bounds, gp, y_best, num_samples=1000, xi=0.01):\n",
    "            candidate_points = np.random.uniform(\n",
    "                low = np.array(bounds)[:,0],\n",
    "                high = np.array(bounds)[:, 1],\n",
    "                size = (num_samples, len(bounds))\n",
    "            )\n",
    "\n",
    "            ei_values = EI(candidate_points, gp, y_best, xi = xi)\n",
    "\n",
    "            best_index = np.argmax(ei_values)\n",
    "            return candidate_points[best_index]\n",
    "        \n",
    "        x_next = random_search(bounds, gp, y_best)\n",
    "\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        x3 = []\n",
    "        for j in range(int(len(x_next)/3)):\n",
    "            para1 = x_next[i][3*j] + x_next[i][3*j+1] * 10.566 + x_next[i][3*j+2] * 2.8274\n",
    "            para2 = x_next[i][3*i] + x_next[i][3*i+1] * 11.3 + x_next[i][3*i+2] * 4.7124\n",
    "            para3 = x_next[i][3*i] + x_next[i][3*i+1] * 4.823 + x_next[i][3*i+2] * 3.7699\n",
    "            x1.append(para1)\n",
    "            x2.append(para2)\n",
    "            x3.append(para3)\n",
    "        para_index = [12,13,14,22,24,34,46,116,134,220,223,226,273,276,313,315]\n",
    "        print(f'run {i+pre_size}')\n",
    "\n",
    "        inp_path1 = f'kinet1_{i+pre_size}.inp\\n'\n",
    "        exe_path1 = f'run_plasRxn1.exe'\n",
    "\n",
    "        paraset1 = -21 * np.ones(316)\n",
    "        paraset1[list(range(12))] = 0\n",
    "        paraset1[list(range(90,102))] = 0\n",
    "        for j in range(len(para_index)):\n",
    "            paraset1[para_index[j]] = x1[j]\n",
    "\n",
    "        print(f'run {i+pre_size}')\n",
    "\n",
    "        update_kinet1(paraset1,i+pre_size)\n",
    "        prep_process = run_prep(inp_path1)\n",
    "        prep_process.wait()\n",
    "        compile_zdp1(exe_path1)\n",
    "        exe_process = run_exe(exe_path1)\n",
    "        exe_process.wait()\n",
    "        y1,_,_,t_time1,_,_,_,_ = cal_error(exp_result[0])\n",
    "        print(f'run {i+pre_size} set1 completion')\n",
    "\n",
    "        inp_path2 = f'kinet2_{i+pre_size}.inp\\n'\n",
    "        exe_path2 = f'run_plasRxn2.exe'\n",
    "\n",
    "        paraset2 = -21 * np.ones(316)\n",
    "        paraset2[list(range(12))] = 0\n",
    "        paraset2[list(range(90,102))] = 0\n",
    "        for j in range(len(para_index)):\n",
    "            paraset2[para_index[j]] = x2[j]\n",
    "\n",
    "        update_kinet2(paraset2,i+pre_size)\n",
    "        prep_process = run_prep(inp_path2)\n",
    "        prep_process.wait()\n",
    "        compile_zdp2(exe_path2)\n",
    "        exe_process = run_exe(exe_path2)\n",
    "        exe_process.wait()\n",
    "        y2,_,_,t_time2,_,_,_,_ = cal_error(exp_result[1])\n",
    "        print(f'run {i+pre_size} set2 completion')\n",
    "\n",
    "        inp_path3 = f'kinet3_{i+pre_size}.inp\\n'\n",
    "        exe_path3 = f'run_plasRxn3.exe'\n",
    "\n",
    "        paraset3 = -21 * np.ones(316)\n",
    "        paraset3[list(range(12))] = 0\n",
    "        paraset3[list(range(90,102))] = 0\n",
    "        for j in range(len(para_index)):\n",
    "            paraset3[para_index[j]] = x3[j]\n",
    "\n",
    "        update_kinet3(paraset3,i+pre_size)\n",
    "        prep_process = run_prep(inp_path3)\n",
    "        prep_process.wait()\n",
    "        compile_zdp3(exe_path3)\n",
    "        exe_process = run_exe(exe_path3)\n",
    "        exe_process.wait()\n",
    "        y3,_,_,t_time3,_,_,_,_ = cal_error(exp_result[2])\n",
    "        print(f'run {i+pre_size} set3 completion')\n",
    "       \n",
    "        y_next = y1 + y2 + y3\n",
    "      \n",
    "        # 데이터셋 업데이트\n",
    "        if (t_time1 > 2.1) & (t_time2 > 3.5) & (t_time3 > 5.6):\n",
    "            X = np.vstack([X, x_next])\n",
    "            y = np.append(y, y_next)\n",
    "            \n",
    "            bnds_low = [i[0] for i in bounds]\n",
    "            bnds_high = [i[1] for i in bounds]\n",
    "            if y_next < y_best:\n",
    "                for j in range(len(bounds)):\n",
    "                    bnds_low[j] = x_next[j] * (1-step)\n",
    "                    bnds_high[j] = x_next[j] + (1+step)\n",
    "            bounds = [[bnds_low[j],bnds_high[j]] for j in range(len(bnds_low))]\n",
    "\n",
    "            np.savetxt('X_final.csv', X, delimiter=',', fmt='%f')\n",
    "            np.savetxt('y_final.csv', y, delimiter=',', fmt='%f')\n",
    "        else:\n",
    "            X = np.vstack([X, x_next])\n",
    "            y = np.append(y, max_error)\n",
    "\n",
    "        # Surrogate 모델 업데이트\n",
    "        gp = GP_Model(X, y)\n",
    "        clear_output()\n",
    "        print(f'현재 iteration: {i+pre_size}, dataset 크기: {len(X)}, 현재 최소값: {y_best}, 이번 y: {y[-1]}\\n')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "df_set = pd.read_csv('parameter_set.csv')\n",
    "para_index = [12,13,14,22,24,34,46,116,134,220,223,226,273,276,313,315]\n",
    "para_init = [-0.3905,0.0223, 0.0066, -0.1598, 0.0242, -0.0527, 1.1040, 0.0960, -0.1287, 10.0715, -0.0049, -0.0076,\n",
    "             0.2022, 0.0171, -0.1276, 1.5793, 0.0127, -0.0859, 1.4756, 0.0503, -0.0348, 12.1475, 0.0134, 0.0102,\n",
    "             -0.0418, 0.0592, -0.0372, 11.3629, 0.0049, 0.0230, 10.6719, -0.0204, 0.0066, 14.9529, 0.0073, -0.0079,\n",
    "             -0.9522, -0.0040, -0.0019, -2.6273, -0.0056, -0.0260, -0.6586, 0.0335, -0.0023, 18.8026, 0.0221, -0.0110]\n",
    "\n",
    "problem = {\n",
    "    'num_vars': len(para_index),\n",
    "    'bounds': [[num * (1-step), num * (1+step)] for num in para_init]\n",
    "}\n",
    "initial_samples = latin_hypercube_sampling(problem['bounds'],pre_size)\n",
    "initial_samples = np.vstack((np.array(para_init), initial_samples))\n",
    "db_error = []\n",
    "db_paraset = []\n",
    "db_toprate = []\n",
    "db_leng = []\n",
    "result_list = ['XCH4','SH2','SC2H6','SC2H4','SC2H2','SC3H8','SC3H6','SC4+','SH','SCH','SCH2','SCH3','SC2H3','SC2H5','SC3H5','SC3H7','SC4H9']\n",
    "exp_result = [[16.1,77.031,12.999,1.535,1.302,3.499,0.467,0.422,0,0,0,0,0,0,0,0,0],[18.873,74.645,14.318,1.739,1.556,3.968,0.555,0.508,0,0,0,0,0,0,0,0,0],[9.05848,58.23691,21.87466,2.52725,1.87428,8.83052,0.56969,1.45217,0,0,0,0,0,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 3\n",
      "check the run of preprocessor\n",
      "check the compiler_run1\n",
      "1.0695E+00           ensity of species not configured for BOLSIG+ solver exceeds 3.17D-02           "
     ]
    }
   ],
   "source": [
    "for i in range(len(initial_samples)):\n",
    "    sample1 = []\n",
    "    sample2 = []\n",
    "    sample3 = []\n",
    "    \n",
    "    for j in range(int(len(initial_samples[i])/3)):\n",
    "        para1 = initial_samples[i][3*j] + initial_samples[i][3*j+1] * 10.566 + initial_samples[i][3*j+2] * 2.8274\n",
    "        para2 = initial_samples[i][3*i] + initial_samples[i][3*i+1] * 11.3 + initial_samples[i][3*i+2] * 4.7124\n",
    "        para3 = initial_samples[i][3*i] + initial_samples[i][3*i+1] * 4.823 + initial_samples[i][3*i+2] * 3.7699\n",
    "        sample1.append(para1)\n",
    "        sample2.append(para2)\n",
    "        sample3.append(para3)\n",
    "\n",
    "    inp_path1 = f'kinet1_{i}.inp\\n'\n",
    "    exe_path1 = f'run_plasRxn1.exe'\n",
    "\n",
    "    paraset1 = -21 * np.ones(316)\n",
    "    paraset1[list(range(12))] = 0\n",
    "    paraset1[list(range(90,102))] = 0\n",
    "    for j in range(len(para_index)):\n",
    "        paraset1[para_index[j]] = sample1[j]\n",
    "\n",
    "    print(f'run {i}')\n",
    "\n",
    "    update_kinet1(paraset1,i)\n",
    "    prep_process = run_prep(inp_path1)\n",
    "    prep_process.wait()\n",
    "    compile_zdp1(exe_path1)\n",
    "    exe_process = run_exe(exe_path1)\n",
    "    exe_process.wait()\n",
    "    error1,_,_,total_time1,_,_,_,_ = cal_error(exp_result[0])\n",
    "    print(f'run {i} set1 completion')\n",
    "\n",
    "    inp_path2 = f'kinet2_{i}.inp\\n'\n",
    "    exe_path2 = f'run_plasRxn2.exe'\n",
    "\n",
    "    paraset2 = -21 * np.ones(316)\n",
    "    paraset2[list(range(12))] = 0\n",
    "    paraset2[list(range(90,102))] = 0\n",
    "    for j in range(len(para_index)):\n",
    "        paraset2[para_index[j]] = sample2[j]\n",
    "\n",
    "    update_kinet2(paraset2,i)\n",
    "    prep_process = run_prep(inp_path2)\n",
    "    prep_process.wait()\n",
    "    compile_zdp2(exe_path2)\n",
    "    exe_process = run_exe(exe_path2)\n",
    "    exe_process.wait()\n",
    "    error2,_,_,total_time2,_,_,_,_ = cal_error(exp_result[1])\n",
    "    print(f'run {i} set2 completion')\n",
    "\n",
    "    inp_path3 = f'kinet3_{i}.inp\\n'\n",
    "    exe_path3 = f'run_plasRxn3.exe'\n",
    "\n",
    "    paraset3 = -21 * np.ones(316)\n",
    "    paraset3[list(range(12))] = 0\n",
    "    paraset3[list(range(90,102))] = 0\n",
    "    for j in range(len(para_index)):\n",
    "        paraset3[para_index[j]] = sample3[j]\n",
    "\n",
    "    update_kinet3(paraset3,i)\n",
    "    prep_process = run_prep(inp_path3)\n",
    "    prep_process.wait()\n",
    "    compile_zdp3(exe_path3)\n",
    "    exe_process = run_exe(exe_path3)\n",
    "    exe_process.wait()\n",
    "    error3,_,_,total_time3,_,_,_,_ = cal_error(exp_result[2])\n",
    "    print(f'run {i} set3 completion')\n",
    "\n",
    "\n",
    "    if (float(total_time1) > 2.1) & (float(total_time2) > 3.5) & (float(total_time3) > 5.6):\n",
    "        db_error.append(error1+error2+error3)\n",
    "        db_paraset.append(initial_samples[i])\n",
    "    clear_output()\n",
    "\n",
    "with open('db_error.txt', 'w') as file:\n",
    "    for i in range(len(db_error)):\n",
    "        file.writelines(str(db_error[i]))\n",
    "        file.writelines('\\n')\n",
    "    file.close()\n",
    "with open('db_paraset.txt', 'w') as file:\n",
    "    for i in range(len(db_paraset)):\n",
    "        file.writelines(str(db_paraset[i].tolist()))\n",
    "        file.writelines('\\n')\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "with open('db_error.txt', 'r') as file:\n",
    "    lines_error = file.readlines()\n",
    "    file.close()\n",
    "with open('db_paraset.txt', 'r') as file:\n",
    "    lines_paraset = file.readlines()\n",
    "    file.close()\n",
    "pre_X = np.array([ast.literal_eval(i) for i in lines_paraset])\n",
    "pre_Y = np.array([float(i[:-1]) for i in lines_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([824.78263887, 759.52213066, 729.30201235])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_model = GP_Model(pre_X,pre_Y)\n",
    "gp_model.predict(pre_X)\n",
    "bounds = [[-1*step,step]] * len(para_index)\n",
    "X_final, y_final = BO(gp_model, bounds, 1000,pre_X,pre_Y,exp_result)\n",
    "\n",
    "np.savetxt('X_final.csv', X_final, delimiter=',', fmt='%f')\n",
    "np.savetxt('y_final.csv', y_final, delimiter=',', fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plaskinsol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
