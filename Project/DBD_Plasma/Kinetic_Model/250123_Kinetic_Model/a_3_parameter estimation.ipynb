{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "import ast\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MVs controller\n",
    "step = 0.01\n",
    "pre_size = 3\n",
    "max_error = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile zdplaskin\n",
    "def run_prep(inp_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            'preprocessor.exe',\n",
    "            stdout = subprocess.DEVNULL,        # ignore outputs\n",
    "            stderr = subprocess.DEVNULL,        # ignore errors\n",
    "            stdin = subprocess.PIPE,            # recognize input\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        process.stdin.write(inp_path)\n",
    "        process.stdin.flush()                   # send a data\n",
    "\n",
    "        while process.poll() is None:           # check the program state, if None, program is still in the run\n",
    "            process.stdin.write('.\\n')\n",
    "            process.stdin.flush()\n",
    "    except:\n",
    "        pass\n",
    "    print('check the run of preprocessor')\n",
    "    return process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile exe\n",
    "def compile_zdp(name):\n",
    "    compile_command = [\n",
    "        'gfortran', '-o', name, 'dvode_f90_m.F90', 'zdplaskin_m.F90',\n",
    "        'run_plasRxn_v2.F90', 'bolsig_x86_64_g.dll'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(compile_command)\n",
    "    except:\n",
    "        pass\n",
    "    print('check the compiler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exe\n",
    "def run_exe(exe_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            exe_path,\n",
    "            stdout = subprocess.PIPE, # Read standard outputs\n",
    "            stderr = subprocess.PIPE, # Read standard errors\n",
    "            universal_newlines=True,  # outputs to str variables\n",
    "            bufsize = 1               # control the size of buffer\n",
    "        )\n",
    "\n",
    "        log_flag = False             # The flag for starting log after \"Caculation Start!!\"\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if not output:\n",
    "                break\n",
    "            if \"Calculation Start\" in output:\n",
    "                log_flag = True\n",
    "\n",
    "            if log_flag:\n",
    "                print(f'\\r{output.strip()}           ',end='',flush=True)\n",
    "\n",
    "            if \"PRESS ENTER TO EXIT\" in output:\n",
    "                process.kill()        # forced shutdown\n",
    "                break\n",
    "            if \"WARNING: BOLSIG+ convergence failed\" in output:\n",
    "                process.kill()        # forced shutdown\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    return process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error calculation\n",
    "def cal_error(exp_result):\n",
    "    # Read a result\n",
    "    species = []\n",
    "    with open('qt_species_list.txt','r') as file:\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            line = line[3:]\n",
    "            species.append(line)\n",
    "        file.close()\n",
    "\n",
    "    df_sp = pd.read_csv('qt_densities.txt', sep=r'\\s+', header=0, names=['Time [s]']+species)\n",
    "\n",
    "    CH4 = (df_sp['CH4'] + df_sp['CH4(V13)'] + df_sp['CH4(V24)'])\n",
    "    C2H2 = (df_sp['C2H2'] + df_sp['C2H2(V13)']+ df_sp['C2H2(V2)']+ df_sp['C2H2(V5)'])\n",
    "    C2H4 = (df_sp['C2H4'] + df_sp['C2H4(V1)']+ df_sp['C2H4(V2)'])\n",
    "    C2H6 = (df_sp['C2H6'] + df_sp['C2H6(V13)']+ df_sp['C2H6(V24)'])\n",
    "    C3H6 = (df_sp['C3H6'] + df_sp['C3H6(V)'])\n",
    "    C3H8 = (df_sp['C3H8'] + df_sp['C3H8(V1)'] + df_sp['C3H8(V2)'])\n",
    "    C4H10 = (df_sp['C4H9H'])\n",
    "    H2 = df_sp['H2']\n",
    "    C = df_sp['C']\n",
    "    H = df_sp['H']\n",
    "    CH = df_sp['CH']\n",
    "    CH2 = df_sp['CH2']\n",
    "    CH3 = df_sp['CH3']\n",
    "    C2H3 = df_sp['C2H3']\n",
    "    C2H5 = df_sp['C2H5']\n",
    "    C3H7 = df_sp['C3H7']\n",
    "    CH3p = df_sp['CH3^+']\n",
    "    C2H5p = df_sp['C2H5^+']\n",
    "\n",
    "    exp = exp_result\n",
    "\n",
    "    t1 = abs(df_sp['Time [s]']-5.65).argmin()\n",
    "    t2 = abs(df_sp['Time [s]']-7.27).argmin()\n",
    "    t3 = abs(df_sp['Time [s]']-10.18).argmin()\n",
    "    t4 = abs(df_sp['Time [s]']-16.96).argmin()\n",
    "    t = [t1, t2, t3, t4]\n",
    "\n",
    "    for i in t:\n",
    "        sim = []\n",
    "        for j in range(18):\n",
    "            sim_XCH4 = (CH4.iloc[0] - CH4.iloc[i])/CH4.iloc[0]*100\n",
    "            sim_SH2 = 0.5*H2.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC2H6 = 2*C2H6.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC2H4 = 2*C2H4.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC2H2 = 2*C2H2.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC3H8 = 3*C3H8.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC3H6 = 3*C3H6.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC4H10 = 4*C4H10.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SH = 0.25*H.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SCH = CH.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SCH2 = CH2.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SCH3 = CH3.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC2H3 = 2*C2H3.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC2H5 = 2*C2H5.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC3H7 = 3*C3H7.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SCH3p = CH3p.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            sim_SC2H5p = 2*C2H5p.iloc[i]/(CH4.iloc[0] - CH4.iloc[i])*100\n",
    "            newsim = [sim_XCH4,sim_SH2,sim_SC2H6,sim_SC2H4,sim_SC2H2,sim_SC3H8,sim_SC3H6,sim_SC4H10,sim_SH,sim_SCH,sim_SCH2,sim_SCH3,sim_SC2H3,sim_SC2H5,sim_SC3H7,sim_SCH3p,sim_SC2H5p]\n",
    "            sim.append(newsim)\n",
    "\n",
    "    err = 0 \n",
    "    for i in range(len(exp)):\n",
    "        for j in range(len(exp[i])):\n",
    "            if j < 9:\n",
    "                err += ((exp[i][j] - sim[i][j])/exp[i][j])**2\n",
    "            else:\n",
    "                err += (exp[i][j] - sim[i][j])**2\n",
    "    \n",
    "    return err, sim, df_sp['Time [s]'].iloc[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial variable sset sampling: latin_hypercube sampling\n",
    "def latin_hypercube_sampling(bounds, n_samples):\n",
    "    return np.random.uniform(\n",
    "        low = np.array([b[0] for b in bounds]),\n",
    "        high = np.array([b[1] for b in bounds]),\n",
    "        size = (n_samples, len(bounds))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update kinet.inp\n",
    "def update_kinet(samples,index):\n",
    "    with open(f'kinet.inp','r') as file:\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "    new_lines=[]\n",
    "    for i in range(316):\n",
    "        new_value = 10**samples[i]\n",
    "        new_value2 = f'{new_value:.4e}'.replace('e-0','e-').replace('e+00','e0').replace('e+0','e+').replace('e','d')\n",
    "        new_line = f'$ double precision, parameter :: f{i} = {new_value2}\\n'\n",
    "        new_lines.append(new_line)\n",
    "    new_inp = lines[:18] + new_lines + lines[334:]\n",
    "    with open(f'./kinet{index}.inp', 'w') as file:\n",
    "        file.writelines(new_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Surrogate Model\n",
    "def GP_Model(X,y):\n",
    "    # Gaussian Process Surrogate Model\n",
    "    constant_kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-5,1e5))\n",
    "    RBF_kernel = RBF(1.0, length_scale_bounds = (1e-5,1e5))\n",
    "    kernel = constant_kernel * RBF_kernel\n",
    "\n",
    "    gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-6)\n",
    "    gp_model.fit(X,y)\n",
    "    return gp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Improvement\n",
    "def EI(X, gp, y_best, xi):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma\n",
    "    #improvement = mu - y_best - xi\n",
    "    improvement = y_best - mu - xi\n",
    "    Z = improvement / sigma\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization for minimization\n",
    "def BO(gp, bounds, n_iter, X, y, exp_result):\n",
    "    \"\"\"Perform Bayesian Optimization using EI for minimization.\"\"\"\n",
    "    for i in range(n_iter):\n",
    "        y_best = np.min(y)  # 현재까지의 최소값\n",
    "        \n",
    "        def random_search(bounds, gp, y_best, num_samples=1000, xi=0.01):\n",
    "            candidate_points = np.random.uniform(\n",
    "                low = np.array(bounds)[:,0],\n",
    "                high = np.array(bounds)[:, 1],\n",
    "                size = (num_samples, len(bounds))\n",
    "            )\n",
    "\n",
    "            ei_values = EI(candidate_points, gp, y_best, xi = xi)\n",
    "\n",
    "            best_index = np.argmax(ei_values)\n",
    "            return candidate_points[best_index]\n",
    "        \n",
    "        x_next = random_search(bounds, gp, y_best)\n",
    "\n",
    "        # 새로운 관측값 추가 (사용자 정의 시뮬레이션 함수 호출 필요)\n",
    "        para_index = [12,13,14,19,22,24,34,45,46,80,88,116,117,118,134,211,220,221,223,226,233,268,269,273,276,291,295,299,300,302,304,308,313,315]\n",
    "        paraset = -21 * np.ones(316)\n",
    "        paraset[list(range(12))] = 0\n",
    "        paraset[list(range(90,102))] = 0\n",
    "        for j in range(len(para_index)):\n",
    "            paraset[para_index[j]] = x_next[j]\n",
    "\n",
    "        update_kinet(paraset,i+pre_size)\n",
    "        inp_path = f'kinet{i+pre_size}.inp\\n'\n",
    "        exe_path = 'run_plasRxn_v2.exe'\n",
    "        run_prep(inp_path)\n",
    "        compile_zdp(exe_path)\n",
    "        run_exe(exe_path)\n",
    "        y_next, _, _, t_time, XCH4, SC2H6, SC2H4, SC2H2 = cal_error(exp_result)  # 사용자의 실제 목적 함수\n",
    "\n",
    "        # 데이터셋 업데이트\n",
    "        if t_time > 16.96:\n",
    "            X = np.vstack([X, x_next])\n",
    "            y = np.append(y, y_next)\n",
    "            \n",
    "            if y_next < y_best:\n",
    "                bounds = [[num * (1-step), num * (1+step)] for num in x_next]\n",
    "\n",
    "            np.savetxt('X_final.csv', X, delimiter=',', fmt='%f')\n",
    "            np.savetxt('y_final.csv', y, delimiter=',', fmt='%f')\n",
    "        else:\n",
    "            X = np.vstack([X, x_next])\n",
    "            y = np.append(y, max_error)\n",
    "\n",
    "        \n",
    "        # Surrogate 모델 업데이트\n",
    "        gp = GP_Model(X, y)\n",
    "        print(f'''현재 iteration: {i+pre_size}, dataset 크기: {len(X)}, 현재 최소값: {y_best}, 이번 y: {y[-1]}\\n \n",
    "              전환율: {XCH4}, C2H6: {SC2H6}, C2H4: {SC2H4}, C2H2: {SC2H2}''',)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameter samples\n",
    "df_set = pd.read_csv('parameter_set.csv')\n",
    "para_index = [12,13,14,19,22,24,34,45,46,80,88,116,117,118,134,211,220,221,223,226,233,268,269,273,276,291,295,299,300,302,304,308,313,315]\n",
    "problem = {\n",
    "    'num_bars': len(para_index),\n",
    "    'bounds' : [[-step,step]] * len(para_index)\n",
    "}\n",
    "\n",
    "initial_samples = latin_hypercube_sampling(problem['bounds'],pre_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the experimental results\n",
    "result_list = ['XCH4','SH2','SC2H6','SC2H4','SC2H2','SC3H8','SC3H6','SC4','SH','SCH','SCH2','SCH3','SC2H3','SC2H5','SC3H7','SCH3^+','SC2H5^+']\n",
    "exp_result = [[7.37,60.84,21.1,3.07,2.16,7.83,0.67,1.23,2.09,0,0,0,0,0,0,0,0],\n",
    "              [7.79,53.13,24.77,3.24,2.38,9.55,0.77,1.59,2.56,0,0,0,0,0,0,0,0],\n",
    "              [8.73,62.08,19.51,1.59,1.38,8.64,0.56,1.84,2.40,0,0,0,0,0,0,0,0],\n",
    "              [21.53,43.31,31.17,3.42,2.67,12.71,0.94,2.31,3.46,0,0,0,0,0,0,0,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0\n",
      "check the run of preprocessor\n",
      "check the compiler\n",
      "1.1310E-02                    "
     ]
    }
   ],
   "source": [
    "# pre-trained mode\n",
    "db_error = []\n",
    "db_paraset = []\n",
    "\n",
    "for i in range(len(initial_samples)):\n",
    "    inp_path = f'kinet{i}.inp\\n'\n",
    "    exe_path = 'run_plasRxn_v2.exe'\n",
    "\n",
    "    print(f'run {i}')\n",
    "\n",
    "    paraset = -21 * np.ones(316)\n",
    "    paraset[list(range(12))] = 0\n",
    "    paraset[list(range(90,102))] = 0\n",
    "    for j in range(len(para_index)):\n",
    "        paraset[para_index[j]] = initial_samples[i][j]\n",
    "\n",
    "    update_kinet(paraset,i)\n",
    "    prep_process = run_prep(inp_path)\n",
    "    prep_process.wait()\n",
    "    compile_zdp(exe_path)\n",
    "    exe_process = run_exe(exe_path)\n",
    "    exe_process.wait()\n",
    "    error, sim, total_time = cal_error(exp_result)\n",
    "    if float(total_time) > 16.96:\n",
    "        db_error.append(error)\n",
    "        db_paraset.append(initial_samples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained data gathering\n",
    "with open('db_error.txt', 'w') as file:\n",
    "    for i in range(len(db_error)):\n",
    "        file.writelines(str(db_error[i]))\n",
    "        file.writelines('\\n')\n",
    "    file.close()\n",
    "with open('db_paraset.txt', 'w') as file:\n",
    "    for i in range(len(db_paraset)):\n",
    "        file.writelines(str(db_paraset[i].tolist()))\n",
    "        file.writelines('\\n')\n",
    "    file.close()\n",
    "\n",
    "# Load Data\n",
    "with open('db_error.txt', 'r') as file:\n",
    "    lines_error = file.readlines()\n",
    "    file.close()\n",
    "with open('db_paraset.txt', 'r') as file:\n",
    "    lines_paraset = file.readlines()\n",
    "    file.close()\n",
    "pre_X = np.array([ast.literal_eval(i) for i in lines_paraset])\n",
    "pre_Y = np.array([float(i[:-1]) for i in lines_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BO run\n",
    "gp_model = GP_Model(pre_X,pre_Y)\n",
    "gp_model.predict(pre_X)\n",
    "bounds = [[-step,step]] * len(para_index)\n",
    "X_final, y_final = BO(gp_model, bounds, 10,pre_X,pre_Y,exp_result)\n",
    "\n",
    "np.savetxt('X_final.csv', X_final, delimiter=',', fmt='%f')\n",
    "np.savetxt('y_final.csv', y_final, delimiter=',', fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plaskinsol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
