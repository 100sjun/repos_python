{
  "lstm_hidden_size": 48,
  "lstm_n_layers": 4,
  "lstm_dropout": 0.30000000000000004,
  "decoder_hidden_size": 64,
  "decoder_n_layers": 3,
  "decoder_dropout": 0.4,
  "current_hidden_size": 16,
  "current_n_layers": 3,
  "current_dropout": 0.1,
  "noam_factor": 0.9,
  "warmup_ratio": 0.05,
  "batch_size": 2
}