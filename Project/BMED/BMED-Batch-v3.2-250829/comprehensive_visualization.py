# ì „ì²´ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ëŠ” ìƒˆë¡œìš´ ì½”ë“œ
# 8ê°œì˜ ê°œë³„ plotì„ ìƒì„±í•˜ì—¬ ê° plotì€ ëª¨ë“  ì‹¤í—˜ì˜ í•´ë‹¹ featureë¥¼ subplotìœ¼ë¡œ í‘œì‹œ

import matplotlib.pyplot as plt
import numpy as np
import torch
import pandas as pd
import math

def comprehensive_model_evaluation(evaluation_model, dataloader, ndf, range_mm, model_info, best_train_loss=None, best_epoch=None):
    """
    ì „ì²´ ì‹¤í—˜ ë°ì´í„°ì— ëŒ€í•œ ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€ ì‹œê°í™”
    8ê°œ featureë³„ë¡œ ê°œë³„ figureë¥¼ ìƒì„±í•˜ê³ , ê° figureëŠ” ëª¨ë“  ì‹¤í—˜ì˜ í•´ë‹¹ featureë¥¼ subplotìœ¼ë¡œ í‘œì‹œ
    
    Args:
        evaluation_model: í‰ê°€í•  ëª¨ë¸
        dataloader: ì „ì²´ ë°ì´í„°ë¡œë”
        ndf: ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„
        range_mm: ì •ê·œí™” ë²”ìœ„ ì •ë³´
        model_info: ëª¨ë¸ ì •ë³´ ë¬¸ìì—´
        best_train_loss: ìµœì  í›ˆë ¨ ì†ì‹¤ (ì„ íƒì )
        best_epoch: ìµœì  ì—í¬í¬ (ì„ íƒì )
    """
    
    # Device ì„¤ì •
    device = next(evaluation_model.parameters()).device
    
    def denormalize_data(normalized_data, feature_name):
        """ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì›ë˜ ë‹¨ìœ„ë¡œ ë³µì›"""
        if feature_name in range_mm:
            min_val = range_mm[feature_name]['min']
            max_val = range_mm[feature_name]['max']
            return normalized_data * (max_val - min_val) + min_val
        return normalized_data

    # feature ì •ë³´ ì •ì˜
    feature_names = ['VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']
    feature_indices = [2, 3, 4, 5, 6, 7, 8, 9]
    feature_units = ['L', 'L', 'L', 'mol/L', 'mol/L', 'mol/L', 'mol/L', 'A']
    feature_titles = ['Feed Volume', 'Acid Volume', 'Base Volume', 'Feed LA Conc.', 'Acid LA Conc.', 'Feed K Conc.', 'Base K Conc.', 'Current']
    
    # ì‹¤í—˜ ë²ˆí˜¸ì™€ ë°ì´í„°ë¡œë” ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±
    exp_num_to_dataloader_idx = {}
    dataloader_idx_to_exp_num = {}
    
    for i, exp_num in enumerate(sorted(ndf['exp'].unique())):
        exp_num_to_dataloader_idx[exp_num] = i
        dataloader_idx_to_exp_num[i] = exp_num
    
    available_exp_nums = sorted(ndf['exp'].unique())
    total_experiments = len(available_exp_nums)
    print(f"ğŸ” ì „ì²´ {total_experiments}ê°œ ì‹¤í—˜ ë°ì´í„° ë¶„ì„ ì‹œì‘...")
    
    # ì „ì²´ ì‹¤í—˜ ë°ì´í„° ìˆ˜ì§‘
    all_experiment_data = {}
    
    evaluation_model.eval()
    with torch.no_grad():
        for dataloader_idx, (input_seq, seq_lengths) in enumerate(dataloader):
            try:
                # ì‹¤ì œ ì‹¤í—˜ ë²ˆí˜¸ í™•ì¸
                actual_exp_num = dataloader_idx_to_exp_num[dataloader_idx]
                
                input_seq = input_seq.to(device)
                seq_lengths = seq_lengths.to(device)
                
                # Teacher forcing ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ tf_data í•¨ìˆ˜ ì‚¬ìš©)
                inputs = input_seq[:, :-1, :-1]  # ì „ë¥˜ ì œì™¸í•œ ì…ë ¥
                targets = input_seq[:, 1:, :]    # ì „ë¥˜ í¬í•¨í•œ íƒ€ê²Ÿ
                target_seq_lengths = seq_lengths - 1
                
                # ëª¨ë¸ ì˜ˆì¸¡
                predictions = evaluation_model(inputs, target_seq_lengths)
                
                # CPUë¡œ ì´ë™
                predictions_cpu = predictions.cpu().numpy()
                targets_cpu = targets.cpu().numpy()
                input_seq_cpu = input_seq.cpu().numpy()
                
                # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì‚¬ìš© (batch size = 4ì´ì§€ë§Œ ê°™ì€ ì‹¤í—˜ ë°ì´í„°)
                pred_sample = predictions_cpu[0]
                actual_sample = targets_cpu[0]
                initial_sample = input_seq_cpu[0]
                seq_len = target_seq_lengths[0].item()
                full_seq_len = seq_lengths[0].item()
                
                # ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ë§Œ ì‚¬ìš©
                pred_sample = pred_sample[:seq_len]
                actual_sample = actual_sample[:seq_len]
                initial_sample = initial_sample[:full_seq_len]
                
                # ì‹œê°„ ì¶• ìƒì„±
                time_points = np.arange(full_seq_len) * 0.25
                
                # ì˜ˆì¸¡ê°’ì„ ì´ˆê¸°ê°’ê³¼ ì—°ê²°í•˜ì—¬ ì „ì²´ ì‹œí€€ìŠ¤ êµ¬ì„±
                prediction_full = np.zeros((full_seq_len, 10))
                prediction_full[0] = initial_sample[0]  # ì´ˆê¸°ê°’
                prediction_full[1:seq_len+1] = pred_sample  # ì˜ˆì¸¡ê°’
                
                # ì‹¤í—˜ ì¡°ê±´ ì •ë³´
                voltage = denormalize_data(initial_sample[0, 0], 'V')
                electrolyte = denormalize_data(initial_sample[0, 1], 'E')
                
                # ë°ì´í„° ì €ì¥
                all_experiment_data[actual_exp_num] = {
                    'dataloader_idx': dataloader_idx,
                    'time_points': time_points,
                    'actual_data': initial_sample,
                    'predicted_data': prediction_full,
                    'seq_len': seq_len,
                    'full_seq_len': full_seq_len,
                    'voltage': voltage,
                    'electrolyte': electrolyte,
                    'actual_sample': actual_sample,
                    'pred_sample': pred_sample
                }
                
                print(f"âœ… ì‹¤í—˜ {actual_exp_num} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (ì‹œê°„: {full_seq_len*0.25:.2f}h, V: {voltage:.1f}V, E: {electrolyte:.3f}M)")
                
            except Exception as e:
                print(f"âŒ ì‹¤í—˜ {dataloader_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
    
    print(f"ğŸ“Š ì´ {len(all_experiment_data)}ê°œ ì‹¤í—˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    
    # ì„œë¸Œí”Œë¡¯ ê·¸ë¦¬ë“œ í¬ê¸° ê³„ì‚° (24ê°œ ì‹¤í—˜ì„ ìœ„í•œ ìµœì  ë°°ì¹˜)
    sqrt_exp = math.ceil(math.sqrt(total_experiments))
    if sqrt_exp * (sqrt_exp - 1) >= total_experiments:
        grid_rows = sqrt_exp - 1
        grid_cols = sqrt_exp
    else:
        grid_rows = sqrt_exp
        grid_cols = sqrt_exp
    
    print(f"ğŸ“ ì„œë¸Œí”Œë¡¯ ê·¸ë¦¬ë“œ: {grid_rows}x{grid_cols} ({grid_rows * grid_cols}ê°œ ìœ„ì¹˜)")
    
    # 8ê°œ featureë³„ë¡œ ê°œë³„ figure ìƒì„±
    for feature_idx, (feature_name, feature_index, unit, title) in enumerate(zip(feature_names, feature_indices, feature_units, feature_titles)):
        print(f"ğŸ¨ {feature_name} ({title}) í”Œë¡¯ ìƒì„± ì¤‘...")
        
        # ìƒˆ figure ìƒì„±
        fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(4*grid_cols, 3*grid_rows))
        fig.suptitle(f'{title} ({feature_name}) - All Experiments\n{model_info}', fontsize=16, fontweight='bold')
        
        # axesë¥¼ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ (ë‹¨ì¼ subplotì¸ ê²½ìš° ì²˜ë¦¬)
        if grid_rows == 1 and grid_cols == 1:
            axes = [axes]
        elif grid_rows == 1 or grid_cols == 1:
            axes = axes.flatten()
        else:
            axes = axes.flatten()
        
        # ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
        all_rmse = []
        all_r2 = []
        
        # ê° ì‹¤í—˜ë³„ subplot ìƒì„±
        for plot_idx, exp_num in enumerate(sorted(all_experiment_data.keys())):
            if plot_idx >= len(axes):
                break
                
            ax = axes[plot_idx]
            exp_data = all_experiment_data[exp_num]
            
            # ë°ì´í„° denormalize
            actual_denorm = denormalize_data(exp_data['actual_data'][:, feature_index], feature_name)
            pred_denorm = denormalize_data(exp_data['predicted_data'][:exp_data['seq_len']+1, feature_index], feature_name)
            
            # ì‹¤ì œ vs ì˜ˆì¸¡ í”Œë¡¯
            ax.plot(exp_data['time_points'], actual_denorm, 'b-', linewidth=1.5, label='Actual', marker='o', markersize=2)
            ax.plot(exp_data['time_points'][:exp_data['seq_len']+1], pred_denorm, 'r--', linewidth=1.5, label='Predicted', marker='s', markersize=2)
            
            # ì´ˆê¸°ê°’ê³¼ ì˜ˆì¸¡ ì‹œì‘ì  í‘œì‹œ
            ax.plot(exp_data['time_points'][0], actual_denorm[0], 'go', markersize=4)
            ax.axvline(x=exp_data['time_points'][1], color='gray', linestyle=':', alpha=0.5)
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì˜ˆì¸¡ êµ¬ê°„ë§Œ)
            actual_pred_denorm = denormalize_data(exp_data['actual_sample'][:, feature_index], feature_name)
            pred_pred_denorm = denormalize_data(exp_data['pred_sample'][:, feature_index], feature_name)
            
            rmse = np.sqrt(np.mean((actual_pred_denorm - pred_pred_denorm)**2))
            r2 = 1 - (np.sum((actual_pred_denorm - pred_pred_denorm)**2) / np.sum((actual_pred_denorm - np.mean(actual_pred_denorm))**2))
            
            all_rmse.append(rmse)
            all_r2.append(r2)
            
            # ì„œë¸Œí”Œë¡¯ ì œëª©ê³¼ ë ˆì´ë¸” ì„¤ì •
            ax.set_title(f'Exp {exp_num}\nV:{exp_data["voltage"]:.1f}V, E:{exp_data["electrolyte"]:.2f}M\nRMSE:{rmse:.3f}', fontsize=8)
            ax.set_xlabel('Time (h)', fontsize=8)
            ax.set_ylabel(f'{feature_name} ({unit})', fontsize=8)
            ax.grid(True, alpha=0.3)
            ax.tick_params(labelsize=7)
            
            # ë²”ë¡€ëŠ” ì²« ë²ˆì§¸ subplotì—ë§Œ í‘œì‹œ
            if plot_idx == 0:
                ax.legend(fontsize=7, loc='best')
        
        # ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” subplot ìˆ¨ê¸°ê¸°
        for plot_idx in range(len(all_experiment_data), len(axes)):
            axes[plot_idx].set_visible(False)
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½ í…ìŠ¤íŠ¸ ì¶”ê°€
        avg_rmse = np.mean(all_rmse)
        avg_r2 = np.mean(all_r2)
        std_rmse = np.std(all_rmse)
        std_r2 = np.std(all_r2)
        
        summary_text = f"Overall Performance:\nRMSE: {avg_rmse:.4f}Â±{std_rmse:.4f} {unit}\nRÂ²: {avg_r2:.3f}Â±{std_r2:.3f}"
        fig.text(0.02, 0.02, summary_text, fontsize=10, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.93, bottom=0.1)
        plt.show()
        
        print(f"âœ… {feature_name} í”Œë¡¯ ì™„ë£Œ - í‰ê·  RMSE: {avg_rmse:.4f} {unit}, í‰ê·  RÂ²: {avg_r2:.3f}")
    
    # ì „ì²´ ìš”ì•½ í†µê³„
    print("\n" + "="*80)
    print("COMPREHENSIVE MODEL EVALUATION SUMMARY")
    print(f"{model_info}")
    print("="*80)
    
    # Featureë³„ ì „ì²´ ì„±ëŠ¥ ìš”ì•½
    overall_performance = {}
    for feature_idx, (feature_name, feature_index, unit) in enumerate(zip(feature_names, feature_indices, feature_units)):
        all_rmse = []
        all_r2 = []
        all_mae = []
        
        for exp_num in sorted(all_experiment_data.keys()):
            exp_data = all_experiment_data[exp_num]
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            actual_pred_denorm = denormalize_data(exp_data['actual_sample'][:, feature_index], feature_name)
            pred_pred_denorm = denormalize_data(exp_data['pred_sample'][:, feature_index], feature_name)
            
            rmse = np.sqrt(np.mean((actual_pred_denorm - pred_pred_denorm)**2))
            mae = np.mean(np.abs(actual_pred_denorm - pred_pred_denorm))
            r2 = 1 - (np.sum((actual_pred_denorm - pred_pred_denorm)**2) / np.sum((actual_pred_denorm - np.mean(actual_pred_denorm))**2))
            
            all_rmse.append(rmse)
            all_mae.append(mae)
            all_r2.append(r2)
        
        overall_performance[feature_name] = {
            'rmse_mean': np.mean(all_rmse),
            'rmse_std': np.std(all_rmse),
            'mae_mean': np.mean(all_mae),
            'mae_std': np.std(all_mae),
            'r2_mean': np.mean(all_r2),
            'r2_std': np.std(all_r2),
            'unit': unit
        }
        
        print(f"{feature_name:6s}: RMSE={np.mean(all_rmse):.4f}Â±{np.std(all_rmse):.4f} {unit}, "
              f"MAE={np.mean(all_mae):.4f}Â±{np.std(all_mae):.4f} {unit}, "
              f"RÂ²={np.mean(all_r2):.3f}Â±{np.std(all_r2):.3f}")
    
    # ì „ì²´ í‰ê·  ì„±ëŠ¥
    all_features_rmse = [overall_performance[fname]['rmse_mean'] for fname in feature_names]
    all_features_r2 = [overall_performance[fname]['r2_mean'] for fname in feature_names]
    
    print("-"*80)
    print(f"OVERALL AVERAGE:")
    print(f"  Average RMSE across features: {np.mean(all_features_rmse):.4f} (mixed units)")
    print(f"  Average RÂ² across features: {np.mean(all_features_r2):.3f}")
    print(f"  Total experiments evaluated: {len(all_experiment_data)}")
    
    if best_train_loss is not None and best_epoch is not None:
        print(f"  Best training loss: {best_train_loss:.6f} (Epoch {best_epoch})")
    
    print("="*80)
    
    return overall_performance, all_experiment_data

# ì‚¬ìš© ì˜ˆì‹œ:
# comprehensive_model_evaluation(best_model, dataloader, ndf, range_mm, model_info, best_train_loss, best_epoch)