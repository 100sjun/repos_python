{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca099116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'Using device: {device}')\n",
    "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    else:\n",
    "        print(f'Using device: {device}')\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7439571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(name):\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CBLA': {'min':0, 'max': df['CBLA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CAK': {'min':0, 'max': df['CAK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef616b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_const(ndf):\n",
    "    sequences = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']\n",
    "    \n",
    "    for exp in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp].sort_values(by='t')\n",
    "        sequences.append(exp_data[feature_cols].values)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a9233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequences(sequences):\n",
    "    max_seq_len = max([len(seq) for seq in sequences])\n",
    "    seq_len = [len(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=-1)\n",
    "\n",
    "    return padded_sequences, seq_len, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f52012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(pad_seq, seq_len):\n",
    "    input_tensor= pad_seq.float()\n",
    "    seq_len_tensor= torch.tensor(seq_len)\n",
    "\n",
    "    device = set_device()\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    seq_len_tensor = seq_len_tensor.to(device)\n",
    "\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e82b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_dataloaders(dataset, k_folds=5, batch_size=8, random_state=42):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "    dataloaders = []\n",
    "    batch_size = math.ceil(len(dataset)/k_folds)\n",
    "    \n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(range(len(dataset)))):\n",
    "        print(f\"Fold {fold + 1}: Train size = {len(train_indices)}, Val size = {len(val_indices)}\")\n",
    "        \n",
    "        # Create subsets for train and validation\n",
    "        train_subset = Subset(dataset, train_indices)\n",
    "        val_subset = Subset(dataset, val_indices)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        dataloaders.append((train_loader, val_loader))\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "        \n",
    "        # 패딩된 시퀀스를 pack하여 효율적 처리\n",
    "        packed_input = pack_padded_sequence(x, seq_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        \n",
    "        # 다시 패딩된 형태로 복원\n",
    "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        norm = self.layer_norm(lstm_out)\n",
    "        return self.dropout(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "084b3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=2, num_nodes=None, dropout = 0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_nodes is None:\n",
    "            num_nodes = hidden_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "    # 첫 번째 레이어: hidden_size → num_nodes\n",
    "        self.layers.append(nn.Linear(hidden_size, num_nodes))\n",
    "        self.layers.append(nn.LayerNorm(num_nodes))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # 중간 은닉층들: num_nodes → num_nodes\n",
    "        for i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_nodes,num_nodes))\n",
    "            self.layers.append(nn.LayerNorm(num_nodes))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # 마지막 출력층: num_nodes → output_size\n",
    "        self.layers.append(nn.Linear(num_nodes, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateUpdateLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, mlp_output, cur_state):\n",
    "        V = cur_state[..., 0]; E = cur_state[..., 1]\n",
    "        VF = cur_state[..., 2]; VA = cur_state[..., 3]; VB = cur_state[..., 4]\n",
    "        CFLA = cur_state[..., 5]; CALA = cur_state[..., 6]; CBLA = cur_state[..., 7]\n",
    "        CFK = cur_state[..., 8]; CAK = cur_state[..., 9]; CBK = cur_state[..., 10]\n",
    "        I = cur_state[..., 11]\n",
    "\n",
    "        NFLA = CFLA * VF; NALA = CALA * VA; NBLA = CBLA * VB\n",
    "        NFK = CFK * VF; NAK = CAK * VA; NBK = CBK * VB\n",
    "\n",
    "        dVA = mlp_output[...,0]\n",
    "        dVB = mlp_output[...,1]\n",
    "        dNALA = mlp_output[...,2]\n",
    "        dNBLA = mlp_output[...,3]\n",
    "        dNAK = mlp_output[...,4]\n",
    "        dNBK = mlp_output[...,5]\n",
    "\n",
    "        nVF = VF - dVA - dVB; nVA = VA + dVA; nVB = VB + dVB\n",
    "        nNFLA = NFLA - dNALA - dNBLA; nNALA = NALA + dNALA; nNBLA = NBLA + dNBLA\n",
    "        nNFK = NFK - dNAK - dNBK; nNAK = NAK + dNAK; nNBK = NBK + dNBK\n",
    "        \n",
    "        # 부피 clamp 후 농도 계산 (division by zero 방지)\n",
    "        nVF = torch.clamp(nVF, min=1e-8)\n",
    "        nVA = torch.clamp(nVA, min=1e-8)\n",
    "        nVB = torch.clamp(nVB, min=1e-8)\n",
    "        \n",
    "        nCFLA = nNFLA / nVF; nCALA = nNALA / nVA; nCBLA = nNBLA / nVB\n",
    "        nCFK = nNFK / nVF; nCAK = nNAK / nVA; nCBK = nNBK / nVB\n",
    "\n",
    "        nCFLA = torch.clamp(nCFLA, min=0)\n",
    "        nCALA = torch.clamp(nCALA, min=0)\n",
    "        nCBLA = torch.clamp(nCBLA, min=0)\n",
    "        nCFK = torch.clamp(nCFK, min=0)\n",
    "        nCAK = torch.clamp(nCAK, min=0)\n",
    "        nCBK = torch.clamp(nCBK, min=0)\n",
    "        nI = mlp_output[...,6]\n",
    "\n",
    "        new_state = torch.cat([V, E, nVF, nVA, nVB, nCFLA, nCALA, nCBLA, nCFK, nCAK, nCBK, nI], dim=-1)\n",
    "        \n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDSeq2SeqModel(nn.Module):\n",
    "    def __init__(self, lstm_params, mlp_params):\n",
    "        super().__init__()\n",
    "        self.lstm_encoder = LSTMEncoder(**lstm_params)\n",
    "        self.mlp_decoder = MLPDecoder(**mlp_params)\n",
    "        self.mass_balance_layer = StateUpdateLayer()\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        # Teacher Forcing: 전체 시퀀스로 다음 상태들 예측\n",
    "        \n",
    "        # LSTM으로 시계열 패턴 학습\n",
    "        lstm_out = self.lstm_encoder(x, seq_len)\n",
    "        \n",
    "        # MLP로 상태 변화량 예측\n",
    "        mlp_out = self.mlp_decoder(lstm_out)\n",
    "        \n",
    "        # 물리적 제약 조건 적용하여 다음 상태 계산\n",
    "        next_states = self.mass_balance_layer(mlp_out, x)\n",
    "        \n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824ead56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def masked_mse_loss(predictions, targets, seq_lengths):\n",
    "    \"\"\"\n",
    "    패딩된 시퀀스에 대한 마스크 적용 MSE 손실 함수\n",
    "    \n",
    "    Args:\n",
    "        predictions: 모델 예측값 [batch_size, seq_len, features]\n",
    "        targets: 실제 타겟값 [batch_size, seq_len, features]  \n",
    "        seq_lengths: 각 시퀀스의 실제 길이 [batch_size]\n",
    "    \n",
    "    Returns:\n",
    "        masked_loss: 패딩 부분을 제외한 평균 MSE 손실\n",
    "    \"\"\"\n",
    "    batch_size, max_len, features = predictions.shape\n",
    "    \n",
    "    # 마스크 생성: 실제 시퀀스 길이만큼만 True\n",
    "    mask = torch.arange(max_len)[None, :] < seq_lengths[:, None]\n",
    "    mask = mask.float().to(predictions.device)\n",
    "    \n",
    "    # 각 요소별 MSE 계산 (reduction='none')\n",
    "    loss = F.mse_loss(predictions, targets, reduction='none')\n",
    "    \n",
    "    # 마스크 적용하여 패딩 부분 제거\n",
    "    masked_loss = (loss * mask.unsqueeze(-1)).sum() / (mask.sum() * features)\n",
    "    \n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8315b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_teacher_forcing_data(input_sequences, seq_lengths):\n",
    "    \"\"\"\n",
    "    Teacher Forcing을 위한 입력-타겟 데이터 준비\n",
    "    \n",
    "    Args:\n",
    "        input_sequences: 전체 시퀀스 [batch_size, seq_len, features]\n",
    "        seq_lengths: 각 시퀀스의 실제 길이 [batch_size]\n",
    "    \n",
    "    Returns:\n",
    "        inputs: [t0, t1, ..., t_{n-1}] 현재 상태들\n",
    "        targets: [t1, t2, ..., t_n] 다음 상태들  \n",
    "        target_seq_lengths: 타겟 시퀀스 길이 (1씩 감소)\n",
    "    \"\"\"\n",
    "    # 입력: 마지막 시점 제외 [:-1]\n",
    "    inputs = input_sequences[:, :-1, :]\n",
    "    \n",
    "    # 타겟: 첫 번째 시점 제외 [1:]  \n",
    "    targets = input_sequences[:, 1:, :]\n",
    "    \n",
    "    # 타겟 시퀀스 길이는 1씩 감소 (마지막 시점 예측 불가)\n",
    "    target_seq_lengths = torch.clamp(seq_lengths - 1, min=1)\n",
    "    \n",
    "    return inputs, targets, target_seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90952eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0e6889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n",
      "Fold 1: Train size = 31, Val size = 8\n",
      "Fold 2: Train size = 31, Val size = 8\n",
      "Fold 3: Train size = 31, Val size = 8\n",
      "Fold 4: Train size = 31, Val size = 8\n",
      "Fold 5: Train size = 32, Val size = 7\n",
      "\n",
      "Created 5 fold dataloaders\n",
      "Each fold contains (train_loader, val_loader) tuple\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('BMED_DATA_AG.csv')\n",
    "feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']\n",
    "ndf = norm_data('BMED_DATA_AG.csv')\n",
    "seq = seq_data_const(ndf)\n",
    "pad_seq,seq_len,max_seq_len = padded_sequences(seq)\n",
    "dataset = gen_dataset(pad_seq, seq_len)\n",
    "dataloaders = kfold_dataloaders(dataset, k_folds=5, batch_size=8, random_state=42)\n",
    "print(f\"\\nCreated {len(dataloaders)} fold dataloaders\")\n",
    "print(f\"Each fold contains (train_loader, val_loader) tuple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7582a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1oezb5wqsgv",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733vckgi2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
