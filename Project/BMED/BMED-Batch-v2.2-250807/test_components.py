#!/usr/bin/env python3
"""
BMED ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì „ì— ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

class LSTMEncoder(nn.Module):
    """
    ê°œì„ ëœ LSTM ì¸ì½”ë” - device í˜¸í™˜ì„± ë° ì•ˆì •ì„± ê°•í™”
    """
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM layer with improved error handling
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers, 
            batch_first=True, dropout=dropout if num_layers > 1 else 0
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, seq_len):
        """
        Forward pass with robust device handling
        """
        # ì…ë ¥ ê²€ì¦
        if x.size(0) != seq_len.size(0):
            raise ValueError(f"Batch size mismatch: input {x.size(0)} vs seq_len {seq_len.size(0)}")
        
        # seq_lenì„ CPUë¡œ ì´ë™í•˜ê³  ì •ë ¬
        seq_len_cpu = seq_len.detach().cpu().long()
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 0ì¸ ê²½ìš° ì²˜ë¦¬
        if (seq_len_cpu <= 0).any():
            seq_len_cpu = torch.clamp(seq_len_cpu, min=1)
        
        try:
            # íŒ¨ë”©ëœ ì‹œí€€ìŠ¤ë¥¼ packí•˜ì—¬ íš¨ìœ¨ì  ì²˜ë¦¬
            packed_input = pack_padded_sequence(
                x, seq_len_cpu, batch_first=True, enforce_sorted=False
            )
            packed_output, (hidden, cell) = self.lstm(packed_input)
            
            # ë‹¤ì‹œ íŒ¨ë”©ëœ í˜•íƒœë¡œ ë³µì› (ì›ë˜ max_len ê¸¸ì´ë¡œ)
            lstm_out, output_lengths = pad_packed_sequence(
                packed_output, batch_first=True, total_length=x.size(1)
            )
            
        except Exception as e:
            # Pack/unpack ì‹¤íŒ¨ì‹œ ì¼ë°˜ LSTM forward pass ì‚¬ìš©
            print(f"Warning: Pack/unpack failed ({e}), using standard LSTM forward")
            lstm_out, (hidden, cell) = self.lstm(x)
        
        # Normalization and dropout
        normalized = self.layer_norm(lstm_out)
        return self.dropout(normalized)

class MLPDecoder(nn.Module):
    def __init__(self, hidden_size, output_size, num_layers=2, num_nodes=None, dropout = 0.3):
        super().__init__()

        if num_nodes is None:
            num_nodes = hidden_size
        
        self.layers = nn.ModuleList()

        # ì²« ë²ˆì§¸ ë ˆì´ì–´: hidden_size â†’ num_nodes
        self.layers.append(nn.Linear(hidden_size, num_nodes))
        self.layers.append(nn.LayerNorm(num_nodes))
        self.layers.append(nn.ReLU())
        self.layers.append(nn.Dropout(dropout))

        # ì¤‘ê°„ ì€ë‹‰ì¸µë“¤: num_nodes â†’ num_nodes
        for i in range(num_layers - 1):
            self.layers.append(nn.Linear(num_nodes,num_nodes))
            self.layers.append(nn.LayerNorm(num_nodes))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))

        # ë§ˆì§€ë§‰ ì¶œë ¥ì¸µ: num_nodes â†’ output_size
        self.layers.append(nn.Linear(num_nodes, output_size))

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

class StateUpdateLayer(nn.Module):
    """
    ê°œì„ ëœ ìƒíƒœ ì—…ë°ì´íŠ¸ ë ˆì´ì–´ - ë¬¼ë¦¬ì  ì œì•½ ì¡°ê±´ ì ìš©
    Bipolar membrane electrodialysis ë¬¼ë¦¬ ë²•ì¹™ ê¸°ë°˜
    """
    def __init__(self, eps=1e-8):
        super().__init__()
        self.eps = eps  # division by zero ë°©ì§€
        
    def forward(self, mlp_output, cur_state):
        """
        ë¬¼ë¦¬ì  ì œì•½ ì¡°ê±´ì„ ì ìš©í•˜ì—¬ ë‹¤ìŒ ìƒíƒœ ê³„ì‚°
        
        Args:
            mlp_output: [batch, seq, 7] - [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI]
            cur_state: [batch, seq, 12] - í˜„ì¬ ìƒíƒœ
        """
        # ì…ë ¥ ì°¨ì› ê²€ì¦
        if mlp_output.dim() != cur_state.dim():
            raise ValueError(f"Dimension mismatch: mlp_output {mlp_output.shape} vs cur_state {cur_state.shape}")
        
        if cur_state.size(-1) != 12:
            raise ValueError(f"Expected 12 state features, got {cur_state.size(-1)}")
            
        if mlp_output.size(-1) != 7:
            raise ValueError(f"Expected 7 MLP outputs, got {mlp_output.size(-1)}")
        
        # í˜„ì¬ ìƒíƒœ ë³€ìˆ˜ ì¶”ì¶œ (ì°¨ì› ìœ ì§€)
        V = cur_state[..., 0:1]     # ì „ì•• (ê³ ì •ê°’)
        E = cur_state[..., 1:2]     # ì™¸ë¶€ ì „í•´ì§ˆ ë†ë„ (ê³ ì •ê°’)
        VF = cur_state[..., 2:3]    # Feed ë¶€í”¼
        VA = cur_state[..., 3:4]    # Acid ë¶€í”¼
        VB = cur_state[..., 4:5]    # Base ë¶€í”¼
        CFLA = cur_state[..., 5:6]  # Feed LA ë†ë„
        CALA = cur_state[..., 6:7]  # Acid LA ë†ë„
        CBLA = cur_state[..., 7:8]  # Base LA ë†ë„
        CFK = cur_state[..., 8:9]   # Feed K ë†ë„
        CAK = cur_state[..., 9:10]  # Acid K ë†ë„
        CBK = cur_state[..., 10:11] # Base K ë†ë„
        I = cur_state[..., 11:12]   # ì „ë¥˜

        # ë¬¼ì§ˆëŸ‰ ê³„ì‚° (ë†ë„ Ã— ë¶€í”¼)
        NFLA = CFLA * VF; NALA = CALA * VA; NBLA = CBLA * VB
        NFK = CFK * VF; NAK = CAK * VA; NBK = CBK * VB

        # MLP ì¶œë ¥ (ë³€í™”ëŸ‰)
        dVA = mlp_output[..., 0:1]    # Acid ë¶€í”¼ ë³€í™”ëŸ‰
        dVB = mlp_output[..., 1:2]    # Base ë¶€í”¼ ë³€í™”ëŸ‰
        dNALA = mlp_output[..., 2:3]  # Acid LA ë¬¼ì§ˆëŸ‰ ë³€í™”ëŸ‰
        dNBLA = mlp_output[..., 3:4]  # Base LA ë¬¼ì§ˆëŸ‰ ë³€í™”ëŸ‰
        dNAK = mlp_output[..., 4:5]   # Acid K ë¬¼ì§ˆëŸ‰ ë³€í™”ëŸ‰
        dNBK = mlp_output[..., 5:6]   # Base K ë¬¼ì§ˆëŸ‰ ë³€í™”ëŸ‰
        nI = mlp_output[..., 6:7]     # ìƒˆë¡œìš´ ì „ë¥˜ê°’

        # ìƒˆë¡œìš´ ë¶€í”¼ ê³„ì‚° (ë¬¼ì§ˆ ë³´ì¡´ ë²•ì¹™)
        nVF = VF - dVA - dVB  # Feedì—ì„œ ë¹ ì ¸ë‚˜ê°„ ë¶€í”¼
        nVA = VA + dVA       # Acidë¡œ ë“¤ì–´ì˜¨ ë¶€í”¼
        nVB = VB + dVB       # Baseë¡œ ë“¤ì–´ì˜¨ ë¶€í”¼
        
        # ìƒˆë¡œìš´ ë¬¼ì§ˆëŸ‰ ê³„ì‚° (ë¬¼ì§ˆ ë³´ì¡´ ë²•ì¹™)
        nNFLA = NFLA - dNALA - dNBLA  # Feedì—ì„œ ë¹ ì ¸ë‚˜ê°„ LA
        nNALA = NALA + dNALA          # Acidë¡œ ë“¤ì–´ì˜¨ LA
        nNBLA = NBLA + dNBLA          # Baseë¡œ ë“¤ì–´ì˜¨ LA
        nNFK = NFK - dNAK - dNBK      # Feedì—ì„œ ë¹ ì ¸ë‚˜ê°„ K
        nNAK = NAK + dNAK             # Acidë¡œ ë“¤ì–´ì˜¨ K
        nNBK = NBK + dNBK             # Baseë¡œ ë“¤ì–´ì˜¨ K
        
        # ë¶€í”¼ ì œì•½ ì¡°ê±´ ì ìš© (ë¬¼ë¦¬ì ìœ¼ë¡œ ì–‘ìˆ˜ ìœ ì§€)
        nVF = torch.clamp(nVF, min=self.eps)
        nVA = torch.clamp(nVA, min=self.eps)
        nVB = torch.clamp(nVB, min=self.eps)
        
        # ìƒˆë¡œìš´ ë†ë„ ê³„ì‚° (ë†ë„ = ë¬¼ì§ˆëŸ‰ / ë¶€í”¼)
        nCFLA = torch.clamp(nNFLA / nVF, min=0)
        nCALA = torch.clamp(nNALA / nVA, min=0)
        nCBLA = torch.clamp(nNBLA / nVB, min=0)
        nCFK = torch.clamp(nNFK / nVF, min=0)
        nCAK = torch.clamp(nNAK / nVA, min=0)
        nCBK = torch.clamp(nNBK / nVB, min=0)
        
        # ì „ë¥˜ëŠ” ì–‘ìˆ˜ ì œì•½
        nI = torch.clamp(nI, min=0)

        # ìƒˆë¡œìš´ ìƒíƒœ ì¡°ë¦½ (V, EëŠ” ê³ ì •ê°’ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€)
        new_state = torch.cat([
            V, E,  # ê³ ì •ê°’: ì „ì••, ì™¸ë¶€ ì „í•´ì§ˆ ë†ë„
            nVF, nVA, nVB,  # ìƒˆë¡œìš´ ë¶€í”¼
            nCFLA, nCALA, nCBLA,  # ìƒˆë¡œìš´ LA ë†ë„
            nCFK, nCAK, nCBK,     # ìƒˆë¡œìš´ K ë†ë„
            nI  # ìƒˆë¡œìš´ ì „ë¥˜
        ], dim=-1)
        
        return new_state

class BMEDSeq2SeqModel(nn.Module):
    def __init__(self, lstm_params, mlp_params):
        super().__init__()
        self.lstm_encoder = LSTMEncoder(**lstm_params)
        self.mlp_decoder = MLPDecoder(**mlp_params)
        self.mass_balance_layer = StateUpdateLayer()

    def forward(self, x, seq_len):
        # Teacher Forcing: ì „ì²´ ì‹œí€€ìŠ¤ë¡œ ë‹¤ìŒ ìƒíƒœë“¤ ì˜ˆì¸¡
        
        # LSTMìœ¼ë¡œ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
        lstm_out = self.lstm_encoder(x, seq_len)
        
        # MLPë¡œ ìƒíƒœ ë³€í™”ëŸ‰ ì˜ˆì¸¡
        mlp_out = self.mlp_decoder(lstm_out)
        
        # ë¬¼ë¦¬ì  ì œì•½ ì¡°ê±´ ì ìš©í•˜ì—¬ ë‹¤ìŒ ìƒíƒœ ê³„ì‚°
        next_states = self.mass_balance_layer(mlp_out, x)
        
        return next_states

def masked_mse_loss(predictions, targets, seq_lengths):
    """
    ê°œì„ ëœ ë§ˆìŠ¤í‚¹ëœ MSE ì†ì‹¤ í•¨ìˆ˜ - device í˜¸í™˜ì„± ë° ì•ˆì •ì„± ê°•í™”
    """
    # ì…ë ¥ ê²€ì¦
    if predictions.shape != targets.shape:
        raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
    
    if predictions.size(0) != seq_lengths.size(0):
        raise ValueError(f"Batch size mismatch: predictions {predictions.size(0)} vs seq_lengths {seq_lengths.size(0)}")
    
    batch_size, max_len, features = predictions.shape
    
    # seq_lengthsë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ arangeì™€ í˜¸í™˜ë˜ë„ë¡ ì²˜ë¦¬
    seq_lengths_cpu = seq_lengths.detach().cpu().long()
    
    # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 0 ì´í•˜ì¸ ê²½ìš° ì²˜ë¦¬
    seq_lengths_cpu = torch.clamp(seq_lengths_cpu, min=1, max=max_len)
    
    # ë§ˆìŠ¤í¬ ìƒì„±: ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ë§Œ True
    mask = torch.arange(max_len, device='cpu')[None, :] < seq_lengths_cpu[:, None]
    mask = mask.float().to(predictions.device)
    
    # ê° ìš”ì†Œë³„ MSE ê³„ì‚° (reduction='none')
    loss = F.mse_loss(predictions, targets, reduction='none')
    
    # ë§ˆìŠ¤í¬ ì ìš©í•˜ì—¬ íŒ¨ë”© ë¶€ë¶„ ì œê±°
    masked_loss_sum = (loss * mask.unsqueeze(-1)).sum()
    valid_elements = mask.sum() * features
    
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    if valid_elements == 0:
        return torch.tensor(0.0, device=predictions.device, requires_grad=True)
    
    masked_loss = masked_loss_sum / valid_elements
    
    return masked_loss

def prepare_teacher_forcing_data(input_sequences, seq_lengths):
    """
    Teacher Forcingì„ ìœ„í•œ ì…ë ¥-íƒ€ê²Ÿ ë°ì´í„° ì¤€ë¹„
    """
    # ì…ë ¥: ë§ˆì§€ë§‰ ì‹œì  ì œì™¸ [:-1]
    inputs = input_sequences[:, :-1, :]
    
    # íƒ€ê²Ÿ: ì²« ë²ˆì§¸ ì‹œì  ì œì™¸ [1:]  
    targets = input_sequences[:, 1:, :]
    
    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” 1ì”© ê°ì†Œ (ë§ˆì§€ë§‰ ì‹œì  ì˜ˆì¸¡ ë¶ˆê°€)
    target_seq_lengths = torch.clamp(seq_lengths - 1, min=1)
    
    return inputs, targets, target_seq_lengths

def test_model_components():
    """ëª¨ë¸ì˜ ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Testing on device: {device}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size = 2
    seq_len = 5
    input_size = 12
    
    # ê°€ì§œ ì…ë ¥ ë°ì´í„° ìƒì„±
    test_input = torch.randn(batch_size, seq_len, input_size).to(device)
    test_seq_len = torch.tensor([4, 3]).to(device)  # ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´
    
    print(f"Test input shape: {test_input.shape}")
    print(f"Test seq_len: {test_seq_len}")
    
    try:
        # 1. LSTM Encoder í…ŒìŠ¤íŠ¸
        print("\n=== Testing LSTM Encoder ===")
        lstm_encoder = LSTMEncoder(input_size=12, hidden_size=64, num_layers=2).to(device)
        lstm_out = lstm_encoder(test_input, test_seq_len)
        print(f"LSTM output shape: {lstm_out.shape}")
        
        # 2. MLP Decoder í…ŒìŠ¤íŠ¸
        print("\n=== Testing MLP Decoder ===")
        mlp_decoder = MLPDecoder(hidden_size=64, output_size=7, num_layers=2).to(device)
        mlp_out = mlp_decoder(lstm_out)
        print(f"MLP output shape: {mlp_out.shape}")
        
        # 3. StateUpdateLayer í…ŒìŠ¤íŠ¸
        print("\n=== Testing StateUpdateLayer ===")
        state_update = StateUpdateLayer().to(device)
        # StateUpdateLayerëŠ” MLP ì¶œë ¥ê³¼ ê°™ì€ ì°¨ì›ì˜ í˜„ì¬ ìƒíƒœê°€ í•„ìš”
        new_state = state_update(mlp_out, test_input)
        print(f"Updated state shape: {new_state.shape}")
        
        # 4. ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("\n=== Testing Full Model ===")
        model_params = {
            'lstm': {'input_size': 12, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2},
            'mlp': {'hidden_size': 64, 'output_size': 7, 'num_layers': 2, 'num_nodes': 128, 'dropout': 0.3}
        }
        full_model = BMEDSeq2SeqModel(model_params['lstm'], model_params['mlp']).to(device)
        
        # Teacher forcing ë°ì´í„° ì¤€ë¹„
        inputs, targets, target_seq_lengths = prepare_teacher_forcing_data(test_input, test_seq_len)
        print(f"Teacher forcing - inputs: {inputs.shape}, targets: {targets.shape}")
        
        # Forward pass
        predictions = full_model(inputs, target_seq_lengths)
        print(f"Model predictions shape: {predictions.shape}")
        
        # Loss ê³„ì‚° í…ŒìŠ¤íŠ¸
        loss = masked_mse_loss(predictions, targets, target_seq_lengths)
        print(f"Loss value: {loss.item()}")
        
        print("\nâœ… All components work correctly!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Error occurred: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ”§ BMED ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("="*60)
    
    test_result = test_model_components()
    
    print("="*60)
    if test_result:
        print("ğŸ‰ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:")
        print("1. Jupyter ë…¸íŠ¸ë¶ì—ì„œ ì „ì²´ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰")
        print("2. ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ")
        print("3. ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„")
        print("\nğŸ’¡ ë…¸íŠ¸ë¶ì˜ ë§ˆì§€ë§‰ ì…€ë“¤ì„ ì°¨ë¡€ëŒ€ë¡œ ì‹¤í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.")
    else:
        print("âš ï¸ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”.")