{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7299bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "import math\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca099116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'Using device: {device}')\n",
    "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    else:\n",
    "        print(f'Using device: {device}')\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7439571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(name):\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CBLA': {'min':0, 'max': df['CBLA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CAK': {'min':0, 'max': df['CAK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ef616b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_const(ndf):\n",
    "    sequences = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']\n",
    "    \n",
    "    for exp in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp].sort_values(by='t')\n",
    "        sequences.append(exp_data[feature_cols].values)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a9233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequences(sequences):\n",
    "    max_seq_len = max([len(seq) for seq in sequences])\n",
    "    seq_len = [len(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=-1)\n",
    "\n",
    "    return padded_sequences, seq_len, max_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f52012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(pad_seq, seq_len):\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e82b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_dataloaders(dataset, k_folds=5, batch_size=8, random_state=87):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "    dataloaders = []\n",
    "    batch_size = math.ceil(len(dataset)/k_folds)\n",
    "    \n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(range(len(dataset)))):\n",
    "        print(f\"Fold {fold + 1}: Train size = {len(train_indices)}, Val size = {len(val_indices)}\")\n",
    "        \n",
    "        # Create subsets for train and validation\n",
    "        train_subset = Subset(dataset, train_indices)\n",
    "        val_subset = Subset(dataset, val_indices)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        dataloaders.append((train_loader, val_loader))\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16dd4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialStateExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    BMED 시스템의 시계열 패턴에서 숨겨진 dynamics를 추출하는 LSTM 기반 모듈\n",
    "    각 시점의 hidden state에는 해당 시점까지의 모든 과거 정보가 누적됨\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer with improved error handling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, \n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"\n",
    "        시계열 상태 시퀀스를 처리하여 각 시점의 hidden state 추출\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, input_size] - BMED 시스템 상태 시퀀스\n",
    "            seq_len: [batch_size] - 각 시퀀스의 실제 길이\n",
    "            \n",
    "        Returns:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size] - 각 시점의 누적된 hidden state\n",
    "        \"\"\"\n",
    "        \n",
    "        # 입력 검증\n",
    "        if x.size(0) != seq_len.size(0):\n",
    "            raise ValueError(f\"Batch size mismatch: input {x.size(0)} vs seq_len {seq_len.size(0)}\")\n",
    "        \n",
    "        # seq_len을 CPU로 이동하고 정수형으로 변환\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        \n",
    "        # 시퀀스 길이 유효성 검사\n",
    "        if (seq_len_cpu <= 0).any():\n",
    "            invalid_lengths = seq_len_cpu[seq_len_cpu <= 0]\n",
    "            raise ValueError(f\"Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths must be positive.\")\n",
    "        \n",
    "        # 패딩된 시퀀스를 pack하여 효율적 처리\n",
    "        packed_input = pack_padded_sequence(\n",
    "            x, seq_len_cpu, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "        \n",
    "        # 다시 패딩된 형태로 복원\n",
    "        lstm_out, output_lengths = pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=x.size(1)\n",
    "        )\n",
    "        \n",
    "        # Normalization and dropout\n",
    "        normalized = self.layer_norm(lstm_out)\n",
    "        return self.dropout(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "084b3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Hidden state로부터 BMED 시스템의 물리적 변화량과 새로운 전류값을 디코딩하는 MLP\n",
    "    출력: [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI] - 7개 물리적 변화량\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, num_layers=2, num_nodes=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_nodes is None:\n",
    "            num_nodes = hidden_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # 첫 번째 레이어: hidden_size → num_nodes\n",
    "        self.layers.append(nn.Linear(hidden_size, num_nodes))\n",
    "        self.layers.append(nn.LayerNorm(num_nodes))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # 중간 은닉층들: num_nodes → num_nodes\n",
    "        for i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_nodes, num_nodes))\n",
    "            self.layers.append(nn.LayerNorm(num_nodes))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # 마지막 출력층: num_nodes → output_size (7개 물리적 변화량)\n",
    "        self.layers.append(nn.Linear(num_nodes, output_size))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Hidden state를 물리적 변화량으로 디코딩\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size] - 시점별 hidden state\n",
    "            \n",
    "        Returns:\n",
    "            physical_changes: [batch_size, seq_len, 7] - 물리적 변화량\n",
    "                [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI]\n",
    "        \"\"\"\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ea1ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    물리적 변화량을 실제 시스템 상태로 변환하면서 물리적 제약 조건을 적용\n",
    "    Bipolar membrane electrodialysis 시스템의 물리 법칙 기반 상태 업데이트\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # division by zero 방지\n",
    "        \n",
    "    def forward(self, physical_changes, current_state):\n",
    "        \"\"\"\n",
    "        물리적 변화량을 현재 상태에 적용하여 다음 상태 계산\n",
    "        \n",
    "        Args:\n",
    "            physical_changes: [batch, seq, 7] - [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI]\n",
    "            current_state: [batch, seq, 12] - 현재 BMED 시스템 상태\n",
    "                V: 전압 (Voltage) - 실험 세트별 고정값\n",
    "                E: 외부 전해질 농도 (External electrolyte concentration) - 실험 세트별 고정값  \n",
    "                VF, VA, VB: Feed, Acid, Base 부피\n",
    "                CFLA, CALA, CBLA: Feed, Acid, Base의 LA 농도\n",
    "                CFK, CAK, CBK: Feed, Acid, Base의 K 농도\n",
    "                I: 전류\n",
    "                \n",
    "        Returns:\n",
    "            next_state: [batch, seq, 12] - 물리 제약이 적용된 다음 상태\n",
    "        \"\"\"\n",
    "        # 입력 차원 검증\n",
    "        if physical_changes.dim() != current_state.dim():\n",
    "            raise ValueError(f\"Dimension mismatch: physical_changes {physical_changes.shape} vs current_state {current_state.shape}\")\n",
    "        \n",
    "        if current_state.size(-1) != 12:\n",
    "            raise ValueError(f\"Expected 12 state features, got {current_state.size(-1)}\")\n",
    "            \n",
    "        if physical_changes.size(-1) != 7:\n",
    "            raise ValueError(f\"Expected 7 physical changes, got {physical_changes.size(-1)}\")\n",
    "        \n",
    "        # 현재 상태 변수 추출 (차원 유지)\n",
    "        V = current_state[..., 0:1]     # 전압 (고정값)\n",
    "        E = current_state[..., 1:2]     # 외부 전해질 농도 (고정값)\n",
    "        VF = current_state[..., 2:3]    # Feed 부피\n",
    "        VA = current_state[..., 3:4]    # Acid 부피\n",
    "        VB = current_state[..., 4:5]    # Base 부피\n",
    "        CFLA = current_state[..., 5:6]  # Feed LA 농도\n",
    "        CALA = current_state[..., 6:7]  # Acid LA 농도\n",
    "        CBLA = current_state[..., 7:8]  # Base LA 농도\n",
    "        CFK = current_state[..., 8:9]   # Feed K 농도\n",
    "        CAK = current_state[..., 9:10]  # Acid K 농도\n",
    "        CBK = current_state[..., 10:11] # Base K 농도\n",
    "        I = current_state[..., 11:12]   # 전류\n",
    "\n",
    "        # 물질량 계산 (농도 × 부피)\n",
    "        NFLA = CFLA * VF; NALA = CALA * VA; NBLA = CBLA * VB\n",
    "        NFK = CFK * VF; NAK = CAK * VA; NBK = CBK * VB\n",
    "\n",
    "        # 물리적 변화량 추출\n",
    "        dVA = physical_changes[..., 0:1]    # Acid 부피 변화량 (양방향 가능: 음수면 A→F)\n",
    "        dVB = physical_changes[..., 1:2]    # Base 부피 변화량 (양방향 가능: 음수면 B→F)\n",
    "        dNALA = physical_changes[..., 2:3]  # Acid LA 물질량 변화량 (일방향: F→A만)\n",
    "        dNBLA = physical_changes[..., 3:4]  # Base LA 물질량 변화량 (일방향: F→B만)\n",
    "        dNAK = physical_changes[..., 4:5]   # Acid K 물질량 변화량 (일방향: F→A만)\n",
    "        dNBK = physical_changes[..., 5:6]   # Base K 물질량 변화량 (일방향: F→B만)\n",
    "        nI = physical_changes[..., 6:7]     # 새로운 전류값\n",
    "\n",
    "        # 새로운 부피 계산 (양방향 흐름 허용)\n",
    "        nVF = VF - dVA - dVB  # dVA, dVB가 음수면 F로 역유입\n",
    "        nVA = VA + dVA        # dVA가 음수면 A에서 F로 유출\n",
    "        nVB = VB + dVB        # dVB가 음수면 B에서 F로 유출\n",
    "        \n",
    "        # 물질 이동량을 일방향으로 제한 (F→A, F→B만 허용)\n",
    "        dNALA_clipped = torch.clamp(dNALA, min=0)  # 음수 제거 (역방향 불가)\n",
    "        dNBLA_clipped = torch.clamp(dNBLA, min=0)\n",
    "        dNAK_clipped = torch.clamp(dNAK, min=0)\n",
    "        dNBK_clipped = torch.clamp(dNBK, min=0)\n",
    "        \n",
    "        # 새로운 물질량 계산 (일방향 이동만)\n",
    "        nNFLA = NFLA - dNALA_clipped - dNBLA_clipped  # Feed에서 유출만\n",
    "        nNALA = NALA + dNALA_clipped                  # Acid로 유입만\n",
    "        nNBLA = NBLA + dNBLA_clipped                  # Base로 유입만\n",
    "        nNFK = NFK - dNAK_clipped - dNBK_clipped      # K도 마찬가지\n",
    "        nNAK = NAK + dNAK_clipped\n",
    "        nNBK = NBK + dNBK_clipped\n",
    "        \n",
    "        # 물리적 제약 조건 적용 (양수 유지)\n",
    "        nVF = torch.clamp(nVF, min=self.eps)\n",
    "        nVA = torch.clamp(nVA, min=self.eps)\n",
    "        nVB = torch.clamp(nVB, min=self.eps)\n",
    "        \n",
    "        # 물질량 음수 방지\n",
    "        nNFLA = torch.clamp(nNFLA, min=0)\n",
    "        nNALA = torch.clamp(nNALA, min=0)\n",
    "        nNBLA = torch.clamp(nNBLA, min=0)\n",
    "        nNFK = torch.clamp(nNFK, min=0)\n",
    "        nNAK = torch.clamp(nNAK, min=0)\n",
    "        nNBK = torch.clamp(nNBK, min=0)\n",
    "        \n",
    "        # 새로운 농도 계산 (농도 = 물질량 / 부피)\n",
    "        nCFLA = nNFLA / nVF\n",
    "        nCALA = nNALA / nVA\n",
    "        nCBLA = nNBLA / nVB\n",
    "        nCFK = nNFK / nVF\n",
    "        nCAK = nNAK / nVA\n",
    "        nCBK = nNBK / nVB\n",
    "        \n",
    "        # 전류는 양수 제약\n",
    "        nI = torch.clamp(nI, min=0)\n",
    "\n",
    "        # 새로운 상태 조립 (V, E는 고정값이므로 그대로 유지)\n",
    "        next_state = torch.cat([\n",
    "            V, E,  # 고정값: 전압, 외부 전해질 농도\n",
    "            nVF, nVA, nVB,  # 새로운 부피 (양방향 흐름 반영)\n",
    "            nCFLA, nCALA, nCBLA,  # 새로운 LA 농도\n",
    "            nCFK, nCAK, nCBK,     # 새로운 K 농도\n",
    "            nI  # 새로운 전류\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c9d4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BMED 시스템의 시계열 상태 예측을 위한 자기회귀 모델\n",
    "    \n",
    "    구조:\n",
    "    1. SequentialStateExtractor: LSTM으로 시계열 패턴의 hidden state 추출\n",
    "    2. PhysicalChangeDecoder: Hidden state를 물리적 변화량으로 디코딩  \n",
    "    3. PhysicsConstraintLayer: 물리 법칙 적용하여 다음 상태 계산\n",
    "    \"\"\"\n",
    "    def __init__(self, state_extractor_params, decoder_params):\n",
    "        super().__init__()\n",
    "        self.state_extractor = SequentialStateExtractor(**state_extractor_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer()\n",
    "\n",
    "    def forward(self, current_states, seq_lengths):\n",
    "        \"\"\"\n",
    "        현재 시점까지의 상태들로부터 다음 상태들 예측\n",
    "        \n",
    "        Args:\n",
    "            current_states: [batch, seq_len, 12] - 현재까지의 BMED 시스템 상태들\n",
    "            seq_lengths: [batch] - 각 시퀀스의 실제 길이\n",
    "            \n",
    "        Returns:\n",
    "            next_states: [batch, seq_len, 12] - 예측된 다음 시점 상태들\n",
    "        \"\"\"\n",
    "        # 1. LSTM으로 각 시점의 hidden state 추출 (과거 정보 누적)\n",
    "        hidden_states = self.state_extractor(current_states, seq_lengths)\n",
    "        \n",
    "        # 2. Hidden state를 물리적 변화량으로 디코딩\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        \n",
    "        # 3. 물리적 제약 조건을 적용하여 다음 상태 계산\n",
    "        next_states = self.physics_constraint(physical_changes, current_states)\n",
    "        \n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "824ead56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(predictions, targets, seq_lengths):\n",
    "    \"\"\"\n",
    "    개선된 마스킹된 MSE 손실 함수 - device 호환성 및 안정성 강화\n",
    "    \n",
    "    Args:\n",
    "        predictions: 모델 예측값 [batch_size, seq_len, features]\n",
    "        targets: 실제 타겟값 [batch_size, seq_len, features]  \n",
    "        seq_lengths: 각 시퀀스의 실제 길이 [batch_size]\n",
    "    \n",
    "    Returns:\n",
    "        masked_loss: 패딩 부분을 제외한 평균 MSE 손실\n",
    "    \"\"\"\n",
    "    # 입력 검증\n",
    "    if predictions.shape != targets.shape:\n",
    "        raise ValueError(f\"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}\")\n",
    "    \n",
    "    if predictions.size(0) != seq_lengths.size(0):\n",
    "        raise ValueError(f\"Batch size mismatch: predictions {predictions.size(0)} vs seq_lengths {seq_lengths.size(0)}\")\n",
    "    \n",
    "    batch_size, max_len, features = predictions.shape\n",
    "    \n",
    "    # seq_lengths를 CPU로 이동하여 arange와 호환되도록 처리\n",
    "    seq_lengths_cpu = seq_lengths.detach().cpu().long()\n",
    "    \n",
    "    # 시퀀스 길이 유효성 검사 - 데이터 구조 오류는 중단해야 함\n",
    "    if (seq_lengths_cpu <= 0).any():\n",
    "        invalid_lengths = seq_lengths_cpu[seq_lengths_cpu <= 0]\n",
    "        raise ValueError(f\"Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths must be positive.\")\n",
    "    \n",
    "    # 최대 길이 초과 검사\n",
    "    if (seq_lengths_cpu > max_len).any():\n",
    "        invalid_lengths = seq_lengths_cpu[seq_lengths_cpu > max_len]\n",
    "        raise ValueError(f\"Sequence lengths exceed max_len: {invalid_lengths.tolist()} > {max_len}\")\n",
    "    \n",
    "    # 마스크 생성: 실제 시퀀스 길이만큼만 True\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_lengths_cpu[:, None]\n",
    "    mask = mask.float().to(predictions.device)\n",
    "    \n",
    "    # 각 요소별 MSE 계산 (reduction='none')\n",
    "    loss = F.mse_loss(predictions, targets, reduction='none')\n",
    "    \n",
    "    # 마스크 적용하여 패딩 부분 제거\n",
    "    masked_loss_sum = (loss * mask.unsqueeze(-1)).sum()\n",
    "    valid_elements = mask.sum() * features\n",
    "    \n",
    "    # 0으로 나누기 방지\n",
    "    if valid_elements == 0:\n",
    "        raise ValueError(\"No valid elements found after masking. Check sequence lengths and data.\")\n",
    "    \n",
    "    masked_loss = masked_loss_sum / valid_elements\n",
    "    \n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd8315b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_teacher_forcing_data(input_sequences, seq_lengths):\n",
    "    \"\"\"\n",
    "    Teacher Forcing을 위한 입력-타겟 데이터 준비\n",
    "    \n",
    "    Args:\n",
    "        input_sequences: 전체 시퀀스 [batch_size, seq_len, features]\n",
    "        seq_lengths: 각 시퀀스의 실제 길이 [batch_size]\n",
    "    \n",
    "    Returns:\n",
    "        inputs: [t0, t1, ..., t_{n-1}] 현재 상태들\n",
    "        targets: [t1, t2, ..., t_n] 다음 상태들  \n",
    "        target_seq_lengths: 타겟 시퀀스 길이 (1씩 감소)\n",
    "    \"\"\"\n",
    "    # 입력: 마지막 시점 제외 [:-1]\n",
    "    inputs = input_sequences[:, :-1, :]\n",
    "    \n",
    "    # 타겟: 첫 번째 시점 제외 [1:]  \n",
    "    targets = input_sequences[:, 1:, :]\n",
    "    \n",
    "    # **타겟 시퀀스 길이는 1씩 감소 (마지막 시점 예측 불가)**\n",
    "    if (seq_lengths - 1 < 1).any():\n",
    "        invalid_lengths = seq_lengths[seq_lengths - 1 < 1]\n",
    "        raise ValueError(f\"타겟 시퀀스 길이가 0보다 작아질 수 없습니다. 잘못된 seq_lengths: {invalid_lengths.tolist()}\")\n",
    "    target_seq_lengths = seq_lengths - 1\n",
    "    \n",
    "    return inputs, targets, target_seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a097d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (input_seq, seq_lengths) in enumerate(train_loader):\n",
    "        # 데이터를 디바이스로 이동\n",
    "        input_seq = input_seq.to(device)\n",
    "        seq_lengths = seq_lengths.to(device)\n",
    "        \n",
    "        # Teacher Forcing 데이터 준비\n",
    "        inputs, targets, target_seq_lengths = prepare_teacher_forcing_data(input_seq, seq_lengths)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(inputs, target_seq_lengths)\n",
    "        \n",
    "        # Loss 계산 (마스크 적용)\n",
    "        loss = masked_mse_loss(predictions, targets, target_seq_lengths)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_seq, seq_lengths in val_loader:\n",
    "            # 데이터를 디바이스로 이동\n",
    "            input_seq = input_seq.to(device)\n",
    "            seq_lengths = seq_lengths.to(device)\n",
    "            \n",
    "            # Teacher Forcing 데이터 준비\n",
    "            inputs, targets, target_seq_lengths = prepare_teacher_forcing_data(input_seq, seq_lengths)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(inputs, target_seq_lengths)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = masked_mse_loss(predictions, targets, target_seq_lengths)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90952eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDHyperparameterOptimizer:\n",
    "    \"\"\"\n",
    "    BMED 자기회귀 모델을 위한 K-fold CV 기반 하이퍼파라미터 최적화 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloaders, device=None):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 하이퍼파라미터 범위 정의\n",
    "        self.param_ranges = {\n",
    "            'hidden_size': {'low': 16, 'high': 256, 'step': 16},\n",
    "            'num_layers': {'low': 2, 'high': 10},\n",
    "            'extractor_dropout': {'low': 0.1, 'high': 0.5},\n",
    "            'decoder_layers': {'low': 2, 'high': 10},\n",
    "            'decoder_nodes': {'low': 16, 'high': 256, 'step': 16},\n",
    "            'decoder_dropout': {'low': 0.1, 'high': 0.5},\n",
    "            'learning_rate': {'low': 1e-4, 'high': 1e-1, 'log': True},\n",
    "            'weight_decay': {'low': 1e-6, 'high': 1e-3, 'log': True}\n",
    "        }\n",
    "        \n",
    "        # 학습 설정 - 작은 데이터셋에 최적화\n",
    "        self.train_config = {\n",
    "            'epochs': 500,      # 더 많은 기회 제공\n",
    "            'patience': 50,     # 더 관대한 early stopping\n",
    "            'min_epochs': 100    # 충분한 학습 보장\n",
    "        }\n",
    "    \n",
    "    def create_model(self, trial):\n",
    "        \"\"\"하이퍼파라미터 샘플링 및 모델 생성\"\"\"\n",
    "        # 하이퍼파라미터 샘플링\n",
    "        params = {}\n",
    "        params['hidden_size'] = trial.suggest_int('hidden_size', **self.param_ranges['hidden_size'])\n",
    "        params['num_layers'] = trial.suggest_int('num_layers', **self.param_ranges['num_layers'])\n",
    "        params['extractor_dropout'] = trial.suggest_float('extractor_dropout', **self.param_ranges['extractor_dropout'])\n",
    "        params['decoder_layers'] = trial.suggest_int('decoder_layers', **self.param_ranges['decoder_layers'])\n",
    "        params['decoder_nodes'] = trial.suggest_int('decoder_nodes', **self.param_ranges['decoder_nodes'])\n",
    "        params['decoder_dropout'] = trial.suggest_float('decoder_dropout', **self.param_ranges['decoder_dropout'])\n",
    "        params['learning_rate'] = trial.suggest_float('learning_rate', **self.param_ranges['learning_rate'])\n",
    "        params['weight_decay'] = trial.suggest_float('weight_decay', **self.param_ranges['weight_decay'])\n",
    "        \n",
    "        # 모델 파라미터 구성\n",
    "        model_params = {\n",
    "            'state_extractor': {\n",
    "                'input_size': 12,\n",
    "                'hidden_size': params['hidden_size'],\n",
    "                'num_layers': params['num_layers'],\n",
    "                'dropout': params['extractor_dropout']\n",
    "            },\n",
    "            'decoder': {\n",
    "                'hidden_size': params['hidden_size'],\n",
    "                'output_size': 7,\n",
    "                'num_layers': params['decoder_layers'],\n",
    "                'num_nodes': params['decoder_nodes'],\n",
    "                'dropout': params['decoder_dropout']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 옵티마이저 파라미터\n",
    "        optimizer_params = {\n",
    "            'lr': params['learning_rate'],\n",
    "            'weight_decay': params['weight_decay']\n",
    "        }\n",
    "        \n",
    "        return model_params, optimizer_params\n",
    "    \n",
    "    def train_single_fold(self, model_params, optimizer_params, train_loader, val_loader):\n",
    "        \"\"\"단일 fold 학습\"\"\"\n",
    "        try:\n",
    "            # 모델 초기화\n",
    "            model = BMEDAutoregressiveModel(model_params['state_extractor'], model_params['decoder']).to(self.device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), **optimizer_params)\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(self.train_config['epochs']):\n",
    "                # 학습\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                train_batches = 0\n",
    "                \n",
    "                for input_seq, seq_lengths in train_loader:\n",
    "                    try:\n",
    "                        input_seq = input_seq.to(self.device)\n",
    "                        seq_lengths = seq_lengths.to(self.device)\n",
    "                        \n",
    "                        # Teacher forcing 데이터 준비\n",
    "                        inputs, targets, target_seq_lengths = prepare_teacher_forcing_data(input_seq, seq_lengths)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        optimizer.zero_grad()\n",
    "                        predictions = model(inputs, target_seq_lengths)\n",
    "                        loss = masked_mse_loss(predictions, targets, target_seq_lengths)\n",
    "                        \n",
    "                        # Backward pass\n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 그래디언트 클리핑\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        train_loss += loss.item()\n",
    "                        train_batches += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Training batch error: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                if train_batches == 0:\n",
    "                    return float('inf')\n",
    "                \n",
    "                train_loss = train_loss / train_batches\n",
    "                \n",
    "                # 검증\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_batches = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for input_seq, seq_lengths in val_loader:\n",
    "                        try:\n",
    "                            input_seq = input_seq.to(self.device)\n",
    "                            seq_lengths = seq_lengths.to(self.device)\n",
    "                            \n",
    "                            inputs, targets, target_seq_lengths = prepare_teacher_forcing_data(input_seq, seq_lengths)\n",
    "                            predictions = model(inputs, target_seq_lengths)\n",
    "                            loss = masked_mse_loss(predictions, targets, target_seq_lengths)\n",
    "                            \n",
    "                            val_loss += loss.item()\n",
    "                            val_batches += 1\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Validation batch error: {str(e)}\")\n",
    "                            continue\n",
    "                \n",
    "                if val_batches == 0:\n",
    "                    return float('inf')\n",
    "                \n",
    "                val_loss = val_loss / val_batches\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                # 최소 에포크 후 early stopping 적용\n",
    "                if epoch >= self.train_config['min_epochs'] and patience_counter >= self.train_config['patience']:\n",
    "                    break\n",
    "            \n",
    "            return best_val_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fold training error: {str(e)}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective 함수 - 모든 fold 완료 후 평균 반환\"\"\"\n",
    "        try:\n",
    "            # 모델 및 옵티마이저 파라미터 생성\n",
    "            model_params, optimizer_params = self.create_model(trial)\n",
    "            \n",
    "            # K-fold 교차검증 - 모든 fold 실행\n",
    "            fold_losses = []\n",
    "            for fold_idx, (train_loader, val_loader) in enumerate(self.dataloaders):\n",
    "                fold_loss = self.train_single_fold(\n",
    "                    model_params, optimizer_params, \n",
    "                    train_loader, val_loader\n",
    "                )\n",
    "                \n",
    "                if fold_loss == float('inf'):\n",
    "                    return float('inf')\n",
    "                \n",
    "                fold_losses.append(fold_loss)\n",
    "            \n",
    "            # 평균 검증 손실 반환 (모든 fold 완료)\n",
    "            mean_loss = sum(fold_losses) / len(fold_losses)\n",
    "            return mean_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return float('inf')\n",
    "\n",
    "def optimize_bmed_hyperparameters(trial, dataloaders):\n",
    "    \"\"\"Optuna를 위한 래퍼 함수\"\"\"\n",
    "    optimizer = BMEDHyperparameterOptimizer(dataloaders)\n",
    "    return optimizer.objective(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc0e6889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 및 전처리 중...\n",
      "Fold 1: Train size = 4, Val size = 1\n",
      "Fold 2: Train size = 4, Val size = 1\n",
      "Fold 3: Train size = 4, Val size = 1\n",
      "Fold 4: Train size = 4, Val size = 1\n",
      "Fold 5: Train size = 4, Val size = 1\n",
      "\n",
      "데이터 전처리 완료!\n",
      "- 시퀀스 개수: 5\n",
      "- 최대 시퀀스 길이: 27\n",
      "- K-fold 수: 5\n",
      "- 각 fold는 (train_loader, val_loader) 튜플\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 전처리\n",
    "print(\"데이터 로드 및 전처리 중...\")\n",
    "\n",
    "# 데이터 로드 및 정규화\n",
    "ndf = norm_data('BMED_DATA_AG.csv')\n",
    "ndf = ndf[ndf['exp'].isin([0, 1, 2, 3, 4])]\n",
    "# 시퀀스 데이터 구성\n",
    "seq = seq_data_const(ndf)\n",
    "pad_seq, seq_len, max_seq_len = padded_sequences(seq)\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = gen_dataset(pad_seq, seq_len)\n",
    "\n",
    "# K-fold 데이터로더 생성\n",
    "dataloaders = kfold_dataloaders(dataset, k_folds=5, batch_size=8, random_state=42)\n",
    "\n",
    "print(f\"\\n데이터 전처리 완료!\")\n",
    "print(f\"- 시퀀스 개수: {len(seq)}\")\n",
    "print(f\"- 최대 시퀀스 길이: {max_seq_len}\")\n",
    "print(f\"- K-fold 수: {len(dataloaders)}\")\n",
    "print(f\"- 각 fold는 (train_loader, val_loader) 튜플\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3acd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config = {\n",
    "    'epochs': 500,      # 더 많은 기회 제공\n",
    "    'patience': 50,     # 더 관대한 early stopping\n",
    "    'min_epochs': 100    # 충분한 학습 보장\n",
    "}\n",
    "params = {}\n",
    "params['hidden_size'] = 16\n",
    "params['num_layers'] = 2\n",
    "params['extractor_dropout'] = 0.3\n",
    "params['decoder_layers'] = 2\n",
    "params['decoder_nodes'] = 16\n",
    "params['decoder_dropout'] = 0.3\n",
    "params['learning_rate'] = 1e-4\n",
    "params['weight_decay'] = 1e-5\n",
    "\n",
    "        model_params = {\n",
    "            'state_extractor': {\n",
    "                'input_size': 12,\n",
    "                'hidden_size': params['hidden_size'],\n",
    "                'num_layers': params['num_layers'],\n",
    "                'dropout': params['extractor_dropout']\n",
    "            },\n",
    "            'decoder': {\n",
    "                'hidden_size': params['hidden_size'],\n",
    "                'output_size': 7,\n",
    "                'num_layers': params['decoder_layers'],\n",
    "                'num_nodes': params['decoder_nodes'],\n",
    "                'dropout': params['decoder_dropout']\n",
    "            }\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
