{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb1e7153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ BMED Autoregressive Model - Pretrained Model Loading\n",
      "ğŸ“ Target model file: BMED_TF_250816.pth\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# BMED Autoregressive Model - Pretrained Model Loading and Testing\n",
    "# ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸(BMED_TF_250816.pth)ì„ ë¶ˆëŸ¬ì™€ì„œ ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¶•\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ğŸš€ BMED Autoregressive Model - Pretrained Model Loading\")\n",
    "print(\"ğŸ“ Target model file: BMED_TF_250816.pth\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "wmv7iazlohb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "def df_treat(name):\n",
    "    \"\"\"ë°ì´í„° ì •ê·œí™” ë° ì „ì²˜ë¦¬\"\"\"\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "\n",
    "    exp_num_list = sorted(ndf['exp'].unique())\n",
    "    return df, ndf, range_mm, exp_num_list\n",
    "\n",
    "def seq_data(ndf, exp_num_list):\n",
    "    \"\"\"ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\"\"\"\n",
    "    seq = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "    \n",
    "    for exp in exp_num_list:\n",
    "        exp_df = ndf[ndf['exp'] == exp]\n",
    "        seq.append(exp_df[feature_cols].values)\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def pad_seq(seq):\n",
    "    \"\"\"ì‹œí€€ìŠ¤ íŒ¨ë”©\"\"\"\n",
    "    max_len = max([len(s) for s in seq])\n",
    "    seq_len = [len(s) for s in seq]\n",
    "    pad_seq = pad_sequence([torch.tensor(s) for s in seq], batch_first=True, padding_value=-1)\n",
    "    return pad_seq, seq_len, max_len\n",
    "\n",
    "def gen_dataset(pad_seq, seq_len):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "h0dma8zvikk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ê³¼ ë™ì¼í•œ êµ¬ì¡°)\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    \"\"\"LSTM layer with layer normalization applied to gates\"\"\"\n",
    "    def __init__(self, input_node, hidden_node):\n",
    "        super().__init__()\n",
    "        self.input_node = input_node\n",
    "        self.hidden_node = hidden_node\n",
    "\n",
    "        self.w_i = nn.Linear(input_node, 4 * hidden_node, bias=False)\n",
    "        self.w_h = nn.Linear(hidden_node, 4 * hidden_node, bias=False)\n",
    "\n",
    "        self.ln_i = nn.LayerNorm(hidden_node)\n",
    "        self.ln_h = nn.LayerNorm(hidden_node)\n",
    "        self.ln_g = nn.LayerNorm(hidden_node)\n",
    "        self.ln_o = nn.LayerNorm(hidden_node)\n",
    "\n",
    "        self.ln_c = nn.LayerNorm(hidden_node)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        gi = self.w_i(input)\n",
    "        gh = self.w_h(h_prev)\n",
    "        i_i, i_f, i_g, i_o = gi.chunk(4, dim=-1)\n",
    "        h_i, h_f, h_g, h_o = gh.chunk(4, dim=-1)\n",
    "\n",
    "        i_g = torch.sigmoid(self.ln_i(i_i + h_i))\n",
    "        f_g = torch.sigmoid(self.ln_h(i_f + h_f))\n",
    "        g_g = torch.tanh(self.ln_g(i_g + h_g))\n",
    "        o_g = torch.sigmoid(self.ln_o(i_o + h_o))\n",
    "\n",
    "        c_new = f_g * c_prev + i_g * g_g\n",
    "        c_new = self.ln_c(c_new)\n",
    "\n",
    "        h_new = o_g * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new\n",
    "class StateExtr(nn.Module):\n",
    "    \"\"\"State Extractor using LayerNorm LSTM\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        for _ in range(self.n_layer):\n",
    "            h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "            c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            layer_input = x_t\n",
    "            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "                h_new, c_new = lstm_cell(layer_input, (h_states[layer_idx], c_states[layer_idx]))\n",
    "\n",
    "                h_states[layer_idx] = h_new\n",
    "                c_states[layer_idx] = c_new\n",
    "\n",
    "                if layer_idx < len(self.lstm_cells) - 1:\n",
    "                    layer_input = self.dropout(h_new)\n",
    "                else:\n",
    "                    layer_input = h_new\n",
    "\n",
    "            outputs.append(layer_input)\n",
    "        \n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        normalized = self.final_layer_norm(masked_output)\n",
    "        return self.final_dropout(normalized)\n",
    "\n",
    "# Stateful State Extractor for Free Running Model\n",
    "class StatefulStateExtr(nn.Module):\n",
    "    \"\"\"Hidden stateë¥¼ ìœ ì§€í•˜ë©° sequential ì²˜ë¦¬í•˜ëŠ” State Extractor\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Hidden statesë¥¼ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ê´€ë¦¬\n",
    "        self.h_states = None\n",
    "        self.c_states = None\n",
    "\n",
    "    def reset_states(self, batch_size=None, device=None):\n",
    "        \"\"\"ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ ì‹œì‘ ì‹œ hidden state ì´ˆê¸°í™”\"\"\"\n",
    "        if batch_size is not None and device is not None:\n",
    "            self.h_states = []\n",
    "            self.c_states = []\n",
    "            for _ in range(self.n_layer):\n",
    "                self.h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "                self.c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        else:\n",
    "            self.h_states = None\n",
    "            self.c_states = None\n",
    "\n",
    "    def forward_single_timestep(self, x_t):\n",
    "        \"\"\"ë‹¨ì¼ ì‹œì  ì²˜ë¦¬ - hidden state ìœ ì§€\"\"\"\n",
    "        batch_size = x_t.size(0)\n",
    "        device = x_t.device\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œ hidden state ì´ˆê¸°í™”\n",
    "        if self.h_states is None:\n",
    "            self.reset_states(batch_size, device)\n",
    "\n",
    "        layer_input = x_t\n",
    "        for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "            h_new, c_new = lstm_cell(layer_input, (self.h_states[layer_idx], self.c_states[layer_idx]))\n",
    "\n",
    "            # Hidden state ì—…ë°ì´íŠ¸\n",
    "            self.h_states[layer_idx] = h_new\n",
    "            self.c_states[layer_idx] = c_new\n",
    "\n",
    "            if layer_idx < len(self.lstm_cells) - 1:\n",
    "                layer_input = self.dropout(h_new)\n",
    "            else:\n",
    "                layer_input = h_new\n",
    "\n",
    "        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ ì ìš©\n",
    "        normalized = self.final_layer_norm(layer_input)\n",
    "        return self.final_dropout(normalized)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"ê¸°ì¡´ teacher forcing ë°©ì‹ê³¼ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # Teacher forcing ëª¨ë“œì—ì„œëŠ” ë§¤ë²ˆ hidden state ì´ˆê¸°í™”\n",
    "        self.reset_states(batch_size, device)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "            output = self.forward_single_timestep(x_t)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        \n",
    "        # ë§ˆìŠ¤í‚¹ ì ìš©\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        return masked_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0jjbstv1vxnn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"Physical Change Decoder\"\"\"\n",
    "    def __init__(self, input_node, output_node, n_layer, hidden_node, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_node, output_node))\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CurrentPredictor(nn.Module):\n",
    "    \"\"\"Current Predictor\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_node, 1))\n",
    "    \n",
    "    def forward(self, new_state):\n",
    "        x = new_state\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "zd4or2cxr4r",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"Physics Constraint Layer with Current Prediction\"\"\"\n",
    "    def __init__(self, range_mm, current_predictor, eps=1e-2):\n",
    "        super().__init__()\n",
    "        self.sps = eps\n",
    "        self.current_predictor = current_predictor\n",
    "        self.register_buffer('range_mm_tensor', self._convert_range_to_tensor(range_mm))\n",
    "\n",
    "    def _convert_range_to_tensor(self, range_mm):\n",
    "        feature_names = ['V','E','VF','VA','VB','CFLA','CALA','CFK','CBK','I']\n",
    "        ranges = torch.zeros(len(feature_names),2)\n",
    "\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if name in range_mm:\n",
    "                ranges[i, 0] = range_mm[name]['min']\n",
    "                ranges[i, 1] = range_mm[name]['max']\n",
    "        \n",
    "        return ranges\n",
    "    \n",
    "    def normalize(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "    def denormalize(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return data * (max_val - min_val) + min_val\n",
    "\n",
    "    def forward(self, physical_changes, current_state):\n",
    "        V_idx, E_idx, VF_idx, VA_idx, VB_idx = 0, 1, 2, 3, 4\n",
    "        CFLA_idx, CALA_idx, CFK_idx, CBK_idx, I_idx = 5, 6, 7, 8, 9\n",
    "\n",
    "        VF = self.denormalize(current_state[..., 2:3], VF_idx)\n",
    "        VA = self.denormalize(current_state[..., 3:4], VA_idx)\n",
    "        VB = self.denormalize(current_state[..., 4:5], VB_idx)\n",
    "        CFLA = self.denormalize(current_state[..., 5:6], CFLA_idx)\n",
    "        CALA = self.denormalize(current_state[..., 6:7], CALA_idx)\n",
    "        CFK = self.denormalize(current_state[..., 7:8], CFK_idx)\n",
    "        CBK = self.denormalize(current_state[..., 8:9], CBK_idx)\n",
    "\n",
    "        dVA = physical_changes[..., 0:1]\n",
    "        dVB = physical_changes[..., 1:2]\n",
    "        dNALA = physical_changes[..., 2:3]\n",
    "        dNBK = physical_changes[..., 3:4]\n",
    "\n",
    "        NFLA = CFLA * VF\n",
    "        NALA = CALA * VA\n",
    "        NFK = CFK * VF\n",
    "        NBK = CBK * VB\n",
    "\n",
    "        nVF = VF - dVA - dVB\n",
    "        nVA = VA + dVA\n",
    "        nVB = VB + dVB\n",
    "\n",
    "        nVF = torch.clamp(nVF, min=self.sps)\n",
    "        nVA = torch.clamp(nVA, min=self.sps)\n",
    "        nVB = torch.clamp(nVB, min=self.sps)\n",
    "        \n",
    "        nNFLA = NFLA - torch.clamp(dNALA, min=0.0)\n",
    "        nNALA = NALA + torch.clamp(dNALA, min=0.0)\n",
    "        nNFK = NFK - torch.clamp(dNBK, min=0.0)\n",
    "        nNBK = NBK + torch.clamp(dNBK, min=0.0)\n",
    "\n",
    "        nNFLA = torch.clamp(nNFLA, min=0.0)\n",
    "        nNALA = torch.clamp(nNALA, min=0.0)\n",
    "        nNFK = torch.clamp(nNFK, min=0.0)\n",
    "        nNBK = torch.clamp(nNBK, min=0.0)\n",
    "\n",
    "        nCFLA = nNFLA / nVF\n",
    "        nCALA = nNALA / nVA\n",
    "        nCFK = nNFK / nVF\n",
    "        nCBK = nNBK / nVB\n",
    "\n",
    "        V = current_state[..., 0:1]\n",
    "        E = current_state[..., 1:2]\n",
    "        nVF_norm = self.normalize(nVF, VF_idx)\n",
    "        nVA_norm = self.normalize(nVA, VA_idx)\n",
    "        nVB_norm = self.normalize(nVB, VB_idx)\n",
    "        nCFLA_norm = self.normalize(nCFLA, CFLA_idx)\n",
    "        nCALA_norm = self.normalize(nCALA, CALA_idx)\n",
    "        nCFK_norm = self.normalize(nCFK, CFK_idx)\n",
    "        nCBK_norm = self.normalize(nCBK, CBK_idx)\n",
    "\n",
    "        temp_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        nI_pred_norm = self.current_predictor(temp_state)\n",
    "        \n",
    "        nI_real = self.denormalize(nI_pred_norm, I_idx)\n",
    "        nI_real = torch.clamp(nI_real, min=0.0)\n",
    "        nI_norm = self.normalize(nI_real, I_idx)\n",
    "\n",
    "        next_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm, nI_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0b7fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í†µí•©ëœ BMEDAutoregressiveModel êµ¬í˜„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"í†µí•© BMED Autoregressive Model - Teacher Forcing & Free Running ëª¨ë“œ ì§€ì›\"\"\"\n",
    "    def __init__(self, state_extr_params, decoder_params, current_predictor_params, range_mm):\n",
    "        super().__init__()\n",
    "        self.state_extr = StateExtr(**state_extr_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.current_predictor = CurrentPredictor(**current_predictor_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer(range_mm, self.current_predictor)\n",
    "        \n",
    "        # Free runningì„ ìœ„í•œ hidden state ê´€ë¦¬\n",
    "        self._hidden_states = None\n",
    "        self._cell_states = None\n",
    "\n",
    "    def _reset_hidden_states(self, batch_size, device):\n",
    "        \"\"\"Free running ì‹œì‘ ì‹œ hidden state ì´ˆê¸°í™”\"\"\"\n",
    "        self._hidden_states = []\n",
    "        self._cell_states = []\n",
    "        for _ in range(self.state_extr.n_layer):\n",
    "            self._hidden_states.append(torch.zeros(batch_size, self.state_extr.hidden_node, device=device))\n",
    "            self._cell_states.append(torch.zeros(batch_size, self.state_extr.hidden_node, device=device))\n",
    "\n",
    "    def teacher_forcing_forward(self, x, seq_len):\n",
    "        \"\"\"Teacher Forcing ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹\"\"\"\n",
    "        hidden_states = self.state_extr(x, seq_len)\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        new_x = self.physics_constraint(physical_changes, x)\n",
    "        return new_x\n",
    "\n",
    "    def free_running_forward(self, initial_state, target_length):\n",
    "        \"\"\"\n",
    "        Free Running ëª¨ë“œ: ì´ˆê¸° ìƒíƒœë§Œìœ¼ë¡œ ì „ì²´ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "        Args:\n",
    "            initial_state: [batch, features] - ì´ˆê¸° ìƒíƒœ (ëª¨ë“  10ê°œ íŠ¹ì„± í¬í•¨)\n",
    "            target_length: int - ì˜ˆì¸¡í•  ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "        Returns:\n",
    "            predictions: [batch, target_length, features] - ì˜ˆì¸¡ëœ ì „ì²´ ì‹œí€€ìŠ¤\n",
    "        \"\"\"\n",
    "        batch_size = initial_state.size(0)\n",
    "        feature_size = initial_state.size(1)\n",
    "        device = initial_state.device\n",
    "        \n",
    "        # Hidden state ì´ˆê¸°í™”\n",
    "        self._reset_hidden_states(batch_size, device)\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "        predictions = torch.zeros(batch_size, target_length, feature_size, device=device)\n",
    "        current_state = initial_state.clone()\n",
    "        \n",
    "        for t in range(target_length):\n",
    "            predictions[:, t, :] = current_state\n",
    "            \n",
    "            if t < target_length - 1:\n",
    "                # í˜„ì¬ ìƒíƒœì—ì„œ I(ì „ë¥˜) ì œê±°í•˜ì—¬ LSTM ì…ë ¥ ìƒì„± (9ê°œ íŠ¹ì„±)\n",
    "                lstm_input = current_state[:, :-1]  # [batch, 9]\n",
    "                \n",
    "                # LSTM forward with hidden state maintenance\n",
    "                hidden_output = self._forward_lstm_single_step(lstm_input)\n",
    "                \n",
    "                # Physical change ì˜ˆì¸¡\n",
    "                physical_changes = self.physical_decoder(hidden_output.unsqueeze(1))  # [batch, 1, output]\n",
    "                \n",
    "                # Physics constraintë¡œ ë‹¤ìŒ ìƒíƒœ ê³„ì‚°\n",
    "                current_state_expanded = current_state.unsqueeze(1)  # [batch, 1, features]\n",
    "                next_state = self.physics_constraint(physical_changes, current_state_expanded)\n",
    "                current_state = next_state.squeeze(1)  # [batch, features]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def _forward_lstm_single_step(self, x_t):\n",
    "        \"\"\"ë‹¨ì¼ ì‹œì  LSTM forward (hidden state ìœ ì§€)\"\"\"\n",
    "        layer_input = x_t\n",
    "        \n",
    "        for layer_idx, lstm_cell in enumerate(self.state_extr.lstm_cells):\n",
    "            h_new, c_new = lstm_cell(layer_input, \n",
    "                                   (self._hidden_states[layer_idx], self._cell_states[layer_idx]))\n",
    "            \n",
    "            # Hidden state ì—…ë°ì´íŠ¸\n",
    "            self._hidden_states[layer_idx] = h_new\n",
    "            self._cell_states[layer_idx] = c_new\n",
    "            \n",
    "            # Dropout ì ìš© (ë§ˆì§€ë§‰ layer ì œì™¸)\n",
    "            if layer_idx < len(self.state_extr.lstm_cells) - 1:\n",
    "                layer_input = self.state_extr.dropout(h_new)\n",
    "            else:\n",
    "                layer_input = h_new\n",
    "        \n",
    "        # ìµœì¢… ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ\n",
    "        normalized = self.state_extr.final_layer_norm(layer_input)\n",
    "        return self.state_extr.final_dropout(normalized)\n",
    "\n",
    "    def forward(self, x, seq_len=None, mode='teacher_forcing', target_length=None):\n",
    "        \"\"\"\n",
    "        í†µí•© forward ë©”ì†Œë“œ\n",
    "        Args:\n",
    "            x: ì…ë ¥ ë°ì´í„°\n",
    "            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´ (teacher forcing ëª¨ë“œì—ì„œ ì‚¬ìš©)\n",
    "            mode: 'teacher_forcing' ë˜ëŠ” 'free_running'\n",
    "            target_length: free runningì—ì„œ ì˜ˆì¸¡í•  ê¸¸ì´\n",
    "        \"\"\"\n",
    "        if mode == 'teacher_forcing':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_lenì€ teacher_forcing ëª¨ë“œì—ì„œ í•„ìˆ˜ì…ë‹ˆë‹¤\")\n",
    "            return self.teacher_forcing_forward(x, seq_len)\n",
    "        \n",
    "        elif mode == 'free_running':\n",
    "            if target_length is None:\n",
    "                raise ValueError(\"target_lengthëŠ” free_running ëª¨ë“œì—ì„œ í•„ìˆ˜ì…ë‹ˆë‹¤\")\n",
    "            return self.free_running_forward(x, target_length)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {mode}. 'teacher_forcing' ë˜ëŠ” 'free_running'ì„ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "\n",
    "print(\"âœ… í†µí•©ëœ BMEDAutoregressiveModel êµ¬í˜„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2f55d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ë“¤ êµ¬í˜„ ì™„ë£Œ!\n",
      "   - tf_data: Teacher Forcingìš© ì…ë ¥/íƒ€ê²Ÿ ì¤€ë¹„\n",
      "   - fr_data: Free Runningìš© ì´ˆê¸°ìƒíƒœ/íƒ€ê²Ÿ ì¤€ë¹„\n",
      "   - masked_mse_loss: ë§ˆìŠ¤í‚¹ëœ MSE ì†ì‹¤ í•¨ìˆ˜\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ë“¤ - Teacher Forcing & Free Running ì§€ì›\n",
    "\n",
    "def tf_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Teacher Forcingìš© ë°ì´í„° ì¤€ë¹„ (ì›ë³¸ bmed_tf_learning.ipynbì™€ ë™ì¼)\n",
    "    \n",
    "    Args:\n",
    "        input_seq: [batch, seq_len, features] - ì „ì²´ ì‹œí€€ìŠ¤ (10ê°œ íŠ¹ì„±)\n",
    "        seq_len: [batch] - ê° ì‹œí€€ìŠ¤ì˜ ì‹¤ì œ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        inputs: [batch, seq_len-1, 9] - LSTM ì…ë ¥ (I ì œì™¸í•œ 9ê°œ íŠ¹ì„±)\n",
    "        targets: [batch, seq_len-1, 10] - ì˜ˆì¸¡ íƒ€ê²Ÿ (ì „ì²´ 10ê°œ íŠ¹ì„±)\n",
    "        target_seq_len: [batch] - íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ê¸¸ì´ (seq_len - 1)\n",
    "    \"\"\"\n",
    "    # ì…ë ¥: ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹œì  ì œì™¸, I(ì „ë¥˜) íŠ¹ì„± ì œì™¸\n",
    "    inputs = input_seq[:, :-1, :-1]  # [batch, seq_len-1, 9]\n",
    "    \n",
    "    # íƒ€ê²Ÿ: ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ ì‹œì  ì œì™¸, ëª¨ë“  íŠ¹ì„± í¬í•¨\n",
    "    targets = input_seq[:, 1:, :]    # [batch, seq_len-1, 10]\n",
    "    \n",
    "    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    target_seq_len = seq_len - 1\n",
    "    \n",
    "    return inputs, targets, target_seq_len\n",
    "\n",
    "def fr_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Free Runningìš© ë°ì´í„° ì¤€ë¹„\n",
    "    \n",
    "    Args:\n",
    "        input_seq: [batch, seq_len, features] - ì „ì²´ ì‹œí€€ìŠ¤ (10ê°œ íŠ¹ì„±)\n",
    "        seq_len: [batch] - ê° ì‹œí€€ìŠ¤ì˜ ì‹¤ì œ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        initial_states: [batch, 10] - ì´ˆê¸° ìƒíƒœ (ì „ì²´ 10ê°œ íŠ¹ì„±)\n",
    "        targets: [batch, seq_len, 10] - ì˜ˆì¸¡ íƒ€ê²Ÿ (ì „ì²´ ì‹œí€€ìŠ¤)\n",
    "        target_lengths: [batch] - ì˜ˆì¸¡í•  ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \"\"\"\n",
    "    # ì´ˆê¸° ìƒíƒœ: ì²« ë²ˆì§¸ ì‹œì \n",
    "    initial_states = input_seq[:, 0, :]  # [batch, 10]\n",
    "    \n",
    "    # íƒ€ê²Ÿ: ì „ì²´ ì‹œí€€ìŠ¤\n",
    "    targets = input_seq  # [batch, seq_len, 10]\n",
    "    \n",
    "    # ì˜ˆì¸¡í•  ê¸¸ì´\n",
    "    target_lengths = seq_len  # [batch]\n",
    "    \n",
    "    return initial_states, targets, target_lengths\n",
    "\n",
    "def masked_mse_loss(pred, target, seq_len):\n",
    "    \"\"\"Masked MSE Loss function\"\"\"\n",
    "    batch_size, max_len, features = pred.shape\n",
    "    seq_len_cpu = seq_len.detach().cpu().long()\n",
    "\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "    mask = mask.float().to(pred.device)\n",
    "\n",
    "    loss = F.mse_loss(pred, target, reduction='none')\n",
    "    masked_loss = loss * mask.unsqueeze(-1)\n",
    "\n",
    "    total_loss = masked_loss.sum()\n",
    "    total_elements = mask.sum()\n",
    "\n",
    "    masked_loss = total_loss / total_elements\n",
    "    return masked_loss\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ë“¤ êµ¬í˜„ ì™„ë£Œ!\")\n",
    "print(\"   - tf_data: Teacher Forcingìš© ì…ë ¥/íƒ€ê²Ÿ ì¤€ë¹„\")\n",
    "print(\"   - fr_data: Free Runningìš© ì´ˆê¸°ìƒíƒœ/íƒ€ê²Ÿ ì¤€ë¹„\") \n",
    "print(\"   - masked_mse_loss: ë§ˆìŠ¤í‚¹ëœ MSE ì†ì‹¤ í•¨ìˆ˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2fd2b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘: BMED_DATA_AG.csv\n",
      "   - ì‹¤í—˜ ìˆ˜: 24\n",
      "   - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: 33\n",
      "ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: cuda\n",
      "ğŸ“¥ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘: BMED_TF_250816.pth\n",
      "âœ… ëª¨ë¸ ì„¤ì • ì •ë³´ ë°œê²¬:\n",
      "   - State Extractor: input=9, hidden=64, layers=4, dropout=0.1\n",
      "   - Decoder: input=64, hidden=128, layers=4, dropout=0.1\n",
      "   - Current Predictor: input=9, hidden=96, layers=2, dropout=0.1\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "data_path = \"BMED_DATA_AG.csv\"\n",
    "print(f\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}\")\n",
    "df, ndf, range_mm, exp_num_list = df_treat(data_path)\n",
    "\n",
    "# ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\n",
    "seq = seq_data(ndf, exp_num_list)\n",
    "pad_sequences, seq_lengths, max_length = pad_seq(seq)\n",
    "dataset = gen_dataset(pad_sequences, seq_lengths)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"   - ì‹¤í—˜ ìˆ˜: {len(exp_num_list)}\")\n",
    "print(f\"   - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_length}\")\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}\")\n",
    "\n",
    "# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ\n",
    "model_path = \"BMED_TF_250816.pth\"\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n",
    "print(f\"ğŸ“¥ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘: {model_path}\")\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì • ì •ë³´ í™•ì¸\n",
    "model_config = checkpoint['model_config']\n",
    "state_extr_params = model_config['state_extr_params']\n",
    "decoder_params = model_config['decoder_params']\n",
    "current_predictor_params = model_config['current_predictor_params']\n",
    "range_mm = model_config['range_mm']\n",
    "print(\"âœ… ëª¨ë¸ ì„¤ì • ì •ë³´ ë°œê²¬:\")\n",
    "print(f\"   - State Extractor: input={state_extr_params['input_node']}, hidden={state_extr_params['hidden_node']}, layers={state_extr_params['n_layer']}, dropout={state_extr_params['dropout']}\")\n",
    "print(f\"   - Decoder: input={decoder_params['input_node']}, hidden={decoder_params['hidden_node']}, layers={decoder_params['n_layer']}, dropout={decoder_params['dropout']}\")\n",
    "print(f\"   - Current Predictor: input={current_predictor_params['input_node']}, hidden={current_predictor_params['hidden_node']}, layers={current_predictor_params['n_layer']}, dropout={current_predictor_params['dropout']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b323d277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í†µí•© ëª¨ë¸ ìƒì„± ì¤‘...\n",
      "âš¡ Pretrained ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘...\n",
      "âœ… í†µí•© ëª¨ë¸ êµ¬ì„± ì™„ë£Œ!\n",
      "   - ëª¨ë¸ ì¥ì¹˜: cuda:0\n",
      "   - ëª¨ë¸ ëª¨ë“œ: eval\n",
      "\n",
      "ğŸ”¥ Teacher Forcing ëª¨ë“œ í…ŒìŠ¤íŠ¸...\n",
      "   - ì›ë³¸ ì…ë ¥ í¬ê¸°: torch.Size([4, 33, 10])\n",
      "   - ì‹œí€€ìŠ¤ ê¸¸ì´: tensor([17, 21, 21, 25], device='cuda:0')\n",
      "   - TF ì…ë ¥ í¬ê¸°: torch.Size([4, 32, 9]) (I ì œì™¸í•œ 9ê°œ íŠ¹ì„±)\n",
      "   - TF íƒ€ê²Ÿ í¬ê¸°: torch.Size([4, 32, 10]) (ì „ì²´ 10ê°œ íŠ¹ì„±)\n",
      "   - TF ì‹œí€€ìŠ¤ ê¸¸ì´: tensor([16, 20, 20, 24], device='cuda:0')\n",
      "   - Teacher Forcing ì¶œë ¥ í¬ê¸°: torch.Size([4, 32, 10])\n",
      "   - ì¶œë ¥ ë²”ìœ„: [-1.0000, 583.3128]\n",
      "\n",
      "ğŸ¯ Free Running ëª¨ë“œ í…ŒìŠ¤íŠ¸...\n",
      "   - ì´ˆê¸° ìƒíƒœ í¬ê¸°: torch.Size([1, 10])\n",
      "   - ì˜ˆì¸¡ ê¸¸ì´: 17\n",
      "   - Free Running ì¶œë ¥ í¬ê¸°: torch.Size([1, 17, 10])\n",
      "   - ì¶œë ¥ ë²”ìœ„: [0.0000, 0.5036]\n",
      "\n",
      "ğŸ“Š Teacher Forcing vs Free Running ë¹„êµ...\n",
      "   - Teacher Forcing MSE: 18260.738281\n",
      "   - Free Running MSE: 0.000003\n",
      "   - ì²« ë²ˆì§¸ ì‹œì  ì°¨ì´ (0ì´ì–´ì•¼ í•¨): 0.000000\n",
      "   - ëª¨ë“œ ê°„ ì˜ˆì¸¡ ì°¨ì´: 0.001068\n",
      "\n",
      "âœ… í†µí•© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "   â†’ Teacher Forcing: tf_data í•¨ìˆ˜ë¡œ ì˜¬ë°”ë¥¸ ì…ë ¥/íƒ€ê²Ÿ ì¤€ë¹„\n",
      "   â†’ Free Running: fr_data í•¨ìˆ˜ë¡œ ì´ˆê¸°ìƒíƒœ/íƒ€ê²Ÿ ì¤€ë¹„\n",
      "   â†’ ë‘ ëª¨ë“œ ëª¨ë‘ ì •ìƒ ë™ì‘ í™•ì¸\n"
     ]
    }
   ],
   "source": [
    "# í†µí•©ëœ BMED Autoregressive Model í…ŒìŠ¤íŠ¸ - tf_data í•¨ìˆ˜ ì‚¬ìš©\n",
    "\n",
    "# í†µí•© ëª¨ë¸ ìƒì„± ë° pretrained ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "print(\"ğŸ“¦ í†µí•© ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "unified_model = BMEDAutoregressiveModel(\n",
    "    state_extr_params=state_extr_params,\n",
    "    decoder_params=decoder_params, \n",
    "    current_predictor_params=current_predictor_params,\n",
    "    range_mm=range_mm\n",
    ").to(device)\n",
    "\n",
    "# Pretrained ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "print(\"âš¡ Pretrained ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘...\")\n",
    "unified_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "unified_model.eval()\n",
    "\n",
    "print(\"âœ… í†µí•© ëª¨ë¸ êµ¬ì„± ì™„ë£Œ!\")\n",
    "print(f\"   - ëª¨ë¸ ì¥ì¹˜: {next(unified_model.parameters()).device}\")\n",
    "print(f\"   - ëª¨ë¸ ëª¨ë“œ: {'eval' if not unified_model.training else 'train'}\")\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„°ë¡œ Teacher Forcing í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ”¥ Teacher Forcing ëª¨ë“œ í…ŒìŠ¤íŠ¸...\")\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(dataloader))\n",
    "    sample_input, sample_seq_len = sample_batch\n",
    "    sample_input = sample_input.to(device)\n",
    "    sample_seq_len = sample_seq_len.to(device)\n",
    "    \n",
    "    print(f\"   - ì›ë³¸ ì…ë ¥ í¬ê¸°: {sample_input.shape}\")\n",
    "    print(f\"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {sample_seq_len}\")\n",
    "    \n",
    "    # Teacher Forcing ë°ì´í„° ì¤€ë¹„ (tf_data í•¨ìˆ˜ ì‚¬ìš©)\n",
    "    tf_inputs, tf_targets, tf_seq_len = tf_data(sample_input, sample_seq_len)\n",
    "    print(f\"   - TF ì…ë ¥ í¬ê¸°: {tf_inputs.shape} (I ì œì™¸í•œ 9ê°œ íŠ¹ì„±)\")\n",
    "    print(f\"   - TF íƒ€ê²Ÿ í¬ê¸°: {tf_targets.shape} (ì „ì²´ 10ê°œ íŠ¹ì„±)\")\n",
    "    print(f\"   - TF ì‹œí€€ìŠ¤ ê¸¸ì´: {tf_seq_len}\")\n",
    "    \n",
    "    # Teacher forcing ì˜ˆì¸¡\n",
    "    tf_output = unified_model(tf_inputs, tf_seq_len, mode='teacher_forcing')\n",
    "    print(f\"   - Teacher Forcing ì¶œë ¥ í¬ê¸°: {tf_output.shape}\")\n",
    "    print(f\"   - ì¶œë ¥ ë²”ìœ„: [{tf_output.min().item():.4f}, {tf_output.max().item():.4f}]\")\n",
    "\n",
    "# Free Running ëª¨ë“œ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ¯ Free Running ëª¨ë“œ í…ŒìŠ¤íŠ¸...\")\n",
    "with torch.no_grad():\n",
    "    # Free Running ë°ì´í„° ì¤€ë¹„ (fr_data í•¨ìˆ˜ ì‚¬ìš©)\n",
    "    fr_initial_states, fr_targets, fr_lengths = fr_data(sample_input, sample_seq_len)\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì‚¬ìš©\n",
    "    initial_state = fr_initial_states[0:1, :]  # [1, 10]\n",
    "    target_length = int(fr_lengths[0].item())  # ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \n",
    "    print(f\"   - ì´ˆê¸° ìƒíƒœ í¬ê¸°: {initial_state.shape}\")\n",
    "    print(f\"   - ì˜ˆì¸¡ ê¸¸ì´: {target_length}\")\n",
    "    \n",
    "    # Free running ì˜ˆì¸¡\n",
    "    fr_output = unified_model(initial_state, mode='free_running', target_length=target_length)\n",
    "    print(f\"   - Free Running ì¶œë ¥ í¬ê¸°: {fr_output.shape}\")\n",
    "    print(f\"   - ì¶œë ¥ ë²”ìœ„: [{fr_output.min().item():.4f}, {fr_output.max().item():.4f}]\")\n",
    "\n",
    "# ë‘ ëª¨ë“œ ê°„ ì°¨ì´ ë¹„êµ\n",
    "print(\"\\nğŸ“Š Teacher Forcing vs Free Running ë¹„êµ...\")\n",
    "with torch.no_grad():\n",
    "    # Teacher Forcing: íƒ€ê²Ÿê³¼ ë¹„êµ (ì‹œì  1ë¶€í„°)\n",
    "    tf_single = tf_output[0:1, :, :]  # [1, seq_len-1, 10]\n",
    "    tf_target = tf_targets[0:1, :, :]  # [1, seq_len-1, 10]\n",
    "    \n",
    "    # Free Running: ì „ì²´ ì‹œí€€ìŠ¤ì™€ ë¹„êµ\n",
    "    fr_single = fr_output  # [1, seq_len, 10]\n",
    "    fr_target = fr_targets[0:1, :target_length, :]  # [1, seq_len, 10]\n",
    "    \n",
    "    # Teacher Forcing ì„±ëŠ¥\n",
    "    tf_loss = F.mse_loss(tf_single, tf_target)\n",
    "    print(f\"   - Teacher Forcing MSE: {tf_loss.item():.6f}\")\n",
    "    \n",
    "    # Free Running ì„±ëŠ¥\n",
    "    fr_loss = F.mse_loss(fr_single, fr_target)\n",
    "    print(f\"   - Free Running MSE: {fr_loss.item():.6f}\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ì‹œì  ë¹„êµ (Free Runningì˜ ì²« ì‹œì ì€ ì´ˆê¸°ê°’ê³¼ ë™ì¼í•´ì•¼ í•¨)\n",
    "    first_step_diff = torch.abs(fr_single[:, 0, :] - initial_state).mean()\n",
    "    print(f\"   - ì²« ë²ˆì§¸ ì‹œì  ì°¨ì´ (0ì´ì–´ì•¼ í•¨): {first_step_diff.item():.6f}\")\n",
    "    \n",
    "    # ë™ì¼ ì‹œì  ë¹„êµ (Teacher Forcing ì˜ˆì¸¡ vs Free Running ì˜ˆì¸¡)\n",
    "    # TFëŠ” ì‹œì  1ë¶€í„°, FRì€ ì‹œì  1ë¶€í„° ë¹„êµ\n",
    "    if tf_single.size(1) > 0 and fr_single.size(1) > 1:\n",
    "        comparison_length = min(tf_single.size(1), fr_single.size(1) - 1)\n",
    "        tf_comparison = tf_single[:, :comparison_length, :]\n",
    "        fr_comparison = fr_single[:, 1:comparison_length+1, :]  # FRì˜ ì‹œì  1ë¶€í„°\n",
    "        \n",
    "        mode_diff = torch.abs(tf_comparison - fr_comparison).mean()\n",
    "        print(f\"   - ëª¨ë“œ ê°„ ì˜ˆì¸¡ ì°¨ì´: {mode_diff.item():.6f}\")\n",
    "\n",
    "print(\"\\nâœ… í†µí•© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"   â†’ Teacher Forcing: tf_data í•¨ìˆ˜ë¡œ ì˜¬ë°”ë¥¸ ì…ë ¥/íƒ€ê²Ÿ ì¤€ë¹„\")\n",
    "print(\"   â†’ Free Running: fr_data í•¨ìˆ˜ë¡œ ì´ˆê¸°ìƒíƒœ/íƒ€ê²Ÿ ì¤€ë¹„\")\n",
    "print(\"   â†’ ë‘ ëª¨ë“œ ëª¨ë‘ ì •ìƒ ë™ì‘ í™•ì¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2t1dwt1ekm",
   "metadata": {},
   "outputs": [],
   "source": "# Teacher Forcing vs Free Running ì „ì²´ ì‹¤í—˜ ë¹„êµ ì‹œê°í™” - Featureë³„ Subplot\n\nprint(\"ğŸ¯ Teacher Forcing vs Free Running ì „ì²´ ì‹¤í—˜ ë¹„êµ ì‹œê°í™”\")\nprint(\"=\"*60)\n\n# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\nunified_model.eval()\n\n# ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\ntf_predictions_dict = {}\nfr_predictions_dict = {}\nactual_dict = {}\n\n# Vì™€ Eë¥¼ ì œì™¸í•œ featureë“¤ì˜ ì¸ë±ìŠ¤ì™€ ì´ë¦„\nfeature_names = ['VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\nfeature_indices = [2, 3, 4, 5, 6, 7, 8, 9]  # V(0), E(1)ë¥¼ ì œì™¸í•œ ì¸ë±ìŠ¤\n\n# ì „ì²´ ì‹¤í—˜ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰\nwith torch.no_grad():\n    for exp_num in exp_num_list:\n        # ì‹¤í—˜ ë°ì´í„° ì¶”ì¶œ\n        exp_data = ndf[ndf['exp'] == exp_num].copy()\n        if len(exp_data) == 0:\n            continue\n        \n        # íŠ¹ì„± ì»¬ëŸ¼ë“¤\n        feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n        \n        # ì‹¤í—˜ ë°ì´í„°ë¥¼ tensorë¡œ ë³€í™˜\n        exp_tensor = torch.tensor(exp_data[feature_cols].values).float().unsqueeze(0).to(device)  # [1, seq_len, 10]\n        seq_length = len(exp_data)\n        seq_len_tensor = torch.tensor([seq_length]).to(device)\n        \n        # Teacher Forcing ë°ì´í„° ì¤€ë¹„ ë° ì˜ˆì¸¡\n        tf_inputs, tf_targets, tf_seq_len = tf_data(exp_tensor, seq_len_tensor)\n        tf_pred = unified_model(tf_inputs, tf_seq_len, mode='teacher_forcing')\n        \n        # Free Running ë°ì´í„° ì¤€ë¹„ ë° ì˜ˆì¸¡\n        fr_initial_states, fr_targets, fr_lengths = fr_data(exp_tensor, seq_len_tensor)\n        initial_state = fr_initial_states  # [1, 10]\n        target_length = int(fr_lengths[0].item())\n        fr_pred = unified_model(initial_state, mode='free_running', target_length=target_length)\n        \n        # CPUë¡œ ì´ë™í•˜ê³  numpy ë³€í™˜\n        tf_pred_np = tf_pred[0].cpu().numpy()  # [seq_len-1, 10]\n        fr_pred_np = fr_pred[0].cpu().numpy()  # [seq_len, 10]\n        actual_tf_np = tf_targets[0].cpu().numpy()  # [seq_len-1, 10] (TF targets)\n        actual_fr_np = fr_targets[0, :target_length].cpu().numpy()  # [seq_len, 10] (FR targets)\n        \n        # ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n        tf_predictions_dict[exp_num] = tf_pred_np\n        fr_predictions_dict[exp_num] = fr_pred_np\n        actual_dict[exp_num] = {\n            'tf': actual_tf_np,  # Teacher Forcing íƒ€ê²Ÿ (ì‹œì  1ë¶€í„°)\n            'fr': actual_fr_np,  # Free Running íƒ€ê²Ÿ (ì „ì²´ ì‹œí€€ìŠ¤)\n            'time_tf': exp_data['t'].values[1:len(tf_pred_np)+1],  # TF ì‹œê°„ì¶•\n            'time_fr': exp_data['t'].values[:len(fr_pred_np)]      # FR ì‹œê°„ì¶•\n        }\n\nprint(f\"âœ… {len(tf_predictions_dict)}ê°œ ì‹¤í—˜ì— ëŒ€í•œ ì˜ˆì¸¡ ì™„ë£Œ\")\n\n# Vì™€ Eë¥¼ ì œì™¸í•œ ê° featureë³„ë¡œ ê·¸ë˜í”„ ìƒì„±\nfor feat_idx, feat_name in zip(feature_indices, feature_names):\n    # subplot ê°œìˆ˜ ê³„ì‚° (í–‰ê³¼ ì—´ ìµœì í™”)\n    n_experiments = len(exp_num_list)\n    n_cols = min(6, n_experiments)  # ìµœëŒ€ 6ì—´\n    n_rows = (n_experiments + n_cols - 1) // n_cols  # í•„ìš”í•œ í–‰ ìˆ˜\n    \n    # ê·¸ë˜í”„ í¬ê¸° ì„¤ì •\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*3))\n    fig.suptitle(f'Feature: {feat_name} - Teacher Forcing vs Free Running vs Actual', fontsize=16, fontweight='bold')\n    \n    # subplotì´ 1ê°œì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n    if n_experiments == 1:\n        axes = [axes]\n    elif n_rows == 1:\n        axes = axes.reshape(1, -1)\n    \n    # ê° ì‹¤í—˜ì— ëŒ€í•´ subplot ìƒì„±\n    for i, exp_num in enumerate(exp_num_list):\n        row = i // n_cols\n        col = i % n_cols\n        \n        if n_rows > 1:\n            ax = axes[row, col]\n        else:\n            ax = axes[col] if n_cols > 1 else axes[0]\n        \n        # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°€ì ¸ì˜¤ê¸°\n        if exp_num in tf_predictions_dict:\n            tf_pred_values = tf_predictions_dict[exp_num][:, feat_idx]\n            fr_pred_values = fr_predictions_dict[exp_num][:, feat_idx]\n            tf_actual_values = actual_dict[exp_num]['tf'][:, feat_idx]\n            fr_actual_values = actual_dict[exp_num]['fr'][:, feat_idx]\n            tf_time = actual_dict[exp_num]['time_tf'][:len(tf_pred_values)]\n            fr_time = actual_dict[exp_num]['time_fr'][:len(fr_pred_values)]\n            \n            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n            # ì‹¤ì œê°’ (Free Runningì€ ì „ì²´, Teacher Forcingì€ ì‹œì  1ë¶€í„°)\n            ax.plot(fr_time, fr_actual_values, 'k-', linewidth=2, label='Actual', alpha=0.8)\n            \n            # Teacher Forcing ì˜ˆì¸¡ (ì‹œì  1ë¶€í„°)\n            ax.plot(tf_time, tf_pred_values, 'b--', linewidth=1.5, label='TF Predicted', alpha=0.7)\n            \n            # Free Running ì˜ˆì¸¡ (ì „ì²´ ì‹œí€€ìŠ¤)\n            ax.plot(fr_time, fr_pred_values, 'r:', linewidth=2, label='FR Predicted', alpha=0.8)\n            \n            # ì´ˆê¸°ê°’ í‘œì‹œ\n            if len(fr_actual_values) > 0:\n                ax.plot(fr_time[0], fr_actual_values[0], 'go', markersize=6, label='Initial')\n            \n            # ì˜ˆì¸¡ ì‹œì‘ì  í‘œì‹œ\n            if len(fr_time) > 1:\n                ax.axvline(x=fr_time[1], color='gray', linestyle=':', alpha=0.5)\n            \n            # ê·¸ë˜í”„ ì„¤ì •\n            ax.set_title(f'Exp {exp_num}', fontsize=11)\n            ax.set_xlabel('Time', fontsize=9)\n            ax.set_ylabel(feat_name, fontsize=9)\n            ax.grid(True, alpha=0.3)\n            ax.legend(fontsize=8)\n            \n            # MSE ê³„ì‚° ë° í‘œì‹œ\n            tf_mse = np.mean((tf_actual_values - tf_pred_values)**2) if len(tf_pred_values) > 0 else 0\n            fr_mse = np.mean((fr_actual_values - fr_pred_values)**2) if len(fr_pred_values) > 0 else 0\n            \n            # í…ìŠ¤íŠ¸ ë°•ìŠ¤ë¡œ MSE í‘œì‹œ\n            textstr = f'TF: {tf_mse:.4f}\\nFR: {fr_mse:.4f}'\n            ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=8,\n                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n            \n            # yì¶• ë²”ìœ„ ì„¤ì •\n            all_values = np.concatenate([tf_actual_values, fr_actual_values, tf_pred_values, fr_pred_values])\n            y_min, y_max = all_values.min(), all_values.max()\n            y_range = y_max - y_min\n            if y_range > 0:\n                ax.set_ylim(y_min - 0.1*y_range, y_max + 0.1*y_range)\n        else:\n            ax.text(0.5, 0.5, f'No data\\nExp {exp_num}', \n                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n            ax.set_xticks([])\n            ax.set_yticks([])\n    \n    # ë¹ˆ subplot ì œê±°\n    for i in range(n_experiments, n_rows * n_cols):\n        row = i // n_cols\n        col = i % n_cols\n        if n_rows > 1:\n            axes[row, col].remove()\n        else:\n            if n_cols > 1:\n                axes[col].remove()\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"ğŸ‰ ëª¨ë“  featureì— ëŒ€í•œ Teacher Forcing vs Free Running ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!\")\nprint(\"ğŸ“Š ê·¸ë˜í”„ ì„¤ëª…:\")\nprint(\"   - ê²€ì€ìƒ‰ ì‹¤ì„ : ì‹¤ì œê°’ (Actual)\")\nprint(\"   - íŒŒë€ìƒ‰ ì ì„ : Teacher Forcing ì˜ˆì¸¡ê°’\")\nprint(\"   - ë¹¨ê°„ìƒ‰ ì ì„ : Free Running ì˜ˆì¸¡ê°’\")\nprint(\"   - ì´ˆë¡ìƒ‰ ì : ì´ˆê¸°ê°’\")\nprint(\"   - íšŒìƒ‰ ìˆ˜ì§ì„ : ì˜ˆì¸¡ ì‹œì‘ì \")\nprint(\"   - ê° subplot: ê°œë³„ ì‹¤í—˜ ê²°ê³¼\")\nprint(\"   - V, E ì œì™¸í•œ 8ê°œ feature ëª¨ë‘ í‘œì‹œ\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}