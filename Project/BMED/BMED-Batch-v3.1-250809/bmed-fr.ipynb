{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb1e7153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 BMED Autoregressive Model - Pretrained Model Loading\n",
      "📁 Target model file: BMED_TF_250816.pth\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# BMED Autoregressive Model - Pretrained Model Loading and Testing\n",
    "# 기존 학습된 모델(BMED_TF_250816.pth)을 불러와서 인공신경망 모델 구축\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"🚀 BMED Autoregressive Model - Pretrained Model Loading\")\n",
    "print(\"📁 Target model file: BMED_TF_250816.pth\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "wmv7iazlohb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수들\n",
    "def df_treat(name):\n",
    "    \"\"\"데이터 정규화 및 전처리\"\"\"\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "\n",
    "    exp_num_list = sorted(ndf['exp'].unique())\n",
    "    return df, ndf, range_mm, exp_num_list\n",
    "\n",
    "def seq_data(ndf, exp_num_list):\n",
    "    \"\"\"시퀀스 데이터 생성\"\"\"\n",
    "    seq = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "    \n",
    "    for exp in exp_num_list:\n",
    "        exp_df = ndf[ndf['exp'] == exp]\n",
    "        seq.append(exp_df[feature_cols].values)\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def pad_seq(seq):\n",
    "    \"\"\"시퀀스 패딩\"\"\"\n",
    "    max_len = max([len(s) for s in seq])\n",
    "    seq_len = [len(s) for s in seq]\n",
    "    pad_seq = pad_sequence([torch.tensor(s) for s in seq], batch_first=True, padding_value=-1)\n",
    "    return pad_seq, seq_len, max_len\n",
    "\n",
    "def gen_dataset(pad_seq, seq_len):\n",
    "    \"\"\"데이터셋 생성\"\"\"\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "h0dma8zvikk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 정의 (기존 학습된 모델과 동일한 구조)\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    \"\"\"LSTM layer with layer normalization applied to gates\"\"\"\n",
    "    def __init__(self, input_node, hidden_node):\n",
    "        super().__init__()\n",
    "        self.input_node = input_node\n",
    "        self.hidden_node = hidden_node\n",
    "\n",
    "        self.w_i = nn.Linear(input_node, 4 * hidden_node, bias=False)\n",
    "        self.w_h = nn.Linear(hidden_node, 4 * hidden_node, bias=False)\n",
    "\n",
    "        self.ln_i = nn.LayerNorm(hidden_node)\n",
    "        self.ln_h = nn.LayerNorm(hidden_node)\n",
    "        self.ln_g = nn.LayerNorm(hidden_node)\n",
    "        self.ln_o = nn.LayerNorm(hidden_node)\n",
    "\n",
    "        self.ln_c = nn.LayerNorm(hidden_node)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        gi = self.w_i(input)\n",
    "        gh = self.w_h(h_prev)\n",
    "        i_i, i_f, i_g, i_o = gi.chunk(4, dim=-1)\n",
    "        h_i, h_f, h_g, h_o = gh.chunk(4, dim=-1)\n",
    "\n",
    "        i_g = torch.sigmoid(self.ln_i(i_i + h_i))\n",
    "        f_g = torch.sigmoid(self.ln_h(i_f + h_f))\n",
    "        g_g = torch.tanh(self.ln_g(i_g + h_g))\n",
    "        o_g = torch.sigmoid(self.ln_o(i_o + h_o))\n",
    "\n",
    "        c_new = f_g * c_prev + i_g * g_g\n",
    "        c_new = self.ln_c(c_new)\n",
    "\n",
    "        h_new = o_g * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new\n",
    "class StateExtr(nn.Module):\n",
    "    \"\"\"State Extractor using LayerNorm LSTM\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        for _ in range(self.n_layer):\n",
    "            h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "            c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            layer_input = x_t\n",
    "            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "                h_new, c_new = lstm_cell(layer_input, (h_states[layer_idx], c_states[layer_idx]))\n",
    "\n",
    "                h_states[layer_idx] = h_new\n",
    "                c_states[layer_idx] = c_new\n",
    "\n",
    "                if layer_idx < len(self.lstm_cells) - 1:\n",
    "                    layer_input = self.dropout(h_new)\n",
    "                else:\n",
    "                    layer_input = h_new\n",
    "\n",
    "            outputs.append(layer_input)\n",
    "        \n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        normalized = self.final_layer_norm(masked_output)\n",
    "        return self.final_dropout(normalized)\n",
    "\n",
    "# Stateful State Extractor for Free Running Model\n",
    "class StatefulStateExtr(nn.Module):\n",
    "    \"\"\"Hidden state를 유지하며 sequential 처리하는 State Extractor\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Hidden states를 클래스 변수로 관리\n",
    "        self.h_states = None\n",
    "        self.c_states = None\n",
    "\n",
    "    def reset_states(self, batch_size=None, device=None):\n",
    "        \"\"\"새로운 시퀀스 시작 시 hidden state 초기화\"\"\"\n",
    "        if batch_size is not None and device is not None:\n",
    "            self.h_states = []\n",
    "            self.c_states = []\n",
    "            for _ in range(self.n_layer):\n",
    "                self.h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "                self.c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        else:\n",
    "            self.h_states = None\n",
    "            self.c_states = None\n",
    "\n",
    "    def forward_single_timestep(self, x_t):\n",
    "        \"\"\"단일 시점 처리 - hidden state 유지\"\"\"\n",
    "        batch_size = x_t.size(0)\n",
    "        device = x_t.device\n",
    "        \n",
    "        # 첫 번째 호출 시 hidden state 초기화\n",
    "        if self.h_states is None:\n",
    "            self.reset_states(batch_size, device)\n",
    "\n",
    "        layer_input = x_t\n",
    "        for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "            h_new, c_new = lstm_cell(layer_input, (self.h_states[layer_idx], self.c_states[layer_idx]))\n",
    "\n",
    "            # Hidden state 업데이트\n",
    "            self.h_states[layer_idx] = h_new\n",
    "            self.c_states[layer_idx] = c_new\n",
    "\n",
    "            if layer_idx < len(self.lstm_cells) - 1:\n",
    "                layer_input = self.dropout(h_new)\n",
    "            else:\n",
    "                layer_input = h_new\n",
    "\n",
    "        # 정규화 및 드롭아웃 적용\n",
    "        normalized = self.final_layer_norm(layer_input)\n",
    "        return self.final_dropout(normalized)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"기존 teacher forcing 방식과 호환성 유지\"\"\"\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # Teacher forcing 모드에서는 매번 hidden state 초기화\n",
    "        self.reset_states(batch_size, device)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "            output = self.forward_single_timestep(x_t)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        \n",
    "        # 마스킹 적용\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        return masked_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0jjbstv1vxnn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"Physical Change Decoder\"\"\"\n",
    "    def __init__(self, input_node, output_node, n_layer, hidden_node, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_node, output_node))\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CurrentPredictor(nn.Module):\n",
    "    \"\"\"Current Predictor\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_node, 1))\n",
    "    \n",
    "    def forward(self, new_state):\n",
    "        x = new_state\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "zd4or2cxr4r",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"Physics Constraint Layer with Current Prediction\"\"\"\n",
    "    def __init__(self, range_mm, current_predictor, eps=1e-2):\n",
    "        super().__init__()\n",
    "        self.sps = eps\n",
    "        self.current_predictor = current_predictor\n",
    "        self.register_buffer('range_mm_tensor', self._convert_range_to_tensor(range_mm))\n",
    "\n",
    "    def _convert_range_to_tensor(self, range_mm):\n",
    "        feature_names = ['V','E','VF','VA','VB','CFLA','CALA','CFK','CBK','I']\n",
    "        ranges = torch.zeros(len(feature_names),2)\n",
    "\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if name in range_mm:\n",
    "                ranges[i, 0] = range_mm[name]['min']\n",
    "                ranges[i, 1] = range_mm[name]['max']\n",
    "        \n",
    "        return ranges\n",
    "    \n",
    "    def normalize(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "    def denormalize(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return data * (max_val - min_val) + min_val\n",
    "\n",
    "    def forward(self, physical_changes, current_state):\n",
    "        V_idx, E_idx, VF_idx, VA_idx, VB_idx = 0, 1, 2, 3, 4\n",
    "        CFLA_idx, CALA_idx, CFK_idx, CBK_idx, I_idx = 5, 6, 7, 8, 9\n",
    "\n",
    "        VF = self.denormalize(current_state[..., 2:3], VF_idx)\n",
    "        VA = self.denormalize(current_state[..., 3:4], VA_idx)\n",
    "        VB = self.denormalize(current_state[..., 4:5], VB_idx)\n",
    "        CFLA = self.denormalize(current_state[..., 5:6], CFLA_idx)\n",
    "        CALA = self.denormalize(current_state[..., 6:7], CALA_idx)\n",
    "        CFK = self.denormalize(current_state[..., 7:8], CFK_idx)\n",
    "        CBK = self.denormalize(current_state[..., 8:9], CBK_idx)\n",
    "\n",
    "        dVA = physical_changes[..., 0:1]\n",
    "        dVB = physical_changes[..., 1:2]\n",
    "        dNALA = physical_changes[..., 2:3]\n",
    "        dNBK = physical_changes[..., 3:4]\n",
    "\n",
    "        NFLA = CFLA * VF\n",
    "        NALA = CALA * VA\n",
    "        NFK = CFK * VF\n",
    "        NBK = CBK * VB\n",
    "\n",
    "        nVF = VF - dVA - dVB\n",
    "        nVA = VA + dVA\n",
    "        nVB = VB + dVB\n",
    "\n",
    "        nVF = torch.clamp(nVF, min=self.sps)\n",
    "        nVA = torch.clamp(nVA, min=self.sps)\n",
    "        nVB = torch.clamp(nVB, min=self.sps)\n",
    "        \n",
    "        nNFLA = NFLA - torch.clamp(dNALA, min=0.0)\n",
    "        nNALA = NALA + torch.clamp(dNALA, min=0.0)\n",
    "        nNFK = NFK - torch.clamp(dNBK, min=0.0)\n",
    "        nNBK = NBK + torch.clamp(dNBK, min=0.0)\n",
    "\n",
    "        nNFLA = torch.clamp(nNFLA, min=0.0)\n",
    "        nNALA = torch.clamp(nNALA, min=0.0)\n",
    "        nNFK = torch.clamp(nNFK, min=0.0)\n",
    "        nNBK = torch.clamp(nNBK, min=0.0)\n",
    "\n",
    "        nCFLA = nNFLA / nVF\n",
    "        nCALA = nNALA / nVA\n",
    "        nCFK = nNFK / nVF\n",
    "        nCBK = nNBK / nVB\n",
    "\n",
    "        V = current_state[..., 0:1]\n",
    "        E = current_state[..., 1:2]\n",
    "        nVF_norm = self.normalize(nVF, VF_idx)\n",
    "        nVA_norm = self.normalize(nVA, VA_idx)\n",
    "        nVB_norm = self.normalize(nVB, VB_idx)\n",
    "        nCFLA_norm = self.normalize(nCFLA, CFLA_idx)\n",
    "        nCALA_norm = self.normalize(nCALA, CALA_idx)\n",
    "        nCFK_norm = self.normalize(nCFK, CFK_idx)\n",
    "        nCBK_norm = self.normalize(nCBK, CBK_idx)\n",
    "\n",
    "        temp_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        nI_pred_norm = self.current_predictor(temp_state)\n",
    "        \n",
    "        nI_real = self.denormalize(nI_pred_norm, I_idx)\n",
    "        nI_real = torch.clamp(nI_real, min=0.0)\n",
    "        nI_norm = self.normalize(nI_real, I_idx)\n",
    "\n",
    "        next_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm, nI_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0b7fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 통합된 BMEDAutoregressiveModel 구현 완료!\n"
     ]
    }
   ],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"통합 BMED Autoregressive Model - Teacher Forcing & Free Running 모드 지원\"\"\"\n",
    "    def __init__(self, state_extr_params, decoder_params, current_predictor_params, range_mm):\n",
    "        super().__init__()\n",
    "        self.state_extr = StateExtr(**state_extr_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.current_predictor = CurrentPredictor(**current_predictor_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer(range_mm, self.current_predictor)\n",
    "        \n",
    "        # Free running을 위한 hidden state 관리\n",
    "        self._hidden_states = None\n",
    "        self._cell_states = None\n",
    "\n",
    "    def _reset_hidden_states(self, batch_size, device):\n",
    "        \"\"\"Free running 시작 시 hidden state 초기화\"\"\"\n",
    "        self._hidden_states = []\n",
    "        self._cell_states = []\n",
    "        for _ in range(self.state_extr.n_layer):\n",
    "            self._hidden_states.append(torch.zeros(batch_size, self.state_extr.hidden_node, device=device))\n",
    "            self._cell_states.append(torch.zeros(batch_size, self.state_extr.hidden_node, device=device))\n",
    "\n",
    "    def teacher_forcing_forward(self, x, seq_len):\n",
    "        \"\"\"Teacher Forcing 모드: 기존 방식\"\"\"\n",
    "        hidden_states = self.state_extr(x, seq_len)\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        new_x = self.physics_constraint(physical_changes, x)\n",
    "        return new_x\n",
    "\n",
    "    def free_running_forward(self, initial_state, target_length):\n",
    "        \"\"\"\n",
    "        Free Running 모드: 초기 상태만으로 전체 시퀀스 생성\n",
    "        Args:\n",
    "            initial_state: [batch, features] - 초기 상태 (모든 10개 특성 포함)\n",
    "            target_length: int - 예측할 시퀀스 길이\n",
    "        Returns:\n",
    "            predictions: [batch, target_length, features] - 예측된 전체 시퀀스\n",
    "        \"\"\"\n",
    "        batch_size = initial_state.size(0)\n",
    "        feature_size = initial_state.size(1)\n",
    "        device = initial_state.device\n",
    "        \n",
    "        # Hidden state 초기화\n",
    "        self._reset_hidden_states(batch_size, device)\n",
    "        \n",
    "        # 예측 결과 저장\n",
    "        predictions = torch.zeros(batch_size, target_length, feature_size, device=device)\n",
    "        current_state = initial_state.clone()\n",
    "        \n",
    "        for t in range(target_length):\n",
    "            predictions[:, t, :] = current_state\n",
    "            \n",
    "            if t < target_length - 1:\n",
    "                # 현재 상태에서 I(전류) 제거하여 LSTM 입력 생성 (9개 특성)\n",
    "                lstm_input = current_state[:, :-1]  # [batch, 9]\n",
    "                \n",
    "                # LSTM forward with hidden state maintenance\n",
    "                hidden_output = self._forward_lstm_single_step(lstm_input)\n",
    "                \n",
    "                # Physical change 예측\n",
    "                physical_changes = self.physical_decoder(hidden_output.unsqueeze(1))  # [batch, 1, output]\n",
    "                \n",
    "                # Physics constraint로 다음 상태 계산\n",
    "                current_state_expanded = current_state.unsqueeze(1)  # [batch, 1, features]\n",
    "                next_state = self.physics_constraint(physical_changes, current_state_expanded)\n",
    "                current_state = next_state.squeeze(1)  # [batch, features]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def _forward_lstm_single_step(self, x_t):\n",
    "        \"\"\"단일 시점 LSTM forward (hidden state 유지)\"\"\"\n",
    "        layer_input = x_t\n",
    "        \n",
    "        for layer_idx, lstm_cell in enumerate(self.state_extr.lstm_cells):\n",
    "            h_new, c_new = lstm_cell(layer_input, \n",
    "                                   (self._hidden_states[layer_idx], self._cell_states[layer_idx]))\n",
    "            \n",
    "            # Hidden state 업데이트\n",
    "            self._hidden_states[layer_idx] = h_new\n",
    "            self._cell_states[layer_idx] = c_new\n",
    "            \n",
    "            # Dropout 적용 (마지막 layer 제외)\n",
    "            if layer_idx < len(self.state_extr.lstm_cells) - 1:\n",
    "                layer_input = self.state_extr.dropout(h_new)\n",
    "            else:\n",
    "                layer_input = h_new\n",
    "        \n",
    "        # 최종 정규화 및 드롭아웃\n",
    "        normalized = self.state_extr.final_layer_norm(layer_input)\n",
    "        return self.state_extr.final_dropout(normalized)\n",
    "\n",
    "    def forward(self, x, seq_len=None, mode='teacher_forcing', target_length=None):\n",
    "        \"\"\"\n",
    "        통합 forward 메소드\n",
    "        Args:\n",
    "            x: 입력 데이터\n",
    "            seq_len: 시퀀스 길이 (teacher forcing 모드에서 사용)\n",
    "            mode: 'teacher_forcing' 또는 'free_running'\n",
    "            target_length: free running에서 예측할 길이\n",
    "        \"\"\"\n",
    "        if mode == 'teacher_forcing':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len은 teacher_forcing 모드에서 필수입니다\")\n",
    "            return self.teacher_forcing_forward(x, seq_len)\n",
    "        \n",
    "        elif mode == 'free_running':\n",
    "            if target_length is None:\n",
    "                raise ValueError(\"target_length는 free_running 모드에서 필수입니다\")\n",
    "            return self.free_running_forward(x, target_length)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 모드: {mode}. 'teacher_forcing' 또는 'free_running'을 사용하세요.\")\n",
    "\n",
    "print(\"✅ 통합된 BMEDAutoregressiveModel 구현 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2f55d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 준비 함수들 구현 완료!\n",
      "   - tf_data: Teacher Forcing용 입력/타겟 준비\n",
      "   - fr_data: Free Running용 초기상태/타겟 준비\n",
      "   - masked_mse_loss: 마스킹된 MSE 손실 함수\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비 함수들 - Teacher Forcing & Free Running 지원\n",
    "\n",
    "def tf_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Teacher Forcing용 데이터 준비 (원본 bmed_tf_learning.ipynb와 동일)\n",
    "    \n",
    "    Args:\n",
    "        input_seq: [batch, seq_len, features] - 전체 시퀀스 (10개 특성)\n",
    "        seq_len: [batch] - 각 시퀀스의 실제 길이\n",
    "    \n",
    "    Returns:\n",
    "        inputs: [batch, seq_len-1, 9] - LSTM 입력 (I 제외한 9개 특성)\n",
    "        targets: [batch, seq_len-1, 10] - 예측 타겟 (전체 10개 특성)\n",
    "        target_seq_len: [batch] - 타겟 시퀀스 길이 (seq_len - 1)\n",
    "    \"\"\"\n",
    "    # 입력: 시퀀스의 마지막 시점 제외, I(전류) 특성 제외\n",
    "    inputs = input_seq[:, :-1, :-1]  # [batch, seq_len-1, 9]\n",
    "    \n",
    "    # 타겟: 시퀀스의 첫 번째 시점 제외, 모든 특성 포함\n",
    "    targets = input_seq[:, 1:, :]    # [batch, seq_len-1, 10]\n",
    "    \n",
    "    # 타겟 시퀀스 길이\n",
    "    target_seq_len = seq_len - 1\n",
    "    \n",
    "    return inputs, targets, target_seq_len\n",
    "\n",
    "def fr_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Free Running용 데이터 준비\n",
    "    \n",
    "    Args:\n",
    "        input_seq: [batch, seq_len, features] - 전체 시퀀스 (10개 특성)\n",
    "        seq_len: [batch] - 각 시퀀스의 실제 길이\n",
    "    \n",
    "    Returns:\n",
    "        initial_states: [batch, 10] - 초기 상태 (전체 10개 특성)\n",
    "        targets: [batch, seq_len, 10] - 예측 타겟 (전체 시퀀스)\n",
    "        target_lengths: [batch] - 예측할 시퀀스 길이\n",
    "    \"\"\"\n",
    "    # 초기 상태: 첫 번째 시점\n",
    "    initial_states = input_seq[:, 0, :]  # [batch, 10]\n",
    "    \n",
    "    # 타겟: 전체 시퀀스\n",
    "    targets = input_seq  # [batch, seq_len, 10]\n",
    "    \n",
    "    # 예측할 길이\n",
    "    target_lengths = seq_len  # [batch]\n",
    "    \n",
    "    return initial_states, targets, target_lengths\n",
    "\n",
    "def masked_mse_loss(pred, target, seq_len):\n",
    "    \"\"\"Masked MSE Loss function\"\"\"\n",
    "    batch_size, max_len, features = pred.shape\n",
    "    seq_len_cpu = seq_len.detach().cpu().long()\n",
    "\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "    mask = mask.float().to(pred.device)\n",
    "\n",
    "    loss = F.mse_loss(pred, target, reduction='none')\n",
    "    masked_loss = loss * mask.unsqueeze(-1)\n",
    "\n",
    "    total_loss = masked_loss.sum()\n",
    "    total_elements = mask.sum()\n",
    "\n",
    "    masked_loss = total_loss / total_elements\n",
    "    return masked_loss\n",
    "\n",
    "print(\"✅ 데이터 준비 함수들 구현 완료!\")\n",
    "print(\"   - tf_data: Teacher Forcing용 입력/타겟 준비\")\n",
    "print(\"   - fr_data: Free Running용 초기상태/타겟 준비\") \n",
    "print(\"   - masked_mse_loss: 마스킹된 MSE 손실 함수\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2fd2b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 데이터 로드 중: BMED_DATA_AG.csv\n",
      "   - 실험 수: 24\n",
      "   - 최대 시퀀스 길이: 33\n",
      "🔧 사용 중인 장치: cuda\n",
      "📥 모델 파일 로드 중: BMED_TF_250816.pth\n",
      "✅ 모델 설정 정보 발견:\n",
      "   - State Extractor: input=9, hidden=64, layers=4, dropout=0.1\n",
      "   - Decoder: input=64, hidden=128, layers=4, dropout=0.1\n",
      "   - Current Predictor: input=9, hidden=96, layers=2, dropout=0.1\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 전처리\n",
    "data_path = \"BMED_DATA_AG.csv\"\n",
    "print(f\"📊 데이터 로드 중: {data_path}\")\n",
    "df, ndf, range_mm, exp_num_list = df_treat(data_path)\n",
    "\n",
    "# 시퀀스 데이터 생성\n",
    "seq = seq_data(ndf, exp_num_list)\n",
    "pad_sequences, seq_lengths, max_length = pad_seq(seq)\n",
    "dataset = gen_dataset(pad_sequences, seq_lengths)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"   - 실험 수: {len(exp_num_list)}\")\n",
    "print(f\"   - 최대 시퀀스 길이: {max_length}\")\n",
    "\n",
    "# 저장된 모델 로드\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 사용 중인 장치: {device}\")\n",
    "\n",
    "# 모델 파일 경로\n",
    "model_path = \"BMED_TF_250816.pth\"\n",
    "\n",
    "# 저장된 모델 체크포인트 로드\n",
    "print(f\"📥 모델 파일 로드 중: {model_path}\")\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# 모델 설정 정보 확인\n",
    "model_config = checkpoint['model_config']\n",
    "state_extr_params = model_config['state_extr_params']\n",
    "decoder_params = model_config['decoder_params']\n",
    "current_predictor_params = model_config['current_predictor_params']\n",
    "range_mm = model_config['range_mm']\n",
    "print(\"✅ 모델 설정 정보 발견:\")\n",
    "print(f\"   - State Extractor: input={state_extr_params['input_node']}, hidden={state_extr_params['hidden_node']}, layers={state_extr_params['n_layer']}, dropout={state_extr_params['dropout']}\")\n",
    "print(f\"   - Decoder: input={decoder_params['input_node']}, hidden={decoder_params['hidden_node']}, layers={decoder_params['n_layer']}, dropout={decoder_params['dropout']}\")\n",
    "print(f\"   - Current Predictor: input={current_predictor_params['input_node']}, hidden={current_predictor_params['hidden_node']}, layers={current_predictor_params['n_layer']}, dropout={current_predictor_params['dropout']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b323d277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 통합 모델 생성 중...\n",
      "⚡ Pretrained 가중치 로드 중...\n",
      "✅ 통합 모델 구성 완료!\n",
      "   - 모델 장치: cuda:0\n",
      "   - 모델 모드: eval\n",
      "\n",
      "🔥 Teacher Forcing 모드 테스트...\n",
      "   - 원본 입력 크기: torch.Size([4, 33, 10])\n",
      "   - 시퀀스 길이: tensor([17, 21, 21, 25], device='cuda:0')\n",
      "   - TF 입력 크기: torch.Size([4, 32, 9]) (I 제외한 9개 특성)\n",
      "   - TF 타겟 크기: torch.Size([4, 32, 10]) (전체 10개 특성)\n",
      "   - TF 시퀀스 길이: tensor([16, 20, 20, 24], device='cuda:0')\n",
      "   - Teacher Forcing 출력 크기: torch.Size([4, 32, 10])\n",
      "   - 출력 범위: [-1.0000, 583.3128]\n",
      "\n",
      "🎯 Free Running 모드 테스트...\n",
      "   - 초기 상태 크기: torch.Size([1, 10])\n",
      "   - 예측 길이: 17\n",
      "   - Free Running 출력 크기: torch.Size([1, 17, 10])\n",
      "   - 출력 범위: [0.0000, 0.5036]\n",
      "\n",
      "📊 Teacher Forcing vs Free Running 비교...\n",
      "   - Teacher Forcing MSE: 18260.738281\n",
      "   - Free Running MSE: 0.000003\n",
      "   - 첫 번째 시점 차이 (0이어야 함): 0.000000\n",
      "   - 모드 간 예측 차이: 0.001068\n",
      "\n",
      "✅ 통합 모델 테스트 완료!\n",
      "   → Teacher Forcing: tf_data 함수로 올바른 입력/타겟 준비\n",
      "   → Free Running: fr_data 함수로 초기상태/타겟 준비\n",
      "   → 두 모드 모두 정상 동작 확인\n"
     ]
    }
   ],
   "source": [
    "# 통합된 BMED Autoregressive Model 테스트 - tf_data 함수 사용\n",
    "\n",
    "# 통합 모델 생성 및 pretrained 가중치 로드\n",
    "print(\"📦 통합 모델 생성 중...\")\n",
    "unified_model = BMEDAutoregressiveModel(\n",
    "    state_extr_params=state_extr_params,\n",
    "    decoder_params=decoder_params, \n",
    "    current_predictor_params=current_predictor_params,\n",
    "    range_mm=range_mm\n",
    ").to(device)\n",
    "\n",
    "# Pretrained 가중치 로드\n",
    "print(\"⚡ Pretrained 가중치 로드 중...\")\n",
    "unified_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "unified_model.eval()\n",
    "\n",
    "print(\"✅ 통합 모델 구성 완료!\")\n",
    "print(f\"   - 모델 장치: {next(unified_model.parameters()).device}\")\n",
    "print(f\"   - 모델 모드: {'eval' if not unified_model.training else 'train'}\")\n",
    "\n",
    "# 샘플 데이터로 Teacher Forcing 테스트\n",
    "print(\"\\n🔥 Teacher Forcing 모드 테스트...\")\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(dataloader))\n",
    "    sample_input, sample_seq_len = sample_batch\n",
    "    sample_input = sample_input.to(device)\n",
    "    sample_seq_len = sample_seq_len.to(device)\n",
    "    \n",
    "    print(f\"   - 원본 입력 크기: {sample_input.shape}\")\n",
    "    print(f\"   - 시퀀스 길이: {sample_seq_len}\")\n",
    "    \n",
    "    # Teacher Forcing 데이터 준비 (tf_data 함수 사용)\n",
    "    tf_inputs, tf_targets, tf_seq_len = tf_data(sample_input, sample_seq_len)\n",
    "    print(f\"   - TF 입력 크기: {tf_inputs.shape} (I 제외한 9개 특성)\")\n",
    "    print(f\"   - TF 타겟 크기: {tf_targets.shape} (전체 10개 특성)\")\n",
    "    print(f\"   - TF 시퀀스 길이: {tf_seq_len}\")\n",
    "    \n",
    "    # Teacher forcing 예측\n",
    "    tf_output = unified_model(tf_inputs, tf_seq_len, mode='teacher_forcing')\n",
    "    print(f\"   - Teacher Forcing 출력 크기: {tf_output.shape}\")\n",
    "    print(f\"   - 출력 범위: [{tf_output.min().item():.4f}, {tf_output.max().item():.4f}]\")\n",
    "\n",
    "# Free Running 모드 테스트\n",
    "print(\"\\n🎯 Free Running 모드 테스트...\")\n",
    "with torch.no_grad():\n",
    "    # Free Running 데이터 준비 (fr_data 함수 사용)\n",
    "    fr_initial_states, fr_targets, fr_lengths = fr_data(sample_input, sample_seq_len)\n",
    "    \n",
    "    # 첫 번째 샘플만 사용\n",
    "    initial_state = fr_initial_states[0:1, :]  # [1, 10]\n",
    "    target_length = int(fr_lengths[0].item())  # 실제 시퀀스 길이\n",
    "    \n",
    "    print(f\"   - 초기 상태 크기: {initial_state.shape}\")\n",
    "    print(f\"   - 예측 길이: {target_length}\")\n",
    "    \n",
    "    # Free running 예측\n",
    "    fr_output = unified_model(initial_state, mode='free_running', target_length=target_length)\n",
    "    print(f\"   - Free Running 출력 크기: {fr_output.shape}\")\n",
    "    print(f\"   - 출력 범위: [{fr_output.min().item():.4f}, {fr_output.max().item():.4f}]\")\n",
    "\n",
    "# 두 모드 간 차이 비교\n",
    "print(\"\\n📊 Teacher Forcing vs Free Running 비교...\")\n",
    "with torch.no_grad():\n",
    "    # Teacher Forcing: 타겟과 비교 (시점 1부터)\n",
    "    tf_single = tf_output[0:1, :, :]  # [1, seq_len-1, 10]\n",
    "    tf_target = tf_targets[0:1, :, :]  # [1, seq_len-1, 10]\n",
    "    \n",
    "    # Free Running: 전체 시퀀스와 비교\n",
    "    fr_single = fr_output  # [1, seq_len, 10]\n",
    "    fr_target = fr_targets[0:1, :target_length, :]  # [1, seq_len, 10]\n",
    "    \n",
    "    # Teacher Forcing 성능\n",
    "    tf_loss = F.mse_loss(tf_single, tf_target)\n",
    "    print(f\"   - Teacher Forcing MSE: {tf_loss.item():.6f}\")\n",
    "    \n",
    "    # Free Running 성능\n",
    "    fr_loss = F.mse_loss(fr_single, fr_target)\n",
    "    print(f\"   - Free Running MSE: {fr_loss.item():.6f}\")\n",
    "    \n",
    "    # 첫 번째 시점 비교 (Free Running의 첫 시점은 초기값과 동일해야 함)\n",
    "    first_step_diff = torch.abs(fr_single[:, 0, :] - initial_state).mean()\n",
    "    print(f\"   - 첫 번째 시점 차이 (0이어야 함): {first_step_diff.item():.6f}\")\n",
    "    \n",
    "    # 동일 시점 비교 (Teacher Forcing 예측 vs Free Running 예측)\n",
    "    # TF는 시점 1부터, FR은 시점 1부터 비교\n",
    "    if tf_single.size(1) > 0 and fr_single.size(1) > 1:\n",
    "        comparison_length = min(tf_single.size(1), fr_single.size(1) - 1)\n",
    "        tf_comparison = tf_single[:, :comparison_length, :]\n",
    "        fr_comparison = fr_single[:, 1:comparison_length+1, :]  # FR의 시점 1부터\n",
    "        \n",
    "        mode_diff = torch.abs(tf_comparison - fr_comparison).mean()\n",
    "        print(f\"   - 모드 간 예측 차이: {mode_diff.item():.6f}\")\n",
    "\n",
    "print(\"\\n✅ 통합 모델 테스트 완료!\")\n",
    "print(\"   → Teacher Forcing: tf_data 함수로 올바른 입력/타겟 준비\")\n",
    "print(\"   → Free Running: fr_data 함수로 초기상태/타겟 준비\")\n",
    "print(\"   → 두 모드 모두 정상 동작 확인\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2t1dwt1ekm",
   "metadata": {},
   "outputs": [],
   "source": "# Teacher Forcing vs Free Running 전체 실험 비교 시각화 - Feature별 Subplot\n\nprint(\"🎯 Teacher Forcing vs Free Running 전체 실험 비교 시각화\")\nprint(\"=\"*60)\n\n# 모델을 평가 모드로 설정\nunified_model.eval()\n\n# 예측 결과와 실제 값을 저장할 딕셔너리\ntf_predictions_dict = {}\nfr_predictions_dict = {}\nactual_dict = {}\n\n# V와 E를 제외한 feature들의 인덱스와 이름\nfeature_names = ['VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\nfeature_indices = [2, 3, 4, 5, 6, 7, 8, 9]  # V(0), E(1)를 제외한 인덱스\n\n# 전체 실험에 대해 예측 수행\nwith torch.no_grad():\n    for exp_num in exp_num_list:\n        # 실험 데이터 추출\n        exp_data = ndf[ndf['exp'] == exp_num].copy()\n        if len(exp_data) == 0:\n            continue\n        \n        # 특성 컬럼들\n        feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n        \n        # 실험 데이터를 tensor로 변환\n        exp_tensor = torch.tensor(exp_data[feature_cols].values).float().unsqueeze(0).to(device)  # [1, seq_len, 10]\n        seq_length = len(exp_data)\n        seq_len_tensor = torch.tensor([seq_length]).to(device)\n        \n        # Teacher Forcing 데이터 준비 및 예측\n        tf_inputs, tf_targets, tf_seq_len = tf_data(exp_tensor, seq_len_tensor)\n        tf_pred = unified_model(tf_inputs, tf_seq_len, mode='teacher_forcing')\n        \n        # Free Running 데이터 준비 및 예측\n        fr_initial_states, fr_targets, fr_lengths = fr_data(exp_tensor, seq_len_tensor)\n        initial_state = fr_initial_states  # [1, 10]\n        target_length = int(fr_lengths[0].item())\n        fr_pred = unified_model(initial_state, mode='free_running', target_length=target_length)\n        \n        # CPU로 이동하고 numpy 변환\n        tf_pred_np = tf_pred[0].cpu().numpy()  # [seq_len-1, 10]\n        fr_pred_np = fr_pred[0].cpu().numpy()  # [seq_len, 10]\n        actual_tf_np = tf_targets[0].cpu().numpy()  # [seq_len-1, 10] (TF targets)\n        actual_fr_np = fr_targets[0, :target_length].cpu().numpy()  # [seq_len, 10] (FR targets)\n        \n        # 딕셔너리에 저장\n        tf_predictions_dict[exp_num] = tf_pred_np\n        fr_predictions_dict[exp_num] = fr_pred_np\n        actual_dict[exp_num] = {\n            'tf': actual_tf_np,  # Teacher Forcing 타겟 (시점 1부터)\n            'fr': actual_fr_np,  # Free Running 타겟 (전체 시퀀스)\n            'time_tf': exp_data['t'].values[1:len(tf_pred_np)+1],  # TF 시간축\n            'time_fr': exp_data['t'].values[:len(fr_pred_np)]      # FR 시간축\n        }\n\nprint(f\"✅ {len(tf_predictions_dict)}개 실험에 대한 예측 완료\")\n\n# V와 E를 제외한 각 feature별로 그래프 생성\nfor feat_idx, feat_name in zip(feature_indices, feature_names):\n    # subplot 개수 계산 (행과 열 최적화)\n    n_experiments = len(exp_num_list)\n    n_cols = min(6, n_experiments)  # 최대 6열\n    n_rows = (n_experiments + n_cols - 1) // n_cols  # 필요한 행 수\n    \n    # 그래프 크기 설정\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*3))\n    fig.suptitle(f'Feature: {feat_name} - Teacher Forcing vs Free Running vs Actual', fontsize=16, fontweight='bold')\n    \n    # subplot이 1개일 경우 리스트로 변환\n    if n_experiments == 1:\n        axes = [axes]\n    elif n_rows == 1:\n        axes = axes.reshape(1, -1)\n    \n    # 각 실험에 대해 subplot 생성\n    for i, exp_num in enumerate(exp_num_list):\n        row = i // n_cols\n        col = i % n_cols\n        \n        if n_rows > 1:\n            ax = axes[row, col]\n        else:\n            ax = axes[col] if n_cols > 1 else axes[0]\n        \n        # 예측값과 실제값 가져오기\n        if exp_num in tf_predictions_dict:\n            tf_pred_values = tf_predictions_dict[exp_num][:, feat_idx]\n            fr_pred_values = fr_predictions_dict[exp_num][:, feat_idx]\n            tf_actual_values = actual_dict[exp_num]['tf'][:, feat_idx]\n            fr_actual_values = actual_dict[exp_num]['fr'][:, feat_idx]\n            tf_time = actual_dict[exp_num]['time_tf'][:len(tf_pred_values)]\n            fr_time = actual_dict[exp_num]['time_fr'][:len(fr_pred_values)]\n            \n            # 그래프 그리기\n            # 실제값 (Free Running은 전체, Teacher Forcing은 시점 1부터)\n            ax.plot(fr_time, fr_actual_values, 'k-', linewidth=2, label='Actual', alpha=0.8)\n            \n            # Teacher Forcing 예측 (시점 1부터)\n            ax.plot(tf_time, tf_pred_values, 'b--', linewidth=1.5, label='TF Predicted', alpha=0.7)\n            \n            # Free Running 예측 (전체 시퀀스)\n            ax.plot(fr_time, fr_pred_values, 'r:', linewidth=2, label='FR Predicted', alpha=0.8)\n            \n            # 초기값 표시\n            if len(fr_actual_values) > 0:\n                ax.plot(fr_time[0], fr_actual_values[0], 'go', markersize=6, label='Initial')\n            \n            # 예측 시작점 표시\n            if len(fr_time) > 1:\n                ax.axvline(x=fr_time[1], color='gray', linestyle=':', alpha=0.5)\n            \n            # 그래프 설정\n            ax.set_title(f'Exp {exp_num}', fontsize=11)\n            ax.set_xlabel('Time', fontsize=9)\n            ax.set_ylabel(feat_name, fontsize=9)\n            ax.grid(True, alpha=0.3)\n            ax.legend(fontsize=8)\n            \n            # MSE 계산 및 표시\n            tf_mse = np.mean((tf_actual_values - tf_pred_values)**2) if len(tf_pred_values) > 0 else 0\n            fr_mse = np.mean((fr_actual_values - fr_pred_values)**2) if len(fr_pred_values) > 0 else 0\n            \n            # 텍스트 박스로 MSE 표시\n            textstr = f'TF: {tf_mse:.4f}\\nFR: {fr_mse:.4f}'\n            ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=8,\n                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n            \n            # y축 범위 설정\n            all_values = np.concatenate([tf_actual_values, fr_actual_values, tf_pred_values, fr_pred_values])\n            y_min, y_max = all_values.min(), all_values.max()\n            y_range = y_max - y_min\n            if y_range > 0:\n                ax.set_ylim(y_min - 0.1*y_range, y_max + 0.1*y_range)\n        else:\n            ax.text(0.5, 0.5, f'No data\\nExp {exp_num}', \n                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n            ax.set_xticks([])\n            ax.set_yticks([])\n    \n    # 빈 subplot 제거\n    for i in range(n_experiments, n_rows * n_cols):\n        row = i // n_cols\n        col = i % n_cols\n        if n_rows > 1:\n            axes[row, col].remove()\n        else:\n            if n_cols > 1:\n                axes[col].remove()\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"🎉 모든 feature에 대한 Teacher Forcing vs Free Running 비교 그래프 생성 완료!\")\nprint(\"📊 그래프 설명:\")\nprint(\"   - 검은색 실선: 실제값 (Actual)\")\nprint(\"   - 파란색 점선: Teacher Forcing 예측값\")\nprint(\"   - 빨간색 점선: Free Running 예측값\")\nprint(\"   - 초록색 점: 초기값\")\nprint(\"   - 회색 수직선: 예측 시작점\")\nprint(\"   - 각 subplot: 개별 실험 결과\")\nprint(\"   - V, E 제외한 8개 feature 모두 표시\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}