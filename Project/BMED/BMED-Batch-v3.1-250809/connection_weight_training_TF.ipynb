{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "993e6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "import math\n",
    "import optuna\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42dc2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'Using device: {device}')\n",
    "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    else:\n",
    "        print(f'Using device: {device}')\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "53a49cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(name):\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "\n",
    "    # Get the unique experiment numbers in order\n",
    "    exp_num_list = sorted(ndf['exp'].unique())\n",
    "    return ndf, exp_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d900108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data(ndf):\n",
    "    seq = []\n",
    "    # CBLA, CAK만 제거: 학습 불안정성 때문에\n",
    "    # 전류(I)는 포함: ground truth로 사용하여 예측 성능 비교\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "    \n",
    "    for exp in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp].sort_values(by='t')\n",
    "        seq.append(exp_data[feature_cols].values)\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c2021b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq):\n",
    "    max_len = max([len(seq) for seq in seq])\n",
    "    seq_len = [len(seq) for seq in seq]\n",
    "    pad_seq = pad_sequence([torch.tensor(seq) for seq in seq], batch_first=True, padding_value=-1)\n",
    "\n",
    "    return pad_seq, seq_len, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f0ddc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(pad_seq, seq_len):\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d67ed67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaders(dataset, exp_num_list, batch_size=4):\n",
    "    \"\"\"\n",
    "    Split the dataset into train/val/test with 8:1:1 ratio\n",
    "    \n",
    "    Args:\n",
    "        dataset: TensorDataset\n",
    "        exp_num_list: list of experiment numbers\n",
    "        batch_size: batch size\n",
    "        random_state: random seed\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # required train experiment numbers\n",
    "    required_train_exps = [0, 2, 4, 5, 8, 9, 10, 13, 15, 17, 20, 22, 24, 26, 33, 34]\n",
    "    \n",
    "    # all experiment numbers\n",
    "    all_exps = exp_num_list\n",
    "    total_exps = len(all_exps)\n",
    "    \n",
    "    # batch_size\n",
    "    batch_size = math.ceil(len(dataset)/10)\n",
    "\n",
    "    # 8:1:1 ratio\n",
    "    train_count = int(total_exps * 0.8)\n",
    "    val_count = math.ceil(total_exps * 0.1)\n",
    "    \n",
    "    # remaining experiments\n",
    "    remaining_exps = [exp for exp in all_exps if exp not in required_train_exps]\n",
    "    \n",
    "    # number of experiments to add to train\n",
    "    additional_train_needed = train_count - len(required_train_exps)\n",
    "    \n",
    "    if additional_train_needed < 0:\n",
    "        raise ValueError(\"The number of required train experiments is greater than the total train set. Please adjust required_train_exps.\")\n",
    "    \n",
    "    # shuffle remaining experiments\n",
    "    np.random.shuffle(remaining_exps)\n",
    "    \n",
    "    # split remaining experiments into train, val, test\n",
    "    train_exps = required_train_exps + remaining_exps[:additional_train_needed]\n",
    "    val_exps = remaining_exps[additional_train_needed:additional_train_needed + val_count]\n",
    "    test_exps = remaining_exps[additional_train_needed + val_count:]\n",
    "    \n",
    "    print(f\"Actual split:\")\n",
    "    print(f\"  Train: {sorted(train_exps)} ({len(train_exps)} experiments)\")\n",
    "    print(f\"  Val: {sorted(val_exps)} ({len(val_exps)} experiments)\")  \n",
    "    print(f\"  Test: {sorted(test_exps)} ({len(test_exps)} experiments)\")\n",
    "    \n",
    "    # find indices of each experiment (exp_num_list and dataset have the same order)\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for idx, exp in enumerate(all_exps):\n",
    "        if exp in train_exps:\n",
    "            train_indices.append(idx)\n",
    "        elif exp in val_exps:\n",
    "            val_indices.append(idx)\n",
    "        elif exp in test_exps:\n",
    "            test_indices.append(idx)\n",
    "    \n",
    "    # split dataset into train, val, test\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "    \n",
    "    # create DataLoader\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nCompleted DataLoader creation:\")\n",
    "    print(f\"  Train: {len(train_subset) if train_subset else 0} sequences\")\n",
    "    print(f\"  Val: {len(val_subset) if val_subset else 0} sequences\")\n",
    "    print(f\"  Test: {len(test_subset) if test_subset else 0} sequences\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d07f3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormLSTMCell(nn.Module):\n",
    "    \"\"\"LSTM Cell with Layer Normalization applied to gates\"\"\"\n",
    "    def __init__(self, input_node, hidden_node):\n",
    "        super().__init__()\n",
    "        self.input_node = input_node\n",
    "        self.hidden_node = hidden_node\n",
    "        \n",
    "        # Input-to-hidden and hidden-to-hidden transformations\n",
    "        self.weight_ih = nn.Linear(input_node, 4 * hidden_node, bias=False)\n",
    "        self.weight_hh = nn.Linear(hidden_node, 4 * hidden_node, bias=False)\n",
    "        \n",
    "        # Layer normalization for each gate\n",
    "        self.ln_i = nn.LayerNorm(hidden_node)  # Input gate\n",
    "        self.ln_f = nn.LayerNorm(hidden_node)  # Forget gate  \n",
    "        self.ln_g = nn.LayerNorm(hidden_node)  # Cell gate\n",
    "        self.ln_o = nn.LayerNorm(hidden_node)  # Output gate\n",
    "        \n",
    "        # Cell state layer norm\n",
    "        self.ln_c = nn.LayerNorm(hidden_node)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        \n",
    "        # Input-to-hidden and hidden-to-hidden transformations\n",
    "        gi = self.weight_ih(input)    # [batch, 4*hidden_size] - 입력에 대한 4개 게이트 계산\n",
    "        gh = self.weight_hh(h_prev)   # [batch, 4*hidden_size] - 이전 히든 상태에 대한 4개 게이트 계산\n",
    "        i_i, i_f, i_g, i_o = gi.chunk(4, 1)\n",
    "        h_i, h_f, h_g, h_o = gh.chunk(4, 1)\n",
    "        \n",
    "        # Apply layer normalization to each gate\n",
    "        i_gate = torch.sigmoid(self.ln_i(i_i + h_i))\n",
    "        f_gate = torch.sigmoid(self.ln_f(i_f + h_f))  \n",
    "        g_gate = torch.tanh(self.ln_g(i_g + h_g))\n",
    "        o_gate = torch.sigmoid(self.ln_o(i_o + h_o))\n",
    "        \n",
    "        # Update cell state with layer norm\n",
    "        c_new = f_gate * c_prev + i_gate * g_gate\n",
    "        c_new = self.ln_c(c_new)\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_new = o_gate * torch.tanh(c_new)\n",
    "        \n",
    "        return h_new, c_new\n",
    "\n",
    "class StateExtr(nn.Module):\n",
    "    def __init__(self, input_node, hidden_node, nlayer, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.nlayer = nlayer\n",
    "        self.input_size = input_node\n",
    "        \n",
    "        # Create LayerNorm LSTM layers\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        \n",
    "        # First layer: input_size -> hidden_size\n",
    "        self.lstm_cells.append(LayerNormLSTMCell(input_node, hidden_node))\n",
    "        \n",
    "        # Additional layers: hidden_size -> hidden_size\n",
    "        for _ in range(nlayer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTMCell(hidden_node, hidden_node))\n",
    "        \n",
    "        # Dropout between layers (only applied if nlayer > 1)\n",
    "        self.dropout = nn.Dropout(dropout) if nlayer > 1 else nn.Identity()\n",
    "        \n",
    "        # Final layer norm and dropout\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"\n",
    "        시계열 상태 시퀀스를 처리하여 각 시점의 hidden state 추출\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, input_size] - BMED 시스템 상태 시퀀스\n",
    "            seq_len: [batch_size] - 각 시퀀스의 실제 길이\n",
    "            \n",
    "        Returns:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size] - 각 시점의 누적된 hidden state\n",
    "        \"\"\"\n",
    "        \n",
    "        # 입력 검증\n",
    "        if x.size(0) != seq_len.size(0):\n",
    "            raise ValueError(f\"Batch size mismatch: input {x.size(0)} vs seq_len {seq_len.size(0)}\")\n",
    "        \n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # 초기 hidden/cell states 초기화\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        for _ in range(self.nlayer):\n",
    "            h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "            c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        \n",
    "        # 각 시점별 출력 저장\n",
    "        outputs = []\n",
    "        \n",
    "        # 시점별로 순차 처리\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]  # [batch_size, input_node]\n",
    "            \n",
    "            # 각 LSTM layer 순차 처리\n",
    "            layer_input = x_t\n",
    "            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "                h_new, c_new = lstm_cell(layer_input, (h_states[layer_idx], c_states[layer_idx]))\n",
    "                \n",
    "                # 상태 업데이트\n",
    "                h_states[layer_idx] = h_new\n",
    "                c_states[layer_idx] = c_new\n",
    "                \n",
    "                # 다음 레이어 입력 준비 (dropout 적용)\n",
    "                if layer_idx < len(self.lstm_cells) - 1:  # 마지막 레이어가 아닌 경우\n",
    "                    layer_input = self.dropout(h_new)\n",
    "                else:\n",
    "                    layer_input = h_new\n",
    "            \n",
    "            outputs.append(layer_input)\n",
    "        \n",
    "        # [batch_size, seq_len, hidden_size] 형태로 변환\n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        \n",
    "        # 시퀀스 길이에 따른 마스킹 (패딩 부분 0으로 설정)\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        \n",
    "        # 시퀀스 길이 유효성 검사\n",
    "        if (seq_len_cpu <= 0).any():\n",
    "            invalid_lengths = seq_len_cpu[seq_len_cpu <= 0]\n",
    "            raise ValueError(f\"Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths must be positive.\")\n",
    "        \n",
    "        # 마스크 생성 및 적용\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)  # [batch, seq_len, 1]\n",
    "        \n",
    "        # 마스킹 적용\n",
    "        masked_output = output_tensor * mask\n",
    "        \n",
    "        # Final normalization and dropout\n",
    "        normalized = self.final_layer_norm(masked_output)\n",
    "        return self.final_dropout(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "be1c313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Hidden state로부터 BMED 시스템의 물리적 변화량과 새로운 전류값을 디코딩하는 MLP\n",
    "    출력: [dVA, dVB, dNALA, dNBK, nI] - 5개 물리적 변화량 (CBLA, CAK 제거로 dNBLA, dNAK 불필요)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, num_layers=2, num_nodes=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_nodes is None:\n",
    "            num_nodes = hidden_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # 첫 번째 레이어: hidden_size → num_nodes\n",
    "        self.layers.append(nn.Linear(hidden_size, num_nodes))\n",
    "        self.layers.append(nn.LayerNorm(num_nodes))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # 중간 은닉층들: num_nodes → num_nodes\n",
    "        for i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_nodes, num_nodes))\n",
    "            self.layers.append(nn.LayerNorm(num_nodes))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # 마지막 출력층: num_nodes → output_size (5개 물리적 변화량)\n",
    "        self.layers.append(nn.Linear(num_nodes, output_size))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Hidden state를 물리적 변화량으로 디코딩\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size] - 시점별 hidden state\n",
    "            \n",
    "        Returns:\n",
    "            physical_changes: [batch_size, seq_len, 5] - 물리적 변화량\n",
    "                [dVA, dVB, dNALA, dNBK, nI]\n",
    "        \"\"\"\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e0213a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    물리적 변화량을 실제 시스템 상태로 변환하면서 물리적 제약 조건을 적용\n",
    "    Bipolar membrane electrodialysis 시스템의 물리 법칙 기반 상태 업데이트\n",
    "    CBLA, CAK는 완전히 제거되어 더 이상 존재하지 않음\n",
    "    전류는 dependent variable이므로 input에 포함하지 않고 output으로만 예측\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-1):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # division by zero 방지\n",
    "        \n",
    "    def forward(self, physical_changes, current_state):\n",
    "        \"\"\"\n",
    "        물리적 변화량을 현재 상태에 적용하여 다음 상태 계산\n",
    "        \n",
    "        Args:\n",
    "            physical_changes: [batch, seq, 5] - [dVA, dVB, dNALA, dNBK, nI]\n",
    "            current_state: [batch, seq, 9] - 현재 BMED 시스템 상태 (전류 제외)\n",
    "                V, E, VF, VA, VB, CFLA, CALA, CFK, CBK\n",
    "                \n",
    "        Returns:\n",
    "            next_state: [batch, seq, 10] - 물리 제약이 적용된 다음 상태\n",
    "                V, E, VF, VA, VB, CFLA, CALA, CFK, CBK, I\n",
    "        \"\"\"\n",
    "        # 입력 차원 검증\n",
    "        if physical_changes.dim() != current_state.dim():\n",
    "            raise ValueError(f\"Dimension mismatch: physical_changes {physical_changes.shape} vs current_state {current_state.shape}\")\n",
    "        \n",
    "        if current_state.size(-1) != 9:\n",
    "            raise ValueError(f\"Expected 9 state features, got {current_state.size(-1)}\")\n",
    "            \n",
    "        if physical_changes.size(-1) != 5:\n",
    "            raise ValueError(f\"Expected 5 physical changes, got {physical_changes.size(-1)}\")\n",
    "        \n",
    "        # 현재 상태 변수 추출 (9개)\n",
    "        V = current_state[..., 0:1]     # 전압 (고정값)\n",
    "        E = current_state[..., 1:2]     # 외부 전해질 농도 (고정값)\n",
    "        VF = current_state[..., 2:3]    # Feed 부피\n",
    "        VA = current_state[..., 3:4]    # Acid 부피\n",
    "        VB = current_state[..., 4:5]    # Base 부피\n",
    "        CFLA = current_state[..., 5:6]  # Feed LA 농도\n",
    "        CALA = current_state[..., 6:7]  # Acid LA 농도\n",
    "        CFK = current_state[..., 7:8]   # Feed K 농도\n",
    "        CBK = current_state[..., 8:9]   # Base K 농도\n",
    "\n",
    "        # 물질량 계산 (농도 × 부피) - CBLA, CAK 관련은 완전 제거\n",
    "        NFLA = CFLA * VF\n",
    "        NALA = CALA * VA  \n",
    "        NFK = CFK * VF\n",
    "        NBK = CBK * VB\n",
    "\n",
    "        # 물리적 변화량 추출 (5개)\n",
    "        dVA = physical_changes[..., 0:1]    # Acid 부피 변화량\n",
    "        dVB = physical_changes[..., 1:2]    # Base 부피 변화량\n",
    "        dNALA = physical_changes[..., 2:3]  # Acid LA 물질량 변화량 (F→A)\n",
    "        dNBK = physical_changes[..., 3:4]   # Base K 물질량 변화량 (F→B)\n",
    "        nI = physical_changes[..., 4:5]     # 새로운 전류값 (모델이 예측)\n",
    "\n",
    "        # 새로운 부피 계산\n",
    "        nVF = VF - dVA - dVB\n",
    "        nVA = VA + dVA        \n",
    "        nVB = VB + dVB        \n",
    "        \n",
    "        # 물질 이동량을 일방향으로 제한\n",
    "        dNALA_clipped = torch.clamp(dNALA, min=0)  # F→A 이동만\n",
    "        dNBK_clipped = torch.clamp(dNBK, min=0)    # F→B 이동만\n",
    "        \n",
    "        # 새로운 물질량 계산 (CBLA, CAK 관련 제거)\n",
    "        nNFLA = NFLA - dNALA_clipped  # Feed에서 LA 유출\n",
    "        nNALA = NALA + dNALA_clipped  # Acid로 LA 유입\n",
    "        nNFK = NFK - dNBK_clipped     # Feed에서 K 유출  \n",
    "        nNBK = NBK + dNBK_clipped     # Base로 K 유입\n",
    "        \n",
    "        # 물리적 제약 조건 적용 (양수 유지)\n",
    "        nVF = torch.clamp(nVF, min=self.eps)\n",
    "        nVA = torch.clamp(nVA, min=self.eps)\n",
    "        nVB = torch.clamp(nVB, min=self.eps)\n",
    "        \n",
    "        # 물질량 음수 방지\n",
    "        nNFLA = torch.clamp(nNFLA, min=0)\n",
    "        nNALA = torch.clamp(nNALA, min=0)\n",
    "        nNFK = torch.clamp(nNFK, min=0)\n",
    "        nNBK = torch.clamp(nNBK, min=0)\n",
    "        \n",
    "        # 새로운 농도 계산\n",
    "        nCFLA = nNFLA / nVF\n",
    "        nCALA = nNALA / nVA\n",
    "        nCFK = nNFK / nVF\n",
    "        nCBK = nNBK / nVB\n",
    "        \n",
    "        # 전류는 양수 제약\n",
    "        nI = torch.clamp(nI, min=0)\n",
    "\n",
    "        # 새로운 상태 조립 (10개 변수, CBLA, CAK 완전 제거, 전류는 예측 결과로 추가)\n",
    "        next_state = torch.cat([\n",
    "            V, E,  # 고정값: 전압, 외부 전해질 농도\n",
    "            nVF, nVA, nVB,  # 새로운 부피\n",
    "            nCFLA, nCALA,   # 새로운 LA 농도 (CBLA 제거)\n",
    "            nCFK, nCBK,     # 새로운 K 농도 (CAK 제거)\n",
    "            nI  # 새로운 전류 (모델이 예측한 dependent variable)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1c20aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BMED 시스템의 시계열 상태 예측을 위한 자기회귀 모델\n",
    "    \n",
    "    구조:\n",
    "    1. StateExtr: LSTM으로 시계열 패턴의 hidden state 추출\n",
    "    2. PhysicalChangeDecoder: Hidden state를 물리적 변화량으로 디코딩  \n",
    "    3. PhysicsConstraintLayer: 물리 법칙 적용하여 다음 상태 계산\n",
    "    \"\"\"\n",
    "    def __init__(self, state_extractor_params, decoder_params):\n",
    "        super().__init__()\n",
    "        self.state_extractor = StateExtr(**state_extractor_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer()\n",
    "\n",
    "    def forward(self, current_states, seq_lengths):\n",
    "        \"\"\"\n",
    "        현재 시점까지의 상태들로부터 다음 상태들 예측\n",
    "        \n",
    "        Args:\n",
    "            current_states: [batch, seq_len, 9] - 현재까지의 전류를 제외한한 BMED 시스템 상태들\n",
    "            seq_lengths: [batch] - 각 시퀀스의 실제 길이\n",
    "            \n",
    "        Returns:\n",
    "            next_states: [batch, seq_len, 10] - 예측된 다음 시점 상태들 (전류 포함)\n",
    "        \"\"\"\n",
    "        # 1. LSTM으로 각 시점의 hidden state 추출 (과거 정보 누적)\n",
    "        hidden_states = self.state_extractor(current_states, seq_lengths)\n",
    "        \n",
    "        # 2. Hidden state를 물리적 변화량으로 디코딩\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        \n",
    "        # 3. 물리적 제약 조건을 적용하여 다음 상태 계산\n",
    "        next_states = self.physics_constraint(physical_changes, current_states)\n",
    "        \n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7b58e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(predictions, targets, seq_lengths):\n",
    "    \"\"\"\n",
    "    개선된 마스킹된 MSE 손실 함수 - device 호환성, 안정성 강화\n",
    "    물리적 의미가 개선되어 feature별 가중치 불필요\n",
    "    \n",
    "    Args:\n",
    "        predictions: 모델 예측값 [batch_size, seq_len, features]\n",
    "        targets: 실제 타겟값 [batch_size, seq_len, features]  \n",
    "        seq_lengths: 각 시퀀스의 실제 길이 [batch_size]\n",
    "    \n",
    "    Returns:\n",
    "        masked_loss: 패딩 부분을 제외한 평균 MSE 손실\n",
    "    \"\"\"\n",
    "    # 입력 검증\n",
    "    if predictions.shape != targets.shape:\n",
    "        raise ValueError(f\"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}\")\n",
    "    \n",
    "    if predictions.size(0) != seq_lengths.size(0):\n",
    "        raise ValueError(f\"Batch size mismatch: predictions {predictions.size(0)} vs seq_lengths {seq_lengths.size(0)}\")\n",
    "    \n",
    "    batch_size, max_len, features = predictions.shape\n",
    "    \n",
    "    # seq_lengths를 CPU로 이동하여 arange와 호환되도록 처리\n",
    "    seq_lengths_cpu = seq_lengths.detach().cpu().long()\n",
    "    \n",
    "    # 시퀀스 길이 유효성 검사\n",
    "    if (seq_lengths_cpu <= 0).any():\n",
    "        invalid_lengths = seq_lengths_cpu[seq_lengths_cpu <= 0]\n",
    "        raise ValueError(f\"Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths must be positive.\")\n",
    "    \n",
    "    # 최대 길이 초과 검사\n",
    "    if (seq_lengths_cpu > max_len).any():\n",
    "        invalid_lengths = seq_lengths_cpu[seq_lengths_cpu > max_len]\n",
    "        raise ValueError(f\"Sequence lengths exceed max_len: {invalid_lengths.tolist()} > {max_len}\")\n",
    "    \n",
    "    # 마스크 생성: 실제 시퀀스 길이만큼만 True\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_lengths_cpu[:, None]\n",
    "    mask = mask.float().to(predictions.device)\n",
    "    \n",
    "    # 각 요소별 MSE 계산 (reduction='none')\n",
    "    loss = F.mse_loss(predictions, targets, reduction='none')  # [batch, seq_len, features]\n",
    "    \n",
    "    # 마스크 적용하여 패딩 부분 제거\n",
    "    masked_loss = loss * mask.unsqueeze(-1)  # [batch, seq_len, features]\n",
    "    \n",
    "    # 전체 손실 합계와 전체 valid elements 계산\n",
    "    total_loss = masked_loss.sum()\n",
    "    total_elements = mask.sum() * features\n",
    "    \n",
    "    # 0으로 나누기 방지\n",
    "    if total_elements == 0:\n",
    "        raise ValueError(\"No valid elements found after masking. Check sequence lengths and data.\")\n",
    "    \n",
    "    masked_loss = total_loss / total_elements\n",
    "    \n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5bf439fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_teacher_forcing_data(input_sequences, seq_lengths):\n",
    "    \"\"\"\n",
    "    Teacher Forcing을 위한 입력-타겟 데이터 준비\n",
    "    전류는 dependent variable이므로 input에서 제외하고 output에만 포함\n",
    "    \n",
    "    Args:\n",
    "        input_sequences: 전체 시퀀스 [batch_size, seq_len, 10] (CBLA, CAK 제거된 상태)\n",
    "        seq_lengths: 각 시퀀스의 실제 길이 [batch_size]\n",
    "    \n",
    "    Returns:\n",
    "        inputs: [t0, t1, ..., t_{n-1}] 현재 상태들 [batch_size, seq_len-1, 9] (전류 제외)\n",
    "        targets: [t1, t2, ..., t_n] 다음 상태들 [batch_size, seq_len-1, 10] (전류 포함)\n",
    "        target_seq_lengths: 타겟 시퀀스 길이 (1씩 감소)\n",
    "    \"\"\"\n",
    "    # 입력: 마지막 시점 제외 [:-1] 및 전류 제외 [:-1]\n",
    "    inputs = input_sequences[:, :-1, :-1]  # 전류 제외하여 9개 features\n",
    "    \n",
    "    # 타겟: 첫 번째 시점 제외 [1:], 전류 포함하여 10개 features\n",
    "    targets = input_sequences[:, 1:, :]\n",
    "    \n",
    "    # 타겟 시퀀스 길이는 1씩 감소 (마지막 시점 예측 불가)\n",
    "    if (seq_lengths - 1 < 1).any():\n",
    "        invalid_lengths = seq_lengths[seq_lengths - 1 < 1]\n",
    "        raise ValueError(f\"타겟 시퀀스 길이가 0보다 작아질 수 없습니다. 잘못된 seq_lengths: {invalid_lengths.tolist()}\")\n",
    "    target_seq_lengths = seq_lengths - 1\n",
    "    \n",
    "    return inputs, targets, target_seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1c8fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Dataset created with 36 experiments\n",
      "Max sequence length: 33\n",
      "Experiment numbers: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35)]\n",
      "Actual split:\n",
      "  Train: [0, np.int64(1), 2, np.int64(3), 4, 5, np.int64(6), 8, 9, 10, np.int64(11), np.int64(12), 13, 15, 17, np.int64(18), np.int64(19), 20, 22, np.int64(23), 24, np.int64(25), 26, np.int64(28), np.int64(30), 33, 34, np.int64(35)] (28 experiments)\n",
      "  Val: [np.int64(7), np.int64(14), np.int64(16), np.int64(31)] (4 experiments)\n",
      "  Test: [np.int64(21), np.int64(27), np.int64(29), np.int64(32)] (4 experiments)\n",
      "\n",
      "Completed DataLoader creation:\n",
      "  Train: 28 sequences\n",
      "  Val: 4 sequences\n",
      "  Test: 4 sequences\n"
     ]
    }
   ],
   "source": [
    "# Load data and create dataloaders\n",
    "print(\"Loading and preprocessing data...\")\n",
    "ndf, exp_num_list = norm_data('BMED_DATA_AG.csv')\n",
    "sequences = seq_data(ndf)\n",
    "padded_seq, seq_len, max_seq_len = pad_seq(sequences)\n",
    "dataset = gen_dataset(padded_seq, seq_len)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} experiments\")\n",
    "print(f\"Max sequence length: {max_seq_len}\")\n",
    "print(f\"Experiment numbers: {sorted(exp_num_list)}\")\n",
    "\n",
    "# Create train/val/test dataloaders with stratified split\n",
    "train_loader, val_loader, test_loader = dataloaders(dataset, exp_num_list, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "95396be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Model architecture:\n",
      "  Input features: 9 (without current)\n",
      "  Hidden size: 256\n",
      "  LSTM layers: 5\n",
      "  Output features: 5 (physical changes)\n",
      "  Model parameters: 2,714,629\n",
      "\n",
      "Training configuration:\n",
      "  Total epochs: 100,000\n",
      "  Warmup epochs: 5,000 (5%)\n",
      "  Min epochs: 1,000\n",
      "  Patience: 1,000\n",
      "  Peak learning rate: 4.42e-04\n"
     ]
    }
   ],
   "source": [
    "class NoamScheduler:\n",
    "    \"\"\"\n",
    "    Transformer에서 사용하는 Noam 학습률 스케줄러\n",
    "    LSTM에 맞게 epoch 기반으로 수정\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, model_size, warmup_epochs, factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_size = model_size\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.factor = factor\n",
    "        self.epoch_num = 0\n",
    "        \n",
    "    def step_epoch(self):\n",
    "        \"\"\"에포크마다 학습률 업데이트\"\"\"\n",
    "        self.epoch_num += 1\n",
    "        lr = self.factor * (\n",
    "            self.model_size ** (-0.5) *\n",
    "            min(self.epoch_num ** (-0.5), self.epoch_num * self.warmup_epochs ** (-1.5))\n",
    "        )\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        return lr\n",
    "\n",
    "device = set_device()\n",
    "\n",
    "# Model parameters - 입력 차원 수정\n",
    "state_extr_params = {\n",
    "    'input_node': 9,   # 수정: 10 -> 9 (전류 제외한 입력)\n",
    "    'hidden_node': 256,\n",
    "    'nlayer': 5,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "\n",
    "decoder_params = {\n",
    "    'hidden_size': 256,\n",
    "    'output_size': 5,  # [dVA, dVB, dNALA, dNBK, nI]\n",
    "    'num_layers': 5,\n",
    "    'num_nodes': 256,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = BMEDAutoregressiveModel(state_extr_params, decoder_params)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Input features: {state_extr_params['input_node']} (without current)\")\n",
    "print(f\"  Hidden size: {state_extr_params['hidden_node']}\")\n",
    "print(f\"  LSTM layers: {state_extr_params['nlayer']}\")\n",
    "print(f\"  Output features: {decoder_params['output_size']} (physical changes)\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Training setup with Noam scheduler (epoch-based)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1.0)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100000\n",
    "min_epochs = 1000\n",
    "patience = 1000\n",
    "\n",
    "# Noam 스케줄러 설정 (epoch 기반)\n",
    "warmup_epochs = int(num_epochs * 0.05)  # 전체 epoch의 5%\n",
    "scheduler = NoamScheduler(\n",
    "    optimizer, \n",
    "    model_size=256,  # hidden_size와 동일\n",
    "    warmup_epochs=warmup_epochs,  # 500 epochs\n",
    "    factor=1  # 학습률 스케일링 팩터\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Total epochs: {num_epochs:,}\")\n",
    "print(f\"  Warmup epochs: {warmup_epochs:,} (5%)\")\n",
    "print(f\"  Min epochs: {min_epochs:,}\")\n",
    "print(f\"  Patience: {patience:,}\")\n",
    "peak_lr = 0.5 * (256 ** (-0.5)) * (warmup_epochs ** (-0.5))\n",
    "print(f\"  Peak learning rate: {peak_lr:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thtkmdiuz0l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Noam scheduler (epoch-based)...\n",
      "데이터 분포 - Train: 28(0.875), Val: 4(0.125)\n",
      "Epoch    1: Train: 1.706277, Val: 1.571129, Total: 1.689383, LR: 1.77e-07 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 1.706277, Val: 1.571129, Total: 1.689383 (Epoch 1)\n",
      "Epoch    2: Train: 1.487161, Val: 1.531463, Total: 1.492699, LR: 3.54e-07 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 1.487161, Val: 1.531463, Total: 1.492699 (Epoch 2)\n",
      "Epoch    3: Train: 1.507271, Val: 1.471402, Total: 1.502788, LR: 5.30e-07 [WARMUP]\n",
      "          Best: Train: 1.487161, Val: 1.531463, Total: 1.492699 (Epoch 2)\n",
      "Epoch    4: Train: 1.411294, Val: 1.410847, Total: 1.411238, LR: 7.07e-07 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 1.411294, Val: 1.410847, Total: 1.411238 (Epoch 4)\n",
      "Epoch    5: Train: 1.305281, Val: 1.326241, Total: 1.307901, LR: 8.84e-07 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 1.305281, Val: 1.326241, Total: 1.307901 (Epoch 5)\n",
      "Epoch    6: Train: 1.411536, Val: 1.223040, Total: 1.387974, LR: 1.06e-06 [WARMUP]\n",
      "          Best: Train: 1.305281, Val: 1.326241, Total: 1.307901 (Epoch 5)\n",
      "Epoch    7: Train: 1.319657, Val: 1.089068, Total: 1.290833, LR: 1.24e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 1.319657, Val: 1.089068, Total: 1.290833 (Epoch 7)\n",
      "Epoch    8: Train: 1.401711, Val: 0.933170, Total: 1.343144, LR: 1.41e-06 [WARMUP]\n",
      "          Best: Train: 1.319657, Val: 1.089068, Total: 1.290833 (Epoch 7)\n",
      "Epoch    9: Train: 1.256265, Val: 0.779863, Total: 1.196715, LR: 1.59e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 1.256265, Val: 0.779863, Total: 1.196715 (Epoch 9)\n",
      "Epoch   10: Train: 1.186861, Val: 0.646506, Total: 1.119317, LR: 1.77e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 1.186861, Val: 0.646506, Total: 1.119317 (Epoch 10)\n",
      "Epoch   11: Train: 1.143512, Val: 0.538491, Total: 1.067884, LR: 1.94e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 1.143512, Val: 0.538491, Total: 1.067884 (Epoch 11)\n",
      "Epoch   12: Train: 0.982668, Val: 0.451771, Total: 0.916306, LR: 2.12e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.982668, Val: 0.451771, Total: 0.916306 (Epoch 12)\n",
      "Epoch   13: Train: 0.904364, Val: 0.356539, Total: 0.835886, LR: 2.30e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.904364, Val: 0.356539, Total: 0.835886 (Epoch 13)\n",
      "Epoch   14: Train: 0.791030, Val: 0.273917, Total: 0.726391, LR: 2.47e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.791030, Val: 0.273917, Total: 0.726391 (Epoch 14)\n",
      "Epoch   15: Train: 0.930869, Val: 0.222571, Total: 0.842332, LR: 2.65e-06 [WARMUP]\n",
      "          Best: Train: 0.791030, Val: 0.273917, Total: 0.726391 (Epoch 14)\n",
      "Epoch   16: Train: 0.733661, Val: 0.175433, Total: 0.663882, LR: 2.83e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.733661, Val: 0.175433, Total: 0.663882 (Epoch 16)\n",
      "Epoch   17: Train: 0.727184, Val: 0.133885, Total: 0.653022, LR: 3.01e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.727184, Val: 0.133885, Total: 0.653022 (Epoch 17)\n",
      "Epoch   18: Train: 0.722291, Val: 0.118000, Total: 0.646754, LR: 3.18e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.722291, Val: 0.118000, Total: 0.646754 (Epoch 18)\n",
      "Epoch   19: Train: 0.544176, Val: 0.117614, Total: 0.490855, LR: 3.36e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.544176, Val: 0.117614, Total: 0.490855 (Epoch 19)\n",
      "Epoch   20: Train: 0.528509, Val: 0.114421, Total: 0.476748, LR: 3.54e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.528509, Val: 0.114421, Total: 0.476748 (Epoch 20)\n",
      "Epoch   21: Train: 0.482043, Val: 0.112972, Total: 0.435909, LR: 3.71e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.482043, Val: 0.112972, Total: 0.435909 (Epoch 21)\n",
      "Epoch   22: Train: 0.434755, Val: 0.113554, Total: 0.394605, LR: 3.89e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.434755, Val: 0.113554, Total: 0.394605 (Epoch 22)\n",
      "Epoch   23: Train: 0.474828, Val: 0.117081, Total: 0.430109, LR: 4.07e-06 [WARMUP]\n",
      "          Best: Train: 0.434755, Val: 0.113554, Total: 0.394605 (Epoch 22)\n",
      "Epoch   24: Train: 0.364694, Val: 0.121201, Total: 0.334258, LR: 4.24e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.364694, Val: 0.121201, Total: 0.334258 (Epoch 24)\n",
      "Epoch   25: Train: 0.444355, Val: 0.124223, Total: 0.404338, LR: 4.42e-06 [WARMUP]\n",
      "          Best: Train: 0.364694, Val: 0.121201, Total: 0.334258 (Epoch 24)\n",
      "Epoch   26: Train: 0.365198, Val: 0.129988, Total: 0.335797, LR: 4.60e-06 [WARMUP]\n",
      "          Best: Train: 0.364694, Val: 0.121201, Total: 0.334258 (Epoch 24)\n",
      "Epoch   27: Train: 0.286521, Val: 0.132681, Total: 0.267291, LR: 4.77e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.286521, Val: 0.132681, Total: 0.267291 (Epoch 27)\n",
      "Epoch   28: Train: 0.363277, Val: 0.123080, Total: 0.333252, LR: 4.95e-06 [WARMUP]\n",
      "          Best: Train: 0.286521, Val: 0.132681, Total: 0.267291 (Epoch 27)\n",
      "Epoch   29: Train: 0.278852, Val: 0.126331, Total: 0.259787, LR: 5.13e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.278852, Val: 0.126331, Total: 0.259787 (Epoch 29)\n",
      "Epoch   30: Train: 0.307617, Val: 0.129767, Total: 0.285386, LR: 5.30e-06 [WARMUP]\n",
      "          Best: Train: 0.278852, Val: 0.126331, Total: 0.259787 (Epoch 29)\n",
      "Epoch   31: Train: 0.280545, Val: 0.122979, Total: 0.260849, LR: 5.48e-06 [WARMUP]\n",
      "          Best: Train: 0.278852, Val: 0.126331, Total: 0.259787 (Epoch 29)\n",
      "Epoch   32: Train: 0.286496, Val: 0.119989, Total: 0.265683, LR: 5.66e-06 [WARMUP]\n",
      "          Best: Train: 0.278852, Val: 0.126331, Total: 0.259787 (Epoch 29)\n",
      "Epoch   33: Train: 0.241857, Val: 0.110493, Total: 0.225436, LR: 5.83e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.241857, Val: 0.110493, Total: 0.225436 (Epoch 33)\n",
      "Epoch   34: Train: 0.221249, Val: 0.099536, Total: 0.206035, LR: 6.01e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.221249, Val: 0.099536, Total: 0.206035 (Epoch 34)\n",
      "Epoch   35: Train: 0.221584, Val: 0.086447, Total: 0.204692, LR: 6.19e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.221584, Val: 0.086447, Total: 0.204692 (Epoch 35)\n",
      "Epoch   36: Train: 0.192078, Val: 0.078978, Total: 0.177940, LR: 6.36e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.192078, Val: 0.078978, Total: 0.177940 (Epoch 36)\n",
      "Epoch   37: Train: 0.211634, Val: 0.067680, Total: 0.193640, LR: 6.54e-06 [WARMUP]\n",
      "          Best: Train: 0.192078, Val: 0.078978, Total: 0.177940 (Epoch 36)\n",
      "Epoch   38: Train: 0.197178, Val: 0.057716, Total: 0.179746, LR: 6.72e-06 [WARMUP]\n",
      "          Best: Train: 0.192078, Val: 0.078978, Total: 0.177940 (Epoch 36)\n",
      "Epoch   39: Train: 0.190768, Val: 0.050543, Total: 0.173240, LR: 6.89e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.190768, Val: 0.050543, Total: 0.173240 (Epoch 39)\n",
      "Epoch   40: Train: 0.187021, Val: 0.047019, Total: 0.169521, LR: 7.07e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.187021, Val: 0.047019, Total: 0.169521 (Epoch 40)\n",
      "Epoch   41: Train: 0.191240, Val: 0.047433, Total: 0.173264, LR: 7.25e-06 [WARMUP]\n",
      "          Best: Train: 0.187021, Val: 0.047019, Total: 0.169521 (Epoch 40)\n",
      "Epoch   42: Train: 0.188957, Val: 0.044416, Total: 0.170890, LR: 7.42e-06 [WARMUP]\n",
      "          Best: Train: 0.187021, Val: 0.047019, Total: 0.169521 (Epoch 40)\n",
      "Epoch   43: Train: 0.177140, Val: 0.039902, Total: 0.159985, LR: 7.60e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.177140, Val: 0.039902, Total: 0.159985 (Epoch 43)\n",
      "Epoch   44: Train: 0.161119, Val: 0.036536, Total: 0.145546, LR: 7.78e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.161119, Val: 0.036536, Total: 0.145546 (Epoch 44)\n",
      "Epoch   45: Train: 0.166209, Val: 0.034501, Total: 0.149745, LR: 7.95e-06 [WARMUP]\n",
      "          Best: Train: 0.161119, Val: 0.036536, Total: 0.145546 (Epoch 44)\n",
      "Epoch   46: Train: 0.190574, Val: 0.030447, Total: 0.170558, LR: 8.13e-06 [WARMUP]\n",
      "          Best: Train: 0.161119, Val: 0.036536, Total: 0.145546 (Epoch 44)\n",
      "Epoch   47: Train: 0.152217, Val: 0.028523, Total: 0.136755, LR: 8.31e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.152217, Val: 0.028523, Total: 0.136755 (Epoch 47)\n",
      "Epoch   48: Train: 0.154790, Val: 0.029935, Total: 0.139183, LR: 8.49e-06 [WARMUP]\n",
      "          Best: Train: 0.152217, Val: 0.028523, Total: 0.136755 (Epoch 47)\n",
      "Epoch   49: Train: 0.141802, Val: 0.029792, Total: 0.127800, LR: 8.66e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.141802, Val: 0.029792, Total: 0.127800 (Epoch 49)\n",
      "Epoch   50: Train: 0.170909, Val: 0.029793, Total: 0.153269, LR: 8.84e-06 [WARMUP]\n",
      "          Best: Train: 0.141802, Val: 0.029792, Total: 0.127800 (Epoch 49)\n",
      "Epoch   51: Train: 0.139084, Val: 0.027688, Total: 0.125160, LR: 9.02e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.139084, Val: 0.027688, Total: 0.125160 (Epoch 51)\n",
      "Epoch   52: Train: 0.161262, Val: 0.024686, Total: 0.144190, LR: 9.19e-06 [WARMUP]\n",
      "          Best: Train: 0.139084, Val: 0.027688, Total: 0.125160 (Epoch 51)\n",
      "Epoch   53: Train: 0.141371, Val: 0.023110, Total: 0.126589, LR: 9.37e-06 [WARMUP]\n",
      "          Best: Train: 0.139084, Val: 0.027688, Total: 0.125160 (Epoch 51)\n",
      "Epoch   54: Train: 0.131567, Val: 0.022160, Total: 0.117891, LR: 9.55e-06 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.131567, Val: 0.022160, Total: 0.117891 (Epoch 54)\n",
      "Epoch   55: Train: 0.141422, Val: 0.021287, Total: 0.126405, LR: 9.72e-06 [WARMUP]\n",
      "          Best: Train: 0.131567, Val: 0.022160, Total: 0.117891 (Epoch 54)\n",
      "Epoch   56: Train: 0.136218, Val: 0.020274, Total: 0.121725, LR: 9.90e-06 [WARMUP]\n",
      "          Best: Train: 0.131567, Val: 0.022160, Total: 0.117891 (Epoch 54)\n",
      "Epoch   57: Train: 0.124236, Val: 0.021824, Total: 0.111435, LR: 1.01e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.124236, Val: 0.021824, Total: 0.111435 (Epoch 57)\n",
      "Epoch   58: Train: 0.120559, Val: 0.019161, Total: 0.107884, LR: 1.03e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.120559, Val: 0.019161, Total: 0.107884 (Epoch 58)\n",
      "Epoch   59: Train: 0.123356, Val: 0.019301, Total: 0.110350, LR: 1.04e-05 [WARMUP]\n",
      "          Best: Train: 0.120559, Val: 0.019161, Total: 0.107884 (Epoch 58)\n",
      "Epoch   60: Train: 0.110028, Val: 0.020481, Total: 0.098834, LR: 1.06e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.110028, Val: 0.020481, Total: 0.098834 (Epoch 60)\n",
      "Epoch   61: Train: 0.113762, Val: 0.019446, Total: 0.101972, LR: 1.08e-05 [WARMUP]\n",
      "          Best: Train: 0.110028, Val: 0.020481, Total: 0.098834 (Epoch 60)\n",
      "Epoch   62: Train: 0.123234, Val: 0.020016, Total: 0.110332, LR: 1.10e-05 [WARMUP]\n",
      "          Best: Train: 0.110028, Val: 0.020481, Total: 0.098834 (Epoch 60)\n",
      "Epoch   63: Train: 0.119129, Val: 0.019774, Total: 0.106709, LR: 1.11e-05 [WARMUP]\n",
      "          Best: Train: 0.110028, Val: 0.020481, Total: 0.098834 (Epoch 60)\n",
      "Epoch   64: Train: 0.113725, Val: 0.020132, Total: 0.102026, LR: 1.13e-05 [WARMUP]\n",
      "          Best: Train: 0.110028, Val: 0.020481, Total: 0.098834 (Epoch 60)\n",
      "Epoch   65: Train: 0.110652, Val: 0.020781, Total: 0.099418, LR: 1.15e-05 [WARMUP]\n",
      "          Best: Train: 0.110028, Val: 0.020481, Total: 0.098834 (Epoch 60)\n",
      "Epoch   66: Train: 0.119422, Val: 0.020583, Total: 0.107067, LR: 1.17e-05 [WARMUP]\n",
      "          Best: Train: 0.110028, Val: 0.020481, Total: 0.098834 (Epoch 60)\n",
      "Epoch   67: Train: 0.099141, Val: 0.020764, Total: 0.089344, LR: 1.18e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.099141, Val: 0.020764, Total: 0.089344 (Epoch 67)\n",
      "Epoch   68: Train: 0.096711, Val: 0.022602, Total: 0.087447, LR: 1.20e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.096711, Val: 0.022602, Total: 0.087447 (Epoch 68)\n",
      "Epoch   69: Train: 0.118650, Val: 0.026254, Total: 0.107101, LR: 1.22e-05 [WARMUP]\n",
      "          Best: Train: 0.096711, Val: 0.022602, Total: 0.087447 (Epoch 68)\n",
      "Epoch   70: Train: 0.101766, Val: 0.028715, Total: 0.092635, LR: 1.24e-05 [WARMUP]\n",
      "          Best: Train: 0.096711, Val: 0.022602, Total: 0.087447 (Epoch 68)\n",
      "Epoch   71: Train: 0.100759, Val: 0.028850, Total: 0.091770, LR: 1.26e-05 [WARMUP]\n",
      "          Best: Train: 0.096711, Val: 0.022602, Total: 0.087447 (Epoch 68)\n",
      "Epoch   72: Train: 0.091326, Val: 0.031188, Total: 0.083809, LR: 1.27e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.091326, Val: 0.031188, Total: 0.083809 (Epoch 72)\n",
      "Epoch   73: Train: 0.099119, Val: 0.029464, Total: 0.090412, LR: 1.29e-05 [WARMUP]\n",
      "          Best: Train: 0.091326, Val: 0.031188, Total: 0.083809 (Epoch 72)\n",
      "Epoch   74: Train: 0.095418, Val: 0.030686, Total: 0.087326, LR: 1.31e-05 [WARMUP]\n",
      "          Best: Train: 0.091326, Val: 0.031188, Total: 0.083809 (Epoch 72)\n",
      "Epoch   75: Train: 0.094134, Val: 0.027837, Total: 0.085847, LR: 1.33e-05 [WARMUP]\n",
      "          Best: Train: 0.091326, Val: 0.031188, Total: 0.083809 (Epoch 72)\n",
      "Epoch   76: Train: 0.092941, Val: 0.025146, Total: 0.084467, LR: 1.34e-05 [WARMUP]\n",
      "          Best: Train: 0.091326, Val: 0.031188, Total: 0.083809 (Epoch 72)\n",
      "Epoch   77: Train: 0.080981, Val: 0.024188, Total: 0.073881, LR: 1.36e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.080981, Val: 0.024188, Total: 0.073881 (Epoch 77)\n",
      "Epoch   78: Train: 0.082531, Val: 0.024089, Total: 0.075226, LR: 1.38e-05 [WARMUP]\n",
      "          Best: Train: 0.080981, Val: 0.024188, Total: 0.073881 (Epoch 77)\n",
      "Epoch   79: Train: 0.091596, Val: 0.023200, Total: 0.083047, LR: 1.40e-05 [WARMUP]\n",
      "          Best: Train: 0.080981, Val: 0.024188, Total: 0.073881 (Epoch 77)\n",
      "Epoch   80: Train: 0.082854, Val: 0.022295, Total: 0.075284, LR: 1.41e-05 [WARMUP]\n",
      "          Best: Train: 0.080981, Val: 0.024188, Total: 0.073881 (Epoch 77)\n",
      "Epoch   81: Train: 0.084751, Val: 0.022772, Total: 0.077003, LR: 1.43e-05 [WARMUP]\n",
      "          Best: Train: 0.080981, Val: 0.024188, Total: 0.073881 (Epoch 77)\n",
      "Epoch   82: Train: 0.103621, Val: 0.025108, Total: 0.093807, LR: 1.45e-05 [WARMUP]\n",
      "          Best: Train: 0.080981, Val: 0.024188, Total: 0.073881 (Epoch 77)\n",
      "Epoch   83: Train: 0.103526, Val: 0.025376, Total: 0.093757, LR: 1.47e-05 [WARMUP]\n",
      "          Best: Train: 0.080981, Val: 0.024188, Total: 0.073881 (Epoch 77)\n",
      "Epoch   84: Train: 0.077812, Val: 0.022623, Total: 0.070913, LR: 1.48e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.077812, Val: 0.022623, Total: 0.070913 (Epoch 84)\n",
      "Epoch   85: Train: 0.080432, Val: 0.021143, Total: 0.073021, LR: 1.50e-05 [WARMUP]\n",
      "          Best: Train: 0.077812, Val: 0.022623, Total: 0.070913 (Epoch 84)\n",
      "Epoch   86: Train: 0.090018, Val: 0.021053, Total: 0.081397, LR: 1.52e-05 [WARMUP]\n",
      "          Best: Train: 0.077812, Val: 0.022623, Total: 0.070913 (Epoch 84)\n",
      "Epoch   87: Train: 0.068725, Val: 0.020659, Total: 0.062717, LR: 1.54e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.068725, Val: 0.020659, Total: 0.062717 (Epoch 87)\n",
      "Epoch   88: Train: 0.079008, Val: 0.020457, Total: 0.071689, LR: 1.56e-05 [WARMUP]\n",
      "          Best: Train: 0.068725, Val: 0.020659, Total: 0.062717 (Epoch 87)\n",
      "Epoch   89: Train: 0.071350, Val: 0.019307, Total: 0.064844, LR: 1.57e-05 [WARMUP]\n",
      "          Best: Train: 0.068725, Val: 0.020659, Total: 0.062717 (Epoch 87)\n",
      "Epoch   90: Train: 0.088492, Val: 0.019458, Total: 0.079863, LR: 1.59e-05 [WARMUP]\n",
      "          Best: Train: 0.068725, Val: 0.020659, Total: 0.062717 (Epoch 87)\n",
      "Epoch   91: Train: 0.076706, Val: 0.018974, Total: 0.069489, LR: 1.61e-05 [WARMUP]\n",
      "          Best: Train: 0.068725, Val: 0.020659, Total: 0.062717 (Epoch 87)\n",
      "Epoch   92: Train: 0.075653, Val: 0.018681, Total: 0.068532, LR: 1.63e-05 [WARMUP]\n",
      "          Best: Train: 0.068725, Val: 0.020659, Total: 0.062717 (Epoch 87)\n",
      "Epoch   93: Train: 0.071048, Val: 0.017832, Total: 0.064396, LR: 1.64e-05 [WARMUP]\n",
      "          Best: Train: 0.068725, Val: 0.020659, Total: 0.062717 (Epoch 87)\n",
      "Epoch   94: Train: 0.065941, Val: 0.017536, Total: 0.059890, LR: 1.66e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.065941, Val: 0.017536, Total: 0.059890 (Epoch 94)\n",
      "Epoch   95: Train: 0.081258, Val: 0.018534, Total: 0.073417, LR: 1.68e-05 [WARMUP]\n",
      "          Best: Train: 0.065941, Val: 0.017536, Total: 0.059890 (Epoch 94)\n",
      "Epoch   96: Train: 0.075396, Val: 0.018365, Total: 0.068267, LR: 1.70e-05 [WARMUP]\n",
      "          Best: Train: 0.065941, Val: 0.017536, Total: 0.059890 (Epoch 94)\n",
      "Epoch   97: Train: 0.088366, Val: 0.018000, Total: 0.079570, LR: 1.71e-05 [WARMUP]\n",
      "          Best: Train: 0.065941, Val: 0.017536, Total: 0.059890 (Epoch 94)\n",
      "Epoch   98: Train: 0.074607, Val: 0.018063, Total: 0.067539, LR: 1.73e-05 [WARMUP]\n",
      "          Best: Train: 0.065941, Val: 0.017536, Total: 0.059890 (Epoch 94)\n",
      "Epoch   99: Train: 0.069814, Val: 0.018294, Total: 0.063374, LR: 1.75e-05 [WARMUP]\n",
      "          Best: Train: 0.065941, Val: 0.017536, Total: 0.059890 (Epoch 94)\n",
      "Epoch  100: Train: 0.070411, Val: 0.018064, Total: 0.063868, LR: 1.77e-05 [WARMUP]\n",
      "          Best: Train: 0.065941, Val: 0.017536, Total: 0.059890 (Epoch 94)\n",
      "Epoch  101: Train: 0.059338, Val: 0.017466, Total: 0.054104, LR: 1.79e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.059338, Val: 0.017466, Total: 0.054104 (Epoch 101)\n",
      "Epoch  102: Train: 0.076335, Val: 0.016671, Total: 0.068877, LR: 1.80e-05 [WARMUP]\n",
      "          Best: Train: 0.059338, Val: 0.017466, Total: 0.054104 (Epoch 101)\n",
      "Epoch  103: Train: 0.068538, Val: 0.016749, Total: 0.062064, LR: 1.82e-05 [WARMUP]\n",
      "          Best: Train: 0.059338, Val: 0.017466, Total: 0.054104 (Epoch 101)\n",
      "Epoch  104: Train: 0.057194, Val: 0.017188, Total: 0.052193, LR: 1.84e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.057194, Val: 0.017188, Total: 0.052193 (Epoch 104)\n",
      "Epoch  105: Train: 0.068297, Val: 0.018335, Total: 0.062052, LR: 1.86e-05 [WARMUP]\n",
      "          Best: Train: 0.057194, Val: 0.017188, Total: 0.052193 (Epoch 104)\n",
      "Epoch  106: Train: 0.064971, Val: 0.019320, Total: 0.059265, LR: 1.87e-05 [WARMUP]\n",
      "          Best: Train: 0.057194, Val: 0.017188, Total: 0.052193 (Epoch 104)\n",
      "Epoch  107: Train: 0.051940, Val: 0.019796, Total: 0.047922, LR: 1.89e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  108: Train: 0.053949, Val: 0.019685, Total: 0.049666, LR: 1.91e-05 [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  109: Train: 0.061207, Val: 0.018855, Total: 0.055913, LR: 1.93e-05 [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  110: Train: 0.062289, Val: 0.018952, Total: 0.056872, LR: 1.94e-05 [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  111: Train: 0.057244, Val: 0.019332, Total: 0.052505, LR: 1.96e-05 [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  112: Train: 0.060648, Val: 0.019339, Total: 0.055485, LR: 1.98e-05 [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  113: Train: 0.057182, Val: 0.019175, Total: 0.052431, LR: 2.00e-05 [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  114: Train: 0.054639, Val: 0.018784, Total: 0.050157, LR: 2.02e-05 [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  115: Train: 0.054568, Val: 0.018437, Total: 0.050052, LR: 2.03e-05 [WARMUP]\n",
      "          Best: Train: 0.051940, Val: 0.019796, Total: 0.047922 (Epoch 107)\n",
      "Epoch  116: Train: 0.051257, Val: 0.017524, Total: 0.047041, LR: 2.05e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.051257, Val: 0.017524, Total: 0.047041 (Epoch 116)\n",
      "Epoch  117: Train: 0.048309, Val: 0.016676, Total: 0.044355, LR: 2.07e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.048309, Val: 0.016676, Total: 0.044355 (Epoch 117)\n",
      "Epoch  118: Train: 0.051404, Val: 0.015324, Total: 0.046894, LR: 2.09e-05 [WARMUP]\n",
      "          Best: Train: 0.048309, Val: 0.016676, Total: 0.044355 (Epoch 117)\n",
      "Epoch  119: Train: 0.050147, Val: 0.014768, Total: 0.045724, LR: 2.10e-05 [WARMUP]\n",
      "          Best: Train: 0.048309, Val: 0.016676, Total: 0.044355 (Epoch 117)\n",
      "Epoch  120: Train: 0.053485, Val: 0.014396, Total: 0.048599, LR: 2.12e-05 [WARMUP]\n",
      "          Best: Train: 0.048309, Val: 0.016676, Total: 0.044355 (Epoch 117)\n",
      "Epoch  121: Train: 0.049731, Val: 0.014774, Total: 0.045361, LR: 2.14e-05 [WARMUP]\n",
      "          Best: Train: 0.048309, Val: 0.016676, Total: 0.044355 (Epoch 117)\n",
      "Epoch  122: Train: 0.052099, Val: 0.014536, Total: 0.047404, LR: 2.16e-05 [WARMUP]\n",
      "          Best: Train: 0.048309, Val: 0.016676, Total: 0.044355 (Epoch 117)\n",
      "Epoch  123: Train: 0.049098, Val: 0.013805, Total: 0.044686, LR: 2.17e-05 [WARMUP]\n",
      "          Best: Train: 0.048309, Val: 0.016676, Total: 0.044355 (Epoch 117)\n",
      "Epoch  124: Train: 0.041184, Val: 0.013129, Total: 0.037677, LR: 2.19e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.041184, Val: 0.013129, Total: 0.037677 (Epoch 124)\n",
      "Epoch  125: Train: 0.050958, Val: 0.013506, Total: 0.046276, LR: 2.21e-05 [WARMUP]\n",
      "          Best: Train: 0.041184, Val: 0.013129, Total: 0.037677 (Epoch 124)\n",
      "Epoch  126: Train: 0.054320, Val: 0.014038, Total: 0.049285, LR: 2.23e-05 [WARMUP]\n",
      "          Best: Train: 0.041184, Val: 0.013129, Total: 0.037677 (Epoch 124)\n",
      "Epoch  127: Train: 0.042957, Val: 0.014699, Total: 0.039424, LR: 2.25e-05 [WARMUP]\n",
      "          Best: Train: 0.041184, Val: 0.013129, Total: 0.037677 (Epoch 124)\n",
      "Epoch  128: Train: 0.040756, Val: 0.015381, Total: 0.037585, LR: 2.26e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.040756, Val: 0.015381, Total: 0.037585 (Epoch 128)\n",
      "Epoch  129: Train: 0.042147, Val: 0.015443, Total: 0.038809, LR: 2.28e-05 [WARMUP]\n",
      "          Best: Train: 0.040756, Val: 0.015381, Total: 0.037585 (Epoch 128)\n",
      "Epoch  130: Train: 0.040399, Val: 0.014600, Total: 0.037174, LR: 2.30e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.040399, Val: 0.014600, Total: 0.037174 (Epoch 130)\n",
      "Epoch  131: Train: 0.044163, Val: 0.013849, Total: 0.040373, LR: 2.32e-05 [WARMUP]\n",
      "          Best: Train: 0.040399, Val: 0.014600, Total: 0.037174 (Epoch 130)\n",
      "Epoch  132: Train: 0.037760, Val: 0.013078, Total: 0.034675, LR: 2.33e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.037760, Val: 0.013078, Total: 0.034675 (Epoch 132)\n",
      "Epoch  133: Train: 0.034120, Val: 0.012519, Total: 0.031420, LR: 2.35e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.034120, Val: 0.012519, Total: 0.031420 (Epoch 133)\n",
      "Epoch  134: Train: 0.042359, Val: 0.012127, Total: 0.038580, LR: 2.37e-05 [WARMUP]\n",
      "          Best: Train: 0.034120, Val: 0.012519, Total: 0.031420 (Epoch 133)\n",
      "Epoch  135: Train: 0.036992, Val: 0.012027, Total: 0.033871, LR: 2.39e-05 [WARMUP]\n",
      "          Best: Train: 0.034120, Val: 0.012519, Total: 0.031420 (Epoch 133)\n",
      "Epoch  136: Train: 0.031654, Val: 0.012533, Total: 0.029264, LR: 2.40e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.031654, Val: 0.012533, Total: 0.029264 (Epoch 136)\n",
      "Epoch  137: Train: 0.034613, Val: 0.012315, Total: 0.031826, LR: 2.42e-05 [WARMUP]\n",
      "          Best: Train: 0.031654, Val: 0.012533, Total: 0.029264 (Epoch 136)\n",
      "Epoch  138: Train: 0.035273, Val: 0.011792, Total: 0.032338, LR: 2.44e-05 [WARMUP]\n",
      "          Best: Train: 0.031654, Val: 0.012533, Total: 0.029264 (Epoch 136)\n",
      "Epoch  139: Train: 0.035672, Val: 0.011160, Total: 0.032608, LR: 2.46e-05 [WARMUP]\n",
      "          Best: Train: 0.031654, Val: 0.012533, Total: 0.029264 (Epoch 136)\n",
      "Epoch  140: Train: 0.031771, Val: 0.010273, Total: 0.029084, LR: 2.47e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.031771, Val: 0.010273, Total: 0.029084 (Epoch 140)\n",
      "Epoch  141: Train: 0.033748, Val: 0.008406, Total: 0.030580, LR: 2.49e-05 [WARMUP]\n",
      "          Best: Train: 0.031771, Val: 0.010273, Total: 0.029084 (Epoch 140)\n",
      "Epoch  142: Train: 0.030935, Val: 0.007694, Total: 0.028030, LR: 2.51e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.030935, Val: 0.007694, Total: 0.028030 (Epoch 142)\n",
      "Epoch  143: Train: 0.034101, Val: 0.007263, Total: 0.030746, LR: 2.53e-05 [WARMUP]\n",
      "          Best: Train: 0.030935, Val: 0.007694, Total: 0.028030 (Epoch 142)\n",
      "Epoch  144: Train: 0.029575, Val: 0.007488, Total: 0.026814, LR: 2.55e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.029575, Val: 0.007488, Total: 0.026814 (Epoch 144)\n",
      "Epoch  145: Train: 0.030970, Val: 0.008127, Total: 0.028115, LR: 2.56e-05 [WARMUP]\n",
      "          Best: Train: 0.029575, Val: 0.007488, Total: 0.026814 (Epoch 144)\n",
      "Epoch  146: Train: 0.026631, Val: 0.008547, Total: 0.024371, LR: 2.58e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.026631, Val: 0.008547, Total: 0.024371 (Epoch 146)\n",
      "Epoch  147: Train: 0.030098, Val: 0.009135, Total: 0.027478, LR: 2.60e-05 [WARMUP]\n",
      "          Best: Train: 0.026631, Val: 0.008547, Total: 0.024371 (Epoch 146)\n",
      "Epoch  148: Train: 0.025233, Val: 0.009088, Total: 0.023215, LR: 2.62e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.025233, Val: 0.009088, Total: 0.023215 (Epoch 148)\n",
      "Epoch  149: Train: 0.027864, Val: 0.009424, Total: 0.025559, LR: 2.63e-05 [WARMUP]\n",
      "          Best: Train: 0.025233, Val: 0.009088, Total: 0.023215 (Epoch 148)\n",
      "Epoch  150: Train: 0.030498, Val: 0.009522, Total: 0.027876, LR: 2.65e-05 [WARMUP]\n",
      "          Best: Train: 0.025233, Val: 0.009088, Total: 0.023215 (Epoch 148)\n",
      "Epoch  151: Train: 0.026442, Val: 0.008776, Total: 0.024234, LR: 2.67e-05 [WARMUP]\n",
      "          Best: Train: 0.025233, Val: 0.009088, Total: 0.023215 (Epoch 148)\n",
      "Epoch  152: Train: 0.025138, Val: 0.007612, Total: 0.022948, LR: 2.69e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.025138, Val: 0.007612, Total: 0.022948 (Epoch 152)\n",
      "Epoch  153: Train: 0.026462, Val: 0.007315, Total: 0.024069, LR: 2.70e-05 [WARMUP]\n",
      "          Best: Train: 0.025138, Val: 0.007612, Total: 0.022948 (Epoch 152)\n",
      "Epoch  154: Train: 0.023595, Val: 0.007357, Total: 0.021565, LR: 2.72e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.023595, Val: 0.007357, Total: 0.021565 (Epoch 154)\n",
      "Epoch  155: Train: 0.024698, Val: 0.007118, Total: 0.022501, LR: 2.74e-05 [WARMUP]\n",
      "          Best: Train: 0.023595, Val: 0.007357, Total: 0.021565 (Epoch 154)\n",
      "Epoch  156: Train: 0.026101, Val: 0.006836, Total: 0.023692, LR: 2.76e-05 [WARMUP]\n",
      "          Best: Train: 0.023595, Val: 0.007357, Total: 0.021565 (Epoch 154)\n",
      "Epoch  157: Train: 0.023418, Val: 0.006801, Total: 0.021340, LR: 2.78e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.023418, Val: 0.006801, Total: 0.021340 (Epoch 157)\n",
      "Epoch  158: Train: 0.023858, Val: 0.006755, Total: 0.021721, LR: 2.79e-05 [WARMUP]\n",
      "          Best: Train: 0.023418, Val: 0.006801, Total: 0.021340 (Epoch 157)\n",
      "Epoch  159: Train: 0.023663, Val: 0.006974, Total: 0.021577, LR: 2.81e-05 [WARMUP]\n",
      "          Best: Train: 0.023418, Val: 0.006801, Total: 0.021340 (Epoch 157)\n",
      "Epoch  160: Train: 0.024554, Val: 0.007214, Total: 0.022386, LR: 2.83e-05 [WARMUP]\n",
      "          Best: Train: 0.023418, Val: 0.006801, Total: 0.021340 (Epoch 157)\n",
      "Epoch  161: Train: 0.023694, Val: 0.007387, Total: 0.021655, LR: 2.85e-05 [WARMUP]\n",
      "          Best: Train: 0.023418, Val: 0.006801, Total: 0.021340 (Epoch 157)\n",
      "Epoch  162: Train: 0.021347, Val: 0.007103, Total: 0.019566, LR: 2.86e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.021347, Val: 0.007103, Total: 0.019566 (Epoch 162)\n",
      "Epoch  163: Train: 0.022506, Val: 0.005757, Total: 0.020412, LR: 2.88e-05 [WARMUP]\n",
      "          Best: Train: 0.021347, Val: 0.007103, Total: 0.019566 (Epoch 162)\n",
      "Epoch  164: Train: 0.020339, Val: 0.006509, Total: 0.018611, LR: 2.90e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.020339, Val: 0.006509, Total: 0.018611 (Epoch 164)\n",
      "Epoch  165: Train: 0.022317, Val: 0.006376, Total: 0.020325, LR: 2.92e-05 [WARMUP]\n",
      "          Best: Train: 0.020339, Val: 0.006509, Total: 0.018611 (Epoch 164)\n",
      "Epoch  166: Train: 0.021864, Val: 0.006890, Total: 0.019992, LR: 2.93e-05 [WARMUP]\n",
      "          Best: Train: 0.020339, Val: 0.006509, Total: 0.018611 (Epoch 164)\n",
      "Epoch  167: Train: 0.020648, Val: 0.006797, Total: 0.018916, LR: 2.95e-05 [WARMUP]\n",
      "          Best: Train: 0.020339, Val: 0.006509, Total: 0.018611 (Epoch 164)\n",
      "Epoch  168: Train: 0.021816, Val: 0.004817, Total: 0.019691, LR: 2.97e-05 [WARMUP]\n",
      "          Best: Train: 0.020339, Val: 0.006509, Total: 0.018611 (Epoch 164)\n",
      "Epoch  169: Train: 0.022209, Val: 0.004641, Total: 0.020013, LR: 2.99e-05 [WARMUP]\n",
      "          Best: Train: 0.020339, Val: 0.006509, Total: 0.018611 (Epoch 164)\n",
      "Epoch  170: Train: 0.020787, Val: 0.004279, Total: 0.018723, LR: 3.01e-05 [WARMUP]\n",
      "          Best: Train: 0.020339, Val: 0.006509, Total: 0.018611 (Epoch 164)\n",
      "Epoch  171: Train: 0.019632, Val: 0.004474, Total: 0.017737, LR: 3.02e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.019632, Val: 0.004474, Total: 0.017737 (Epoch 171)\n",
      "Epoch  172: Train: 0.019249, Val: 0.004512, Total: 0.017407, LR: 3.04e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.019249, Val: 0.004512, Total: 0.017407 (Epoch 172)\n",
      "Epoch  173: Train: 0.018508, Val: 0.004173, Total: 0.016716, LR: 3.06e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.018508, Val: 0.004173, Total: 0.016716 (Epoch 173)\n",
      "Epoch  174: Train: 0.019245, Val: 0.003725, Total: 0.017305, LR: 3.08e-05 [WARMUP]\n",
      "          Best: Train: 0.018508, Val: 0.004173, Total: 0.016716 (Epoch 173)\n",
      "Epoch  175: Train: 0.018837, Val: 0.003444, Total: 0.016913, LR: 3.09e-05 [WARMUP]\n",
      "          Best: Train: 0.018508, Val: 0.004173, Total: 0.016716 (Epoch 173)\n",
      "Epoch  176: Train: 0.018292, Val: 0.003484, Total: 0.016441, LR: 3.11e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.018292, Val: 0.003484, Total: 0.016441 (Epoch 176)\n",
      "Epoch  177: Train: 0.019169, Val: 0.003723, Total: 0.017239, LR: 3.13e-05 [WARMUP]\n",
      "          Best: Train: 0.018292, Val: 0.003484, Total: 0.016441 (Epoch 176)\n",
      "Epoch  178: Train: 0.018707, Val: 0.004271, Total: 0.016902, LR: 3.15e-05 [WARMUP]\n",
      "          Best: Train: 0.018292, Val: 0.003484, Total: 0.016441 (Epoch 176)\n",
      "Epoch  179: Train: 0.019686, Val: 0.004939, Total: 0.017843, LR: 3.16e-05 [WARMUP]\n",
      "          Best: Train: 0.018292, Val: 0.003484, Total: 0.016441 (Epoch 176)\n",
      "Epoch  180: Train: 0.018705, Val: 0.004389, Total: 0.016916, LR: 3.18e-05 [WARMUP]\n",
      "          Best: Train: 0.018292, Val: 0.003484, Total: 0.016441 (Epoch 176)\n",
      "Epoch  181: Train: 0.018708, Val: 0.004722, Total: 0.016960, LR: 3.20e-05 [WARMUP]\n",
      "          Best: Train: 0.018292, Val: 0.003484, Total: 0.016441 (Epoch 176)\n",
      "Epoch  182: Train: 0.016886, Val: 0.004823, Total: 0.015378, LR: 3.22e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.016886, Val: 0.004823, Total: 0.015378 (Epoch 182)\n",
      "Epoch  183: Train: 0.016507, Val: 0.004264, Total: 0.014977, LR: 3.24e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.016507, Val: 0.004264, Total: 0.014977 (Epoch 183)\n",
      "Epoch  184: Train: 0.016643, Val: 0.003505, Total: 0.015001, LR: 3.25e-05 [WARMUP]\n",
      "          Best: Train: 0.016507, Val: 0.004264, Total: 0.014977 (Epoch 183)\n",
      "Epoch  185: Train: 0.017907, Val: 0.003225, Total: 0.016072, LR: 3.27e-05 [WARMUP]\n",
      "          Best: Train: 0.016507, Val: 0.004264, Total: 0.014977 (Epoch 183)\n",
      "Epoch  186: Train: 0.017253, Val: 0.003130, Total: 0.015488, LR: 3.29e-05 [WARMUP]\n",
      "          Best: Train: 0.016507, Val: 0.004264, Total: 0.014977 (Epoch 183)\n",
      "Epoch  187: Train: 0.017878, Val: 0.003290, Total: 0.016055, LR: 3.31e-05 [WARMUP]\n",
      "          Best: Train: 0.016507, Val: 0.004264, Total: 0.014977 (Epoch 183)\n",
      "Epoch  188: Train: 0.016862, Val: 0.003582, Total: 0.015202, LR: 3.32e-05 [WARMUP]\n",
      "          Best: Train: 0.016507, Val: 0.004264, Total: 0.014977 (Epoch 183)\n",
      "Epoch  189: Train: 0.016288, Val: 0.003626, Total: 0.014705, LR: 3.34e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.016288, Val: 0.003626, Total: 0.014705 (Epoch 189)\n",
      "Epoch  190: Train: 0.016711, Val: 0.003351, Total: 0.015041, LR: 3.36e-05 [WARMUP]\n",
      "          Best: Train: 0.016288, Val: 0.003626, Total: 0.014705 (Epoch 189)\n",
      "Epoch  191: Train: 0.016860, Val: 0.003216, Total: 0.015155, LR: 3.38e-05 [WARMUP]\n",
      "          Best: Train: 0.016288, Val: 0.003626, Total: 0.014705 (Epoch 189)\n",
      "Epoch  192: Train: 0.016311, Val: 0.003008, Total: 0.014648, LR: 3.39e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.016311, Val: 0.003008, Total: 0.014648 (Epoch 192)\n",
      "Epoch  193: Train: 0.016864, Val: 0.003101, Total: 0.015144, LR: 3.41e-05 [WARMUP]\n",
      "          Best: Train: 0.016311, Val: 0.003008, Total: 0.014648 (Epoch 192)\n",
      "Epoch  194: Train: 0.016802, Val: 0.003226, Total: 0.015105, LR: 3.43e-05 [WARMUP]\n",
      "          Best: Train: 0.016311, Val: 0.003008, Total: 0.014648 (Epoch 192)\n",
      "Epoch  195: Train: 0.016370, Val: 0.003601, Total: 0.014774, LR: 3.45e-05 [WARMUP]\n",
      "          Best: Train: 0.016311, Val: 0.003008, Total: 0.014648 (Epoch 192)\n",
      "Epoch  196: Train: 0.015826, Val: 0.003540, Total: 0.014290, LR: 3.46e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.015826, Val: 0.003540, Total: 0.014290 (Epoch 196)\n",
      "Epoch  197: Train: 0.015490, Val: 0.003026, Total: 0.013932, LR: 3.48e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.015490, Val: 0.003026, Total: 0.013932 (Epoch 197)\n",
      "Epoch  198: Train: 0.014914, Val: 0.002850, Total: 0.013406, LR: 3.50e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.014914, Val: 0.002850, Total: 0.013406 (Epoch 198)\n",
      "Epoch  199: Train: 0.014939, Val: 0.002833, Total: 0.013425, LR: 3.52e-05 [WARMUP]\n",
      "          Best: Train: 0.014914, Val: 0.002850, Total: 0.013406 (Epoch 198)\n",
      "Epoch  200: Train: 0.014963, Val: 0.002984, Total: 0.013466, LR: 3.54e-05 [WARMUP]\n",
      "          Best: Train: 0.014914, Val: 0.002850, Total: 0.013406 (Epoch 198)\n",
      "Epoch  201: Train: 0.014580, Val: 0.002901, Total: 0.013120, LR: 3.55e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.014580, Val: 0.002901, Total: 0.013120 (Epoch 201)\n",
      "Epoch  202: Train: 0.014200, Val: 0.002799, Total: 0.012775, LR: 3.57e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.014200, Val: 0.002799, Total: 0.012775 (Epoch 202)\n",
      "Epoch  203: Train: 0.014512, Val: 0.002900, Total: 0.013060, LR: 3.59e-05 [WARMUP]\n",
      "          Best: Train: 0.014200, Val: 0.002799, Total: 0.012775 (Epoch 202)\n",
      "Epoch  204: Train: 0.015204, Val: 0.002734, Total: 0.013645, LR: 3.61e-05 [WARMUP]\n",
      "          Best: Train: 0.014200, Val: 0.002799, Total: 0.012775 (Epoch 202)\n",
      "Epoch  205: Train: 0.014055, Val: 0.002704, Total: 0.012636, LR: 3.62e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.014055, Val: 0.002704, Total: 0.012636 (Epoch 205)\n",
      "Epoch  206: Train: 0.013584, Val: 0.002682, Total: 0.012221, LR: 3.64e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.013584, Val: 0.002682, Total: 0.012221 (Epoch 206)\n",
      "Epoch  207: Train: 0.014339, Val: 0.002748, Total: 0.012890, LR: 3.66e-05 [WARMUP]\n",
      "          Best: Train: 0.013584, Val: 0.002682, Total: 0.012221 (Epoch 206)\n",
      "Epoch  208: Train: 0.014784, Val: 0.002740, Total: 0.013278, LR: 3.68e-05 [WARMUP]\n",
      "          Best: Train: 0.013584, Val: 0.002682, Total: 0.012221 (Epoch 206)\n",
      "Epoch  209: Train: 0.014994, Val: 0.002872, Total: 0.013479, LR: 3.69e-05 [WARMUP]\n",
      "          Best: Train: 0.013584, Val: 0.002682, Total: 0.012221 (Epoch 206)\n",
      "Epoch  210: Train: 0.014683, Val: 0.003049, Total: 0.013229, LR: 3.71e-05 [WARMUP]\n",
      "          Best: Train: 0.013584, Val: 0.002682, Total: 0.012221 (Epoch 206)\n",
      "Epoch  211: Train: 0.013453, Val: 0.002828, Total: 0.012125, LR: 3.73e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.013453, Val: 0.002828, Total: 0.012125 (Epoch 211)\n",
      "Epoch  212: Train: 0.013456, Val: 0.002696, Total: 0.012111, LR: 3.75e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.013456, Val: 0.002696, Total: 0.012111 (Epoch 212)\n",
      "Epoch  213: Train: 0.013176, Val: 0.002548, Total: 0.011847, LR: 3.77e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.013176, Val: 0.002548, Total: 0.011847 (Epoch 213)\n",
      "Epoch  214: Train: 0.013248, Val: 0.002509, Total: 0.011906, LR: 3.78e-05 [WARMUP]\n",
      "          Best: Train: 0.013176, Val: 0.002548, Total: 0.011847 (Epoch 213)\n",
      "Epoch  215: Train: 0.013270, Val: 0.002558, Total: 0.011931, LR: 3.80e-05 [WARMUP]\n",
      "          Best: Train: 0.013176, Val: 0.002548, Total: 0.011847 (Epoch 213)\n",
      "Epoch  216: Train: 0.012741, Val: 0.002448, Total: 0.011454, LR: 3.82e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.012741, Val: 0.002448, Total: 0.011454 (Epoch 216)\n",
      "Epoch  217: Train: 0.013036, Val: 0.002392, Total: 0.011705, LR: 3.84e-05 [WARMUP]\n",
      "          Best: Train: 0.012741, Val: 0.002448, Total: 0.011454 (Epoch 216)\n",
      "Epoch  218: Train: 0.013040, Val: 0.002312, Total: 0.011699, LR: 3.85e-05 [WARMUP]\n",
      "          Best: Train: 0.012741, Val: 0.002448, Total: 0.011454 (Epoch 216)\n",
      "Epoch  219: Train: 0.013029, Val: 0.002343, Total: 0.011693, LR: 3.87e-05 [WARMUP]\n",
      "          Best: Train: 0.012741, Val: 0.002448, Total: 0.011454 (Epoch 216)\n",
      "Epoch  220: Train: 0.012922, Val: 0.002484, Total: 0.011618, LR: 3.89e-05 [WARMUP]\n",
      "          Best: Train: 0.012741, Val: 0.002448, Total: 0.011454 (Epoch 216)\n",
      "Epoch  221: Train: 0.012754, Val: 0.002673, Total: 0.011494, LR: 3.91e-05 [WARMUP]\n",
      "          Best: Train: 0.012741, Val: 0.002448, Total: 0.011454 (Epoch 216)\n",
      "Epoch  222: Train: 0.013413, Val: 0.002759, Total: 0.012082, LR: 3.92e-05 [WARMUP]\n",
      "          Best: Train: 0.012741, Val: 0.002448, Total: 0.011454 (Epoch 216)\n",
      "Epoch  223: Train: 0.012484, Val: 0.002892, Total: 0.011285, LR: 3.94e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.012484, Val: 0.002892, Total: 0.011285 (Epoch 223)\n",
      "Epoch  224: Train: 0.011952, Val: 0.002860, Total: 0.010815, LR: 3.96e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.011952, Val: 0.002860, Total: 0.010815 (Epoch 224)\n",
      "Epoch  225: Train: 0.011364, Val: 0.002733, Total: 0.010285, LR: 3.98e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.011364, Val: 0.002733, Total: 0.010285 (Epoch 225)\n",
      "Epoch  226: Train: 0.011560, Val: 0.002542, Total: 0.010433, LR: 4.00e-05 [WARMUP]\n",
      "          Best: Train: 0.011364, Val: 0.002733, Total: 0.010285 (Epoch 225)\n",
      "Epoch  227: Train: 0.011974, Val: 0.002441, Total: 0.010782, LR: 4.01e-05 [WARMUP]\n",
      "          Best: Train: 0.011364, Val: 0.002733, Total: 0.010285 (Epoch 225)\n",
      "Epoch  228: Train: 0.012326, Val: 0.002337, Total: 0.011077, LR: 4.03e-05 [WARMUP]\n",
      "          Best: Train: 0.011364, Val: 0.002733, Total: 0.010285 (Epoch 225)\n",
      "Epoch  229: Train: 0.012152, Val: 0.002307, Total: 0.010921, LR: 4.05e-05 [WARMUP]\n",
      "          Best: Train: 0.011364, Val: 0.002733, Total: 0.010285 (Epoch 225)\n",
      "Epoch  230: Train: 0.011253, Val: 0.002462, Total: 0.010154, LR: 4.07e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.011253, Val: 0.002462, Total: 0.010154 (Epoch 230)\n",
      "Epoch  231: Train: 0.011697, Val: 0.002673, Total: 0.010569, LR: 4.08e-05 [WARMUP]\n",
      "          Best: Train: 0.011253, Val: 0.002462, Total: 0.010154 (Epoch 230)\n",
      "Epoch  232: Train: 0.011378, Val: 0.002887, Total: 0.010317, LR: 4.10e-05 [WARMUP]\n",
      "          Best: Train: 0.011253, Val: 0.002462, Total: 0.010154 (Epoch 230)\n",
      "Epoch  233: Train: 0.011979, Val: 0.002712, Total: 0.010820, LR: 4.12e-05 [WARMUP]\n",
      "          Best: Train: 0.011253, Val: 0.002462, Total: 0.010154 (Epoch 230)\n",
      "Epoch  234: Train: 0.011980, Val: 0.002535, Total: 0.010800, LR: 4.14e-05 [WARMUP]\n",
      "          Best: Train: 0.011253, Val: 0.002462, Total: 0.010154 (Epoch 230)\n",
      "Epoch  235: Train: 0.011499, Val: 0.002613, Total: 0.010388, LR: 4.15e-05 [WARMUP]\n",
      "          Best: Train: 0.011253, Val: 0.002462, Total: 0.010154 (Epoch 230)\n",
      "Epoch  236: Train: 0.010232, Val: 0.002782, Total: 0.009300, LR: 4.17e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.010232, Val: 0.002782, Total: 0.009300 (Epoch 236)\n",
      "Epoch  237: Train: 0.010507, Val: 0.002899, Total: 0.009556, LR: 4.19e-05 [WARMUP]\n",
      "          Best: Train: 0.010232, Val: 0.002782, Total: 0.009300 (Epoch 236)\n",
      "Epoch  238: Train: 0.011022, Val: 0.002806, Total: 0.009995, LR: 4.21e-05 [WARMUP]\n",
      "          Best: Train: 0.010232, Val: 0.002782, Total: 0.009300 (Epoch 236)\n",
      "Epoch  239: Train: 0.010876, Val: 0.002830, Total: 0.009870, LR: 4.22e-05 [WARMUP]\n",
      "          Best: Train: 0.010232, Val: 0.002782, Total: 0.009300 (Epoch 236)\n",
      "Epoch  240: Train: 0.011045, Val: 0.002893, Total: 0.010026, LR: 4.24e-05 [WARMUP]\n",
      "          Best: Train: 0.010232, Val: 0.002782, Total: 0.009300 (Epoch 236)\n",
      "Epoch  241: Train: 0.010386, Val: 0.002922, Total: 0.009453, LR: 4.26e-05 [WARMUP]\n",
      "          Best: Train: 0.010232, Val: 0.002782, Total: 0.009300 (Epoch 236)\n",
      "Epoch  242: Train: 0.010294, Val: 0.002959, Total: 0.009377, LR: 4.28e-05 [WARMUP]\n",
      "          Best: Train: 0.010232, Val: 0.002782, Total: 0.009300 (Epoch 236)\n",
      "Epoch  243: Train: 0.010051, Val: 0.002880, Total: 0.009155, LR: 4.30e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.010051, Val: 0.002880, Total: 0.009155 (Epoch 243)\n",
      "Epoch  244: Train: 0.009987, Val: 0.003025, Total: 0.009117, LR: 4.31e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.009987, Val: 0.003025, Total: 0.009117 (Epoch 244)\n",
      "Epoch  245: Train: 0.010454, Val: 0.003165, Total: 0.009543, LR: 4.33e-05 [WARMUP]\n",
      "          Best: Train: 0.009987, Val: 0.003025, Total: 0.009117 (Epoch 244)\n",
      "Epoch  246: Train: 0.010283, Val: 0.003131, Total: 0.009389, LR: 4.35e-05 [WARMUP]\n",
      "          Best: Train: 0.009987, Val: 0.003025, Total: 0.009117 (Epoch 244)\n",
      "Epoch  247: Train: 0.009780, Val: 0.003252, Total: 0.008964, LR: 4.37e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.009780, Val: 0.003252, Total: 0.008964 (Epoch 247)\n",
      "Epoch  248: Train: 0.009874, Val: 0.003390, Total: 0.009064, LR: 4.38e-05 [WARMUP]\n",
      "          Best: Train: 0.009780, Val: 0.003252, Total: 0.008964 (Epoch 247)\n",
      "Epoch  249: Train: 0.009869, Val: 0.003357, Total: 0.009055, LR: 4.40e-05 [WARMUP]\n",
      "          Best: Train: 0.009780, Val: 0.003252, Total: 0.008964 (Epoch 247)\n",
      "Epoch  250: Train: 0.009642, Val: 0.003424, Total: 0.008864, LR: 4.42e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.009642, Val: 0.003424, Total: 0.008864 (Epoch 250)\n",
      "Epoch  251: Train: 0.010166, Val: 0.003353, Total: 0.009314, LR: 4.44e-05 [WARMUP]\n",
      "          Best: Train: 0.009642, Val: 0.003424, Total: 0.008864 (Epoch 250)\n",
      "Epoch  252: Train: 0.009237, Val: 0.003561, Total: 0.008527, LR: 4.45e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.009237, Val: 0.003561, Total: 0.008527 (Epoch 252)\n",
      "Epoch  253: Train: 0.009177, Val: 0.003780, Total: 0.008502, LR: 4.47e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.009177, Val: 0.003780, Total: 0.008502 (Epoch 253)\n",
      "Epoch  254: Train: 0.008808, Val: 0.003728, Total: 0.008173, LR: 4.49e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.008808, Val: 0.003728, Total: 0.008173 (Epoch 254)\n",
      "Epoch  255: Train: 0.008619, Val: 0.003755, Total: 0.008011, LR: 4.51e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.008619, Val: 0.003755, Total: 0.008011 (Epoch 255)\n",
      "Epoch  256: Train: 0.009400, Val: 0.004042, Total: 0.008730, LR: 4.53e-05 [WARMUP]\n",
      "          Best: Train: 0.008619, Val: 0.003755, Total: 0.008011 (Epoch 255)\n",
      "Epoch  257: Train: 0.008732, Val: 0.003966, Total: 0.008137, LR: 4.54e-05 [WARMUP]\n",
      "          Best: Train: 0.008619, Val: 0.003755, Total: 0.008011 (Epoch 255)\n",
      "Epoch  258: Train: 0.008665, Val: 0.003835, Total: 0.008061, LR: 4.56e-05 [WARMUP]\n",
      "          Best: Train: 0.008619, Val: 0.003755, Total: 0.008011 (Epoch 255)\n",
      "Epoch  259: Train: 0.008755, Val: 0.003873, Total: 0.008145, LR: 4.58e-05 [WARMUP]\n",
      "          Best: Train: 0.008619, Val: 0.003755, Total: 0.008011 (Epoch 255)\n",
      "Epoch  260: Train: 0.008772, Val: 0.003852, Total: 0.008157, LR: 4.60e-05 [WARMUP]\n",
      "          Best: Train: 0.008619, Val: 0.003755, Total: 0.008011 (Epoch 255)\n",
      "Epoch  261: Train: 0.008218, Val: 0.004105, Total: 0.007704, LR: 4.61e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.008218, Val: 0.004105, Total: 0.007704 (Epoch 261)\n",
      "Epoch  262: Train: 0.008980, Val: 0.004460, Total: 0.008415, LR: 4.63e-05 [WARMUP]\n",
      "          Best: Train: 0.008218, Val: 0.004105, Total: 0.007704 (Epoch 261)\n",
      "Epoch  263: Train: 0.008645, Val: 0.004516, Total: 0.008129, LR: 4.65e-05 [WARMUP]\n",
      "          Best: Train: 0.008218, Val: 0.004105, Total: 0.007704 (Epoch 261)\n",
      "Epoch  264: Train: 0.008325, Val: 0.004398, Total: 0.007834, LR: 4.67e-05 [WARMUP]\n",
      "          Best: Train: 0.008218, Val: 0.004105, Total: 0.007704 (Epoch 261)\n",
      "Epoch  265: Train: 0.008351, Val: 0.004146, Total: 0.007826, LR: 4.68e-05 [WARMUP]\n",
      "          Best: Train: 0.008218, Val: 0.004105, Total: 0.007704 (Epoch 261)\n",
      "Epoch  266: Train: 0.007949, Val: 0.004340, Total: 0.007498, LR: 4.70e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.007949, Val: 0.004340, Total: 0.007498 (Epoch 266)\n",
      "Epoch  267: Train: 0.007925, Val: 0.004519, Total: 0.007500, LR: 4.72e-05 [WARMUP]\n",
      "          Best: Train: 0.007949, Val: 0.004340, Total: 0.007498 (Epoch 266)\n",
      "Epoch  268: Train: 0.008460, Val: 0.004746, Total: 0.007996, LR: 4.74e-05 [WARMUP]\n",
      "          Best: Train: 0.007949, Val: 0.004340, Total: 0.007498 (Epoch 266)\n",
      "Epoch  269: Train: 0.007871, Val: 0.004468, Total: 0.007446, LR: 4.76e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.007871, Val: 0.004468, Total: 0.007446 (Epoch 269)\n",
      "Epoch  270: Train: 0.008396, Val: 0.004286, Total: 0.007882, LR: 4.77e-05 [WARMUP]\n",
      "          Best: Train: 0.007871, Val: 0.004468, Total: 0.007446 (Epoch 269)\n",
      "Epoch  271: Train: 0.007865, Val: 0.004293, Total: 0.007418, LR: 4.79e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.007865, Val: 0.004293, Total: 0.007418 (Epoch 271)\n",
      "Epoch  272: Train: 0.008503, Val: 0.003996, Total: 0.007940, LR: 4.81e-05 [WARMUP]\n",
      "          Best: Train: 0.007865, Val: 0.004293, Total: 0.007418 (Epoch 271)\n",
      "Epoch  273: Train: 0.008044, Val: 0.003912, Total: 0.007528, LR: 4.83e-05 [WARMUP]\n",
      "          Best: Train: 0.007865, Val: 0.004293, Total: 0.007418 (Epoch 271)\n",
      "Epoch  274: Train: 0.008196, Val: 0.004098, Total: 0.007684, LR: 4.84e-05 [WARMUP]\n",
      "          Best: Train: 0.007865, Val: 0.004293, Total: 0.007418 (Epoch 271)\n",
      "Epoch  275: Train: 0.007368, Val: 0.004164, Total: 0.006967, LR: 4.86e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.007368, Val: 0.004164, Total: 0.006967 (Epoch 275)\n",
      "Epoch  276: Train: 0.007850, Val: 0.004506, Total: 0.007432, LR: 4.88e-05 [WARMUP]\n",
      "          Best: Train: 0.007368, Val: 0.004164, Total: 0.006967 (Epoch 275)\n",
      "Epoch  277: Train: 0.007859, Val: 0.004456, Total: 0.007434, LR: 4.90e-05 [WARMUP]\n",
      "          Best: Train: 0.007368, Val: 0.004164, Total: 0.006967 (Epoch 275)\n",
      "Epoch  278: Train: 0.007973, Val: 0.004282, Total: 0.007512, LR: 4.91e-05 [WARMUP]\n",
      "          Best: Train: 0.007368, Val: 0.004164, Total: 0.006967 (Epoch 275)\n",
      "Epoch  279: Train: 0.007515, Val: 0.004160, Total: 0.007096, LR: 4.93e-05 [WARMUP]\n",
      "          Best: Train: 0.007368, Val: 0.004164, Total: 0.006967 (Epoch 275)\n",
      "Epoch  280: Train: 0.007460, Val: 0.003945, Total: 0.007021, LR: 4.95e-05 [WARMUP]\n",
      "          Best: Train: 0.007368, Val: 0.004164, Total: 0.006967 (Epoch 275)\n",
      "Epoch  281: Train: 0.007958, Val: 0.003958, Total: 0.007458, LR: 4.97e-05 [WARMUP]\n",
      "          Best: Train: 0.007368, Val: 0.004164, Total: 0.006967 (Epoch 275)\n",
      "Epoch  282: Train: 0.007336, Val: 0.003818, Total: 0.006896, LR: 4.99e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.007336, Val: 0.003818, Total: 0.006896 (Epoch 282)\n",
      "Epoch  283: Train: 0.007632, Val: 0.003657, Total: 0.007135, LR: 5.00e-05 [WARMUP]\n",
      "          Best: Train: 0.007336, Val: 0.003818, Total: 0.006896 (Epoch 282)\n",
      "Epoch  284: Train: 0.006972, Val: 0.003531, Total: 0.006542, LR: 5.02e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.006972, Val: 0.003531, Total: 0.006542 (Epoch 284)\n",
      "Epoch  285: Train: 0.007297, Val: 0.003564, Total: 0.006830, LR: 5.04e-05 [WARMUP]\n",
      "          Best: Train: 0.006972, Val: 0.003531, Total: 0.006542 (Epoch 284)\n",
      "Epoch  286: Train: 0.006935, Val: 0.003664, Total: 0.006526, LR: 5.06e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.006935, Val: 0.003664, Total: 0.006526 (Epoch 286)\n",
      "Epoch  287: Train: 0.006507, Val: 0.003659, Total: 0.006151, LR: 5.07e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.006507, Val: 0.003659, Total: 0.006151 (Epoch 287)\n",
      "Epoch  288: Train: 0.007530, Val: 0.003931, Total: 0.007080, LR: 5.09e-05 [WARMUP]\n",
      "          Best: Train: 0.006507, Val: 0.003659, Total: 0.006151 (Epoch 287)\n",
      "Epoch  289: Train: 0.006779, Val: 0.003818, Total: 0.006409, LR: 5.11e-05 [WARMUP]\n",
      "          Best: Train: 0.006507, Val: 0.003659, Total: 0.006151 (Epoch 287)\n",
      "Epoch  290: Train: 0.007311, Val: 0.003890, Total: 0.006884, LR: 5.13e-05 [WARMUP]\n",
      "          Best: Train: 0.006507, Val: 0.003659, Total: 0.006151 (Epoch 287)\n",
      "Epoch  291: Train: 0.006577, Val: 0.003865, Total: 0.006238, LR: 5.14e-05 [WARMUP]\n",
      "          Best: Train: 0.006507, Val: 0.003659, Total: 0.006151 (Epoch 287)\n",
      "Epoch  292: Train: 0.006736, Val: 0.004024, Total: 0.006397, LR: 5.16e-05 [WARMUP]\n",
      "          Best: Train: 0.006507, Val: 0.003659, Total: 0.006151 (Epoch 287)\n",
      "Epoch  293: Train: 0.007246, Val: 0.004283, Total: 0.006876, LR: 5.18e-05 [WARMUP]\n",
      "          Best: Train: 0.006507, Val: 0.003659, Total: 0.006151 (Epoch 287)\n",
      "Epoch  294: Train: 0.006880, Val: 0.004421, Total: 0.006573, LR: 5.20e-05 [WARMUP]\n",
      "          Best: Train: 0.006507, Val: 0.003659, Total: 0.006151 (Epoch 287)\n",
      "Epoch  295: Train: 0.006360, Val: 0.004146, Total: 0.006083, LR: 5.21e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.006360, Val: 0.004146, Total: 0.006083 (Epoch 295)\n",
      "Epoch  296: Train: 0.006547, Val: 0.004101, Total: 0.006241, LR: 5.23e-05 [WARMUP]\n",
      "          Best: Train: 0.006360, Val: 0.004146, Total: 0.006083 (Epoch 295)\n",
      "Epoch  297: Train: 0.006883, Val: 0.003826, Total: 0.006501, LR: 5.25e-05 [WARMUP]\n",
      "          Best: Train: 0.006360, Val: 0.004146, Total: 0.006083 (Epoch 295)\n",
      "Epoch  298: Train: 0.006255, Val: 0.003863, Total: 0.005956, LR: 5.27e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.006255, Val: 0.003863, Total: 0.005956 (Epoch 298)\n",
      "Epoch  299: Train: 0.006552, Val: 0.003977, Total: 0.006231, LR: 5.29e-05 [WARMUP]\n",
      "          Best: Train: 0.006255, Val: 0.003863, Total: 0.005956 (Epoch 298)\n",
      "Epoch  300: Train: 0.006763, Val: 0.003757, Total: 0.006387, LR: 5.30e-05 [WARMUP]\n",
      "          Best: Train: 0.006255, Val: 0.003863, Total: 0.005956 (Epoch 298)\n",
      "Epoch  301: Train: 0.006621, Val: 0.003785, Total: 0.006266, LR: 5.32e-05 [WARMUP]\n",
      "          Best: Train: 0.006255, Val: 0.003863, Total: 0.005956 (Epoch 298)\n",
      "Epoch  302: Train: 0.006480, Val: 0.003589, Total: 0.006119, LR: 5.34e-05 [WARMUP]\n",
      "          Best: Train: 0.006255, Val: 0.003863, Total: 0.005956 (Epoch 298)\n",
      "Epoch  303: Train: 0.006399, Val: 0.003556, Total: 0.006044, LR: 5.36e-05 [WARMUP]\n",
      "          Best: Train: 0.006255, Val: 0.003863, Total: 0.005956 (Epoch 298)\n",
      "Epoch  304: Train: 0.006225, Val: 0.003597, Total: 0.005897, LR: 5.37e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.006225, Val: 0.003597, Total: 0.005897 (Epoch 304)\n",
      "Epoch  305: Train: 0.006116, Val: 0.003805, Total: 0.005827, LR: 5.39e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.006116, Val: 0.003805, Total: 0.005827 (Epoch 305)\n",
      "Epoch  306: Train: 0.005989, Val: 0.003812, Total: 0.005716, LR: 5.41e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005989, Val: 0.003812, Total: 0.005716 (Epoch 306)\n",
      "Epoch  307: Train: 0.005887, Val: 0.003787, Total: 0.005625, LR: 5.43e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005887, Val: 0.003787, Total: 0.005625 (Epoch 307)\n",
      "Epoch  308: Train: 0.005817, Val: 0.003942, Total: 0.005582, LR: 5.44e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005817, Val: 0.003942, Total: 0.005582 (Epoch 308)\n",
      "Epoch  309: Train: 0.006475, Val: 0.003978, Total: 0.006163, LR: 5.46e-05 [WARMUP]\n",
      "          Best: Train: 0.005817, Val: 0.003942, Total: 0.005582 (Epoch 308)\n",
      "Epoch  310: Train: 0.006139, Val: 0.003729, Total: 0.005838, LR: 5.48e-05 [WARMUP]\n",
      "          Best: Train: 0.005817, Val: 0.003942, Total: 0.005582 (Epoch 308)\n",
      "Epoch  311: Train: 0.005855, Val: 0.003652, Total: 0.005579, LR: 5.50e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005855, Val: 0.003652, Total: 0.005579 (Epoch 311)\n",
      "Epoch  312: Train: 0.005954, Val: 0.003544, Total: 0.005653, LR: 5.52e-05 [WARMUP]\n",
      "          Best: Train: 0.005855, Val: 0.003652, Total: 0.005579 (Epoch 311)\n",
      "Epoch  313: Train: 0.005807, Val: 0.003601, Total: 0.005531, LR: 5.53e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005807, Val: 0.003601, Total: 0.005531 (Epoch 313)\n",
      "Epoch  314: Train: 0.006246, Val: 0.003531, Total: 0.005907, LR: 5.55e-05 [WARMUP]\n",
      "          Best: Train: 0.005807, Val: 0.003601, Total: 0.005531 (Epoch 313)\n",
      "Epoch  315: Train: 0.006036, Val: 0.003670, Total: 0.005740, LR: 5.57e-05 [WARMUP]\n",
      "          Best: Train: 0.005807, Val: 0.003601, Total: 0.005531 (Epoch 313)\n",
      "Epoch  316: Train: 0.005670, Val: 0.003890, Total: 0.005447, LR: 5.59e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005670, Val: 0.003890, Total: 0.005447 (Epoch 316)\n",
      "Epoch  317: Train: 0.006517, Val: 0.003499, Total: 0.006140, LR: 5.60e-05 [WARMUP]\n",
      "          Best: Train: 0.005670, Val: 0.003890, Total: 0.005447 (Epoch 316)\n",
      "Epoch  318: Train: 0.005828, Val: 0.003056, Total: 0.005481, LR: 5.62e-05 [WARMUP]\n",
      "          Best: Train: 0.005670, Val: 0.003890, Total: 0.005447 (Epoch 316)\n",
      "Epoch  319: Train: 0.005772, Val: 0.003098, Total: 0.005438, LR: 5.64e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005772, Val: 0.003098, Total: 0.005438 (Epoch 319)\n",
      "Epoch  320: Train: 0.005838, Val: 0.003279, Total: 0.005518, LR: 5.66e-05 [WARMUP]\n",
      "          Best: Train: 0.005772, Val: 0.003098, Total: 0.005438 (Epoch 319)\n",
      "Epoch  321: Train: 0.006015, Val: 0.003432, Total: 0.005692, LR: 5.67e-05 [WARMUP]\n",
      "          Best: Train: 0.005772, Val: 0.003098, Total: 0.005438 (Epoch 319)\n",
      "Epoch  322: Train: 0.006198, Val: 0.003355, Total: 0.005842, LR: 5.69e-05 [WARMUP]\n",
      "          Best: Train: 0.005772, Val: 0.003098, Total: 0.005438 (Epoch 319)\n",
      "Epoch  323: Train: 0.005447, Val: 0.003220, Total: 0.005169, LR: 5.71e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005447, Val: 0.003220, Total: 0.005169 (Epoch 323)\n",
      "Epoch  324: Train: 0.005686, Val: 0.003118, Total: 0.005365, LR: 5.73e-05 [WARMUP]\n",
      "          Best: Train: 0.005447, Val: 0.003220, Total: 0.005169 (Epoch 323)\n",
      "Epoch  325: Train: 0.005757, Val: 0.003242, Total: 0.005442, LR: 5.75e-05 [WARMUP]\n",
      "          Best: Train: 0.005447, Val: 0.003220, Total: 0.005169 (Epoch 323)\n",
      "Epoch  326: Train: 0.005334, Val: 0.003188, Total: 0.005066, LR: 5.76e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005334, Val: 0.003188, Total: 0.005066 (Epoch 326)\n",
      "Epoch  327: Train: 0.005692, Val: 0.003210, Total: 0.005381, LR: 5.78e-05 [WARMUP]\n",
      "          Best: Train: 0.005334, Val: 0.003188, Total: 0.005066 (Epoch 326)\n",
      "Epoch  328: Train: 0.005477, Val: 0.003231, Total: 0.005196, LR: 5.80e-05 [WARMUP]\n",
      "          Best: Train: 0.005334, Val: 0.003188, Total: 0.005066 (Epoch 326)\n",
      "Epoch  329: Train: 0.005408, Val: 0.003158, Total: 0.005127, LR: 5.82e-05 [WARMUP]\n",
      "          Best: Train: 0.005334, Val: 0.003188, Total: 0.005066 (Epoch 326)\n",
      "Epoch  330: Train: 0.005561, Val: 0.002985, Total: 0.005239, LR: 5.83e-05 [WARMUP]\n",
      "          Best: Train: 0.005334, Val: 0.003188, Total: 0.005066 (Epoch 326)\n",
      "Epoch  331: Train: 0.005320, Val: 0.002927, Total: 0.005021, LR: 5.85e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005320, Val: 0.002927, Total: 0.005021 (Epoch 331)\n",
      "Epoch  332: Train: 0.005271, Val: 0.002968, Total: 0.004983, LR: 5.87e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005271, Val: 0.002968, Total: 0.004983 (Epoch 332)\n",
      "Epoch  333: Train: 0.005242, Val: 0.003091, Total: 0.004973, LR: 5.89e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.005242, Val: 0.003091, Total: 0.004973 (Epoch 333)\n",
      "Epoch  334: Train: 0.005442, Val: 0.002991, Total: 0.005136, LR: 5.90e-05 [WARMUP]\n",
      "          Best: Train: 0.005242, Val: 0.003091, Total: 0.004973 (Epoch 333)\n",
      "Epoch  335: Train: 0.004841, Val: 0.002968, Total: 0.004607, LR: 5.92e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.004841, Val: 0.002968, Total: 0.004607 (Epoch 335)\n",
      "Epoch  336: Train: 0.005084, Val: 0.003050, Total: 0.004829, LR: 5.94e-05 [WARMUP]\n",
      "          Best: Train: 0.004841, Val: 0.002968, Total: 0.004607 (Epoch 335)\n",
      "Epoch  337: Train: 0.005231, Val: 0.002954, Total: 0.004947, LR: 5.96e-05 [WARMUP]\n",
      "          Best: Train: 0.004841, Val: 0.002968, Total: 0.004607 (Epoch 335)\n",
      "Epoch  338: Train: 0.004672, Val: 0.002875, Total: 0.004447, LR: 5.98e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.004672, Val: 0.002875, Total: 0.004447 (Epoch 338)\n",
      "Epoch  339: Train: 0.004833, Val: 0.002828, Total: 0.004582, LR: 5.99e-05 [WARMUP]\n",
      "          Best: Train: 0.004672, Val: 0.002875, Total: 0.004447 (Epoch 338)\n",
      "Epoch  340: Train: 0.005356, Val: 0.002894, Total: 0.005049, LR: 6.01e-05 [WARMUP]\n",
      "          Best: Train: 0.004672, Val: 0.002875, Total: 0.004447 (Epoch 338)\n",
      "Epoch  341: Train: 0.005230, Val: 0.002900, Total: 0.004938, LR: 6.03e-05 [WARMUP]\n",
      "          Best: Train: 0.004672, Val: 0.002875, Total: 0.004447 (Epoch 338)\n",
      "Epoch  342: Train: 0.004825, Val: 0.002937, Total: 0.004589, LR: 6.05e-05 [WARMUP]\n",
      "          Best: Train: 0.004672, Val: 0.002875, Total: 0.004447 (Epoch 338)\n",
      "Epoch  343: Train: 0.004629, Val: 0.002833, Total: 0.004405, LR: 6.06e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.004629, Val: 0.002833, Total: 0.004405 (Epoch 343)\n",
      "Epoch  344: Train: 0.004623, Val: 0.002854, Total: 0.004402, LR: 6.08e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.004623, Val: 0.002854, Total: 0.004402 (Epoch 344)\n",
      "Epoch  345: Train: 0.004999, Val: 0.002784, Total: 0.004722, LR: 6.10e-05 [WARMUP]\n",
      "          Best: Train: 0.004623, Val: 0.002854, Total: 0.004402 (Epoch 344)\n",
      "Epoch  346: Train: 0.004810, Val: 0.002897, Total: 0.004571, LR: 6.12e-05 [WARMUP]\n",
      "          Best: Train: 0.004623, Val: 0.002854, Total: 0.004402 (Epoch 344)\n",
      "Epoch  347: Train: 0.004849, Val: 0.002995, Total: 0.004617, LR: 6.13e-05 [WARMUP]\n",
      "          Best: Train: 0.004623, Val: 0.002854, Total: 0.004402 (Epoch 344)\n",
      "Epoch  348: Train: 0.004819, Val: 0.003076, Total: 0.004601, LR: 6.15e-05 [WARMUP]\n",
      "          Best: Train: 0.004623, Val: 0.002854, Total: 0.004402 (Epoch 344)\n",
      "Epoch  349: Train: 0.004436, Val: 0.002981, Total: 0.004254, LR: 6.17e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.004436, Val: 0.002981, Total: 0.004254 (Epoch 349)\n",
      "Epoch  350: Train: 0.004763, Val: 0.002802, Total: 0.004518, LR: 6.19e-05 [WARMUP]\n",
      "          Best: Train: 0.004436, Val: 0.002981, Total: 0.004254 (Epoch 349)\n",
      "Epoch  351: Train: 0.004597, Val: 0.002708, Total: 0.004361, LR: 6.20e-05 [WARMUP]\n",
      "          Best: Train: 0.004436, Val: 0.002981, Total: 0.004254 (Epoch 349)\n",
      "Epoch  352: Train: 0.004947, Val: 0.002868, Total: 0.004688, LR: 6.22e-05 [WARMUP]\n",
      "          Best: Train: 0.004436, Val: 0.002981, Total: 0.004254 (Epoch 349)\n",
      "Epoch  353: Train: 0.004606, Val: 0.002866, Total: 0.004388, LR: 6.24e-05 [WARMUP]\n",
      "          Best: Train: 0.004436, Val: 0.002981, Total: 0.004254 (Epoch 349)\n",
      "Epoch  354: Train: 0.004309, Val: 0.002687, Total: 0.004106, LR: 6.26e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.004309, Val: 0.002687, Total: 0.004106 (Epoch 354)\n",
      "Epoch  355: Train: 0.004696, Val: 0.002729, Total: 0.004450, LR: 6.28e-05 [WARMUP]\n",
      "          Best: Train: 0.004309, Val: 0.002687, Total: 0.004106 (Epoch 354)\n",
      "Epoch  356: Train: 0.004504, Val: 0.002734, Total: 0.004283, LR: 6.29e-05 [WARMUP]\n",
      "          Best: Train: 0.004309, Val: 0.002687, Total: 0.004106 (Epoch 354)\n",
      "Epoch  357: Train: 0.004695, Val: 0.002727, Total: 0.004449, LR: 6.31e-05 [WARMUP]\n",
      "          Best: Train: 0.004309, Val: 0.002687, Total: 0.004106 (Epoch 354)\n",
      "Epoch  358: Train: 0.004535, Val: 0.002767, Total: 0.004314, LR: 6.33e-05 [WARMUP]\n",
      "          Best: Train: 0.004309, Val: 0.002687, Total: 0.004106 (Epoch 354)\n",
      "Epoch  359: Train: 0.004300, Val: 0.002852, Total: 0.004119, LR: 6.35e-05 [WARMUP]\n",
      "          Best: Train: 0.004309, Val: 0.002687, Total: 0.004106 (Epoch 354)\n",
      "Epoch  360: Train: 0.004329, Val: 0.002837, Total: 0.004143, LR: 6.36e-05 [WARMUP]\n",
      "          Best: Train: 0.004309, Val: 0.002687, Total: 0.004106 (Epoch 354)\n",
      "Epoch  361: Train: 0.004286, Val: 0.002475, Total: 0.004059, LR: 6.38e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.004286, Val: 0.002475, Total: 0.004059 (Epoch 361)\n",
      "Epoch  362: Train: 0.004063, Val: 0.002415, Total: 0.003857, LR: 6.40e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.004063, Val: 0.002415, Total: 0.003857 (Epoch 362)\n",
      "Epoch  363: Train: 0.004534, Val: 0.002549, Total: 0.004285, LR: 6.42e-05 [WARMUP]\n",
      "          Best: Train: 0.004063, Val: 0.002415, Total: 0.003857 (Epoch 362)\n",
      "Epoch  364: Train: 0.004554, Val: 0.002672, Total: 0.004318, LR: 6.43e-05 [WARMUP]\n",
      "          Best: Train: 0.004063, Val: 0.002415, Total: 0.003857 (Epoch 362)\n",
      "Epoch  365: Train: 0.004187, Val: 0.002528, Total: 0.003980, LR: 6.45e-05 [WARMUP]\n",
      "          Best: Train: 0.004063, Val: 0.002415, Total: 0.003857 (Epoch 362)\n",
      "Epoch  366: Train: 0.004317, Val: 0.002526, Total: 0.004093, LR: 6.47e-05 [WARMUP]\n",
      "          Best: Train: 0.004063, Val: 0.002415, Total: 0.003857 (Epoch 362)\n",
      "Epoch  367: Train: 0.004424, Val: 0.002318, Total: 0.004161, LR: 6.49e-05 [WARMUP]\n",
      "          Best: Train: 0.004063, Val: 0.002415, Total: 0.003857 (Epoch 362)\n",
      "Epoch  368: Train: 0.004325, Val: 0.002311, Total: 0.004073, LR: 6.51e-05 [WARMUP]\n",
      "          Best: Train: 0.004063, Val: 0.002415, Total: 0.003857 (Epoch 362)\n",
      "Epoch  369: Train: 0.004186, Val: 0.002229, Total: 0.003941, LR: 6.52e-05 [WARMUP]\n",
      "          Best: Train: 0.004063, Val: 0.002415, Total: 0.003857 (Epoch 362)\n",
      "Epoch  370: Train: 0.003867, Val: 0.002214, Total: 0.003661, LR: 6.54e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  371: Train: 0.004245, Val: 0.002358, Total: 0.004009, LR: 6.56e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  372: Train: 0.004395, Val: 0.002659, Total: 0.004178, LR: 6.58e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  373: Train: 0.004072, Val: 0.002766, Total: 0.003908, LR: 6.59e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  374: Train: 0.003990, Val: 0.002522, Total: 0.003806, LR: 6.61e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  375: Train: 0.004202, Val: 0.002206, Total: 0.003953, LR: 6.63e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  376: Train: 0.004568, Val: 0.002132, Total: 0.004264, LR: 6.65e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  377: Train: 0.004089, Val: 0.002153, Total: 0.003847, LR: 6.66e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  378: Train: 0.004290, Val: 0.002369, Total: 0.004050, LR: 6.68e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  379: Train: 0.004001, Val: 0.002681, Total: 0.003836, LR: 6.70e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  380: Train: 0.003853, Val: 0.002647, Total: 0.003702, LR: 6.72e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  381: Train: 0.003918, Val: 0.002332, Total: 0.003720, LR: 6.74e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  382: Train: 0.003931, Val: 0.002240, Total: 0.003720, LR: 6.75e-05 [WARMUP]\n",
      "          Best: Train: 0.003867, Val: 0.002214, Total: 0.003661 (Epoch 370)\n",
      "Epoch  383: Train: 0.003576, Val: 0.002374, Total: 0.003426, LR: 6.77e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003576, Val: 0.002374, Total: 0.003426 (Epoch 383)\n",
      "Epoch  384: Train: 0.003670, Val: 0.002235, Total: 0.003491, LR: 6.79e-05 [WARMUP]\n",
      "          Best: Train: 0.003576, Val: 0.002374, Total: 0.003426 (Epoch 383)\n",
      "Epoch  385: Train: 0.003688, Val: 0.002204, Total: 0.003503, LR: 6.81e-05 [WARMUP]\n",
      "          Best: Train: 0.003576, Val: 0.002374, Total: 0.003426 (Epoch 383)\n",
      "Epoch  386: Train: 0.003713, Val: 0.002185, Total: 0.003522, LR: 6.82e-05 [WARMUP]\n",
      "          Best: Train: 0.003576, Val: 0.002374, Total: 0.003426 (Epoch 383)\n",
      "Epoch  387: Train: 0.003543, Val: 0.002319, Total: 0.003390, LR: 6.84e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003543, Val: 0.002319, Total: 0.003390 (Epoch 387)\n",
      "Epoch  388: Train: 0.003869, Val: 0.002347, Total: 0.003679, LR: 6.86e-05 [WARMUP]\n",
      "          Best: Train: 0.003543, Val: 0.002319, Total: 0.003390 (Epoch 387)\n",
      "Epoch  389: Train: 0.003722, Val: 0.002357, Total: 0.003552, LR: 6.88e-05 [WARMUP]\n",
      "          Best: Train: 0.003543, Val: 0.002319, Total: 0.003390 (Epoch 387)\n",
      "Epoch  390: Train: 0.003791, Val: 0.002422, Total: 0.003620, LR: 6.89e-05 [WARMUP]\n",
      "          Best: Train: 0.003543, Val: 0.002319, Total: 0.003390 (Epoch 387)\n",
      "Epoch  391: Train: 0.003775, Val: 0.002247, Total: 0.003584, LR: 6.91e-05 [WARMUP]\n",
      "          Best: Train: 0.003543, Val: 0.002319, Total: 0.003390 (Epoch 387)\n",
      "Epoch  392: Train: 0.003668, Val: 0.002117, Total: 0.003474, LR: 6.93e-05 [WARMUP]\n",
      "          Best: Train: 0.003543, Val: 0.002319, Total: 0.003390 (Epoch 387)\n",
      "Epoch  393: Train: 0.003544, Val: 0.002174, Total: 0.003373, LR: 6.95e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003544, Val: 0.002174, Total: 0.003373 (Epoch 393)\n",
      "Epoch  394: Train: 0.003564, Val: 0.002246, Total: 0.003399, LR: 6.97e-05 [WARMUP]\n",
      "          Best: Train: 0.003544, Val: 0.002174, Total: 0.003373 (Epoch 393)\n",
      "Epoch  395: Train: 0.003676, Val: 0.002343, Total: 0.003509, LR: 6.98e-05 [WARMUP]\n",
      "          Best: Train: 0.003544, Val: 0.002174, Total: 0.003373 (Epoch 393)\n",
      "Epoch  396: Train: 0.003865, Val: 0.002377, Total: 0.003679, LR: 7.00e-05 [WARMUP]\n",
      "          Best: Train: 0.003544, Val: 0.002174, Total: 0.003373 (Epoch 393)\n",
      "Epoch  397: Train: 0.003955, Val: 0.002350, Total: 0.003754, LR: 7.02e-05 [WARMUP]\n",
      "          Best: Train: 0.003544, Val: 0.002174, Total: 0.003373 (Epoch 393)\n",
      "Epoch  398: Train: 0.003858, Val: 0.002500, Total: 0.003688, LR: 7.04e-05 [WARMUP]\n",
      "          Best: Train: 0.003544, Val: 0.002174, Total: 0.003373 (Epoch 393)\n",
      "Epoch  399: Train: 0.003430, Val: 0.002558, Total: 0.003321, LR: 7.05e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003430, Val: 0.002558, Total: 0.003321 (Epoch 399)\n",
      "Epoch  400: Train: 0.003698, Val: 0.002312, Total: 0.003525, LR: 7.07e-05 [WARMUP]\n",
      "          Best: Train: 0.003430, Val: 0.002558, Total: 0.003321 (Epoch 399)\n",
      "Epoch  401: Train: 0.003280, Val: 0.002244, Total: 0.003150, LR: 7.09e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003280, Val: 0.002244, Total: 0.003150 (Epoch 401)\n",
      "Epoch  402: Train: 0.003627, Val: 0.002148, Total: 0.003442, LR: 7.11e-05 [WARMUP]\n",
      "          Best: Train: 0.003280, Val: 0.002244, Total: 0.003150 (Epoch 401)\n",
      "Epoch  403: Train: 0.003507, Val: 0.002074, Total: 0.003328, LR: 7.12e-05 [WARMUP]\n",
      "          Best: Train: 0.003280, Val: 0.002244, Total: 0.003150 (Epoch 401)\n",
      "Epoch  404: Train: 0.003329, Val: 0.002029, Total: 0.003167, LR: 7.14e-05 [WARMUP]\n",
      "          Best: Train: 0.003280, Val: 0.002244, Total: 0.003150 (Epoch 401)\n",
      "Epoch  405: Train: 0.003584, Val: 0.001965, Total: 0.003382, LR: 7.16e-05 [WARMUP]\n",
      "          Best: Train: 0.003280, Val: 0.002244, Total: 0.003150 (Epoch 401)\n",
      "Epoch  406: Train: 0.003555, Val: 0.002052, Total: 0.003367, LR: 7.18e-05 [WARMUP]\n",
      "          Best: Train: 0.003280, Val: 0.002244, Total: 0.003150 (Epoch 401)\n",
      "Epoch  407: Train: 0.003355, Val: 0.002161, Total: 0.003206, LR: 7.19e-05 [WARMUP]\n",
      "          Best: Train: 0.003280, Val: 0.002244, Total: 0.003150 (Epoch 401)\n",
      "Epoch  408: Train: 0.003377, Val: 0.002203, Total: 0.003230, LR: 7.21e-05 [WARMUP]\n",
      "          Best: Train: 0.003280, Val: 0.002244, Total: 0.003150 (Epoch 401)\n",
      "Epoch  409: Train: 0.003266, Val: 0.002281, Total: 0.003143, LR: 7.23e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003266, Val: 0.002281, Total: 0.003143 (Epoch 409)\n",
      "Epoch  410: Train: 0.003206, Val: 0.002337, Total: 0.003098, LR: 7.25e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003206, Val: 0.002337, Total: 0.003098 (Epoch 410)\n",
      "Epoch  411: Train: 0.003260, Val: 0.002243, Total: 0.003133, LR: 7.27e-05 [WARMUP]\n",
      "          Best: Train: 0.003206, Val: 0.002337, Total: 0.003098 (Epoch 410)\n",
      "Epoch  412: Train: 0.003489, Val: 0.002282, Total: 0.003338, LR: 7.28e-05 [WARMUP]\n",
      "          Best: Train: 0.003206, Val: 0.002337, Total: 0.003098 (Epoch 410)\n",
      "Epoch  413: Train: 0.003278, Val: 0.002198, Total: 0.003143, LR: 7.30e-05 [WARMUP]\n",
      "          Best: Train: 0.003206, Val: 0.002337, Total: 0.003098 (Epoch 410)\n",
      "Epoch  414: Train: 0.003181, Val: 0.002003, Total: 0.003034, LR: 7.32e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.003181, Val: 0.002003, Total: 0.003034 (Epoch 414)\n",
      "Epoch  415: Train: 0.003415, Val: 0.002018, Total: 0.003240, LR: 7.34e-05 [WARMUP]\n",
      "          Best: Train: 0.003181, Val: 0.002003, Total: 0.003034 (Epoch 414)\n",
      "Epoch  416: Train: 0.003180, Val: 0.002119, Total: 0.003048, LR: 7.35e-05 [WARMUP]\n",
      "          Best: Train: 0.003181, Val: 0.002003, Total: 0.003034 (Epoch 414)\n",
      "Epoch  417: Train: 0.003484, Val: 0.002107, Total: 0.003312, LR: 7.37e-05 [WARMUP]\n",
      "          Best: Train: 0.003181, Val: 0.002003, Total: 0.003034 (Epoch 414)\n",
      "Epoch  418: Train: 0.003317, Val: 0.002054, Total: 0.003159, LR: 7.39e-05 [WARMUP]\n",
      "          Best: Train: 0.003181, Val: 0.002003, Total: 0.003034 (Epoch 414)\n",
      "Epoch  419: Train: 0.002981, Val: 0.002192, Total: 0.002882, LR: 7.41e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002981, Val: 0.002192, Total: 0.002882 (Epoch 419)\n",
      "Epoch  420: Train: 0.003164, Val: 0.002057, Total: 0.003025, LR: 7.42e-05 [WARMUP]\n",
      "          Best: Train: 0.002981, Val: 0.002192, Total: 0.002882 (Epoch 419)\n",
      "Epoch  421: Train: 0.003169, Val: 0.002082, Total: 0.003033, LR: 7.44e-05 [WARMUP]\n",
      "          Best: Train: 0.002981, Val: 0.002192, Total: 0.002882 (Epoch 419)\n",
      "Epoch  422: Train: 0.003245, Val: 0.002184, Total: 0.003113, LR: 7.46e-05 [WARMUP]\n",
      "          Best: Train: 0.002981, Val: 0.002192, Total: 0.002882 (Epoch 419)\n",
      "Epoch  423: Train: 0.002953, Val: 0.002324, Total: 0.002874, LR: 7.48e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002953, Val: 0.002324, Total: 0.002874 (Epoch 423)\n",
      "Epoch  424: Train: 0.003078, Val: 0.002352, Total: 0.002988, LR: 7.50e-05 [WARMUP]\n",
      "          Best: Train: 0.002953, Val: 0.002324, Total: 0.002874 (Epoch 423)\n",
      "Epoch  425: Train: 0.003120, Val: 0.002227, Total: 0.003008, LR: 7.51e-05 [WARMUP]\n",
      "          Best: Train: 0.002953, Val: 0.002324, Total: 0.002874 (Epoch 423)\n",
      "Epoch  426: Train: 0.003296, Val: 0.002082, Total: 0.003145, LR: 7.53e-05 [WARMUP]\n",
      "          Best: Train: 0.002953, Val: 0.002324, Total: 0.002874 (Epoch 423)\n",
      "Epoch  427: Train: 0.003194, Val: 0.001995, Total: 0.003044, LR: 7.55e-05 [WARMUP]\n",
      "          Best: Train: 0.002953, Val: 0.002324, Total: 0.002874 (Epoch 423)\n",
      "Epoch  428: Train: 0.002932, Val: 0.002035, Total: 0.002820, LR: 7.57e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002932, Val: 0.002035, Total: 0.002820 (Epoch 428)\n",
      "Epoch  429: Train: 0.003197, Val: 0.002171, Total: 0.003069, LR: 7.58e-05 [WARMUP]\n",
      "          Best: Train: 0.002932, Val: 0.002035, Total: 0.002820 (Epoch 428)\n",
      "Epoch  430: Train: 0.002850, Val: 0.002109, Total: 0.002757, LR: 7.60e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002850, Val: 0.002109, Total: 0.002757 (Epoch 430)\n",
      "Epoch  431: Train: 0.002960, Val: 0.001987, Total: 0.002838, LR: 7.62e-05 [WARMUP]\n",
      "          Best: Train: 0.002850, Val: 0.002109, Total: 0.002757 (Epoch 430)\n",
      "Epoch  432: Train: 0.003077, Val: 0.001924, Total: 0.002933, LR: 7.64e-05 [WARMUP]\n",
      "          Best: Train: 0.002850, Val: 0.002109, Total: 0.002757 (Epoch 430)\n",
      "Epoch  433: Train: 0.003065, Val: 0.002001, Total: 0.002932, LR: 7.65e-05 [WARMUP]\n",
      "          Best: Train: 0.002850, Val: 0.002109, Total: 0.002757 (Epoch 430)\n",
      "Epoch  434: Train: 0.002746, Val: 0.002035, Total: 0.002657, LR: 7.67e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002746, Val: 0.002035, Total: 0.002657 (Epoch 434)\n",
      "Epoch  435: Train: 0.002905, Val: 0.002051, Total: 0.002798, LR: 7.69e-05 [WARMUP]\n",
      "          Best: Train: 0.002746, Val: 0.002035, Total: 0.002657 (Epoch 434)\n",
      "Epoch  436: Train: 0.002839, Val: 0.002040, Total: 0.002739, LR: 7.71e-05 [WARMUP]\n",
      "          Best: Train: 0.002746, Val: 0.002035, Total: 0.002657 (Epoch 434)\n",
      "Epoch  437: Train: 0.002819, Val: 0.001976, Total: 0.002714, LR: 7.73e-05 [WARMUP]\n",
      "          Best: Train: 0.002746, Val: 0.002035, Total: 0.002657 (Epoch 434)\n",
      "Epoch  438: Train: 0.002895, Val: 0.002031, Total: 0.002787, LR: 7.74e-05 [WARMUP]\n",
      "          Best: Train: 0.002746, Val: 0.002035, Total: 0.002657 (Epoch 434)\n",
      "Epoch  439: Train: 0.002606, Val: 0.002021, Total: 0.002533, LR: 7.76e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002606, Val: 0.002021, Total: 0.002533 (Epoch 439)\n",
      "Epoch  440: Train: 0.002776, Val: 0.002062, Total: 0.002687, LR: 7.78e-05 [WARMUP]\n",
      "          Best: Train: 0.002606, Val: 0.002021, Total: 0.002533 (Epoch 439)\n",
      "Epoch  441: Train: 0.002776, Val: 0.002066, Total: 0.002687, LR: 7.80e-05 [WARMUP]\n",
      "          Best: Train: 0.002606, Val: 0.002021, Total: 0.002533 (Epoch 439)\n",
      "Epoch  442: Train: 0.002872, Val: 0.002059, Total: 0.002770, LR: 7.81e-05 [WARMUP]\n",
      "          Best: Train: 0.002606, Val: 0.002021, Total: 0.002533 (Epoch 439)\n",
      "Epoch  443: Train: 0.002916, Val: 0.001945, Total: 0.002794, LR: 7.83e-05 [WARMUP]\n",
      "          Best: Train: 0.002606, Val: 0.002021, Total: 0.002533 (Epoch 439)\n",
      "Epoch  444: Train: 0.002594, Val: 0.001877, Total: 0.002504, LR: 7.85e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002594, Val: 0.001877, Total: 0.002504 (Epoch 444)\n",
      "Epoch  445: Train: 0.002791, Val: 0.001821, Total: 0.002670, LR: 7.87e-05 [WARMUP]\n",
      "          Best: Train: 0.002594, Val: 0.001877, Total: 0.002504 (Epoch 444)\n",
      "Epoch  446: Train: 0.002731, Val: 0.001852, Total: 0.002621, LR: 7.88e-05 [WARMUP]\n",
      "          Best: Train: 0.002594, Val: 0.001877, Total: 0.002504 (Epoch 444)\n",
      "Epoch  447: Train: 0.002610, Val: 0.001927, Total: 0.002525, LR: 7.90e-05 [WARMUP]\n",
      "          Best: Train: 0.002594, Val: 0.001877, Total: 0.002504 (Epoch 444)\n",
      "Epoch  448: Train: 0.002843, Val: 0.001919, Total: 0.002728, LR: 7.92e-05 [WARMUP]\n",
      "          Best: Train: 0.002594, Val: 0.001877, Total: 0.002504 (Epoch 444)\n",
      "Epoch  449: Train: 0.002549, Val: 0.001942, Total: 0.002473, LR: 7.94e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002549, Val: 0.001942, Total: 0.002473 (Epoch 449)\n",
      "Epoch  450: Train: 0.002740, Val: 0.001904, Total: 0.002635, LR: 7.95e-05 [WARMUP]\n",
      "          Best: Train: 0.002549, Val: 0.001942, Total: 0.002473 (Epoch 449)\n",
      "Epoch  451: Train: 0.002668, Val: 0.001954, Total: 0.002579, LR: 7.97e-05 [WARMUP]\n",
      "          Best: Train: 0.002549, Val: 0.001942, Total: 0.002473 (Epoch 449)\n",
      "Epoch  452: Train: 0.002767, Val: 0.001951, Total: 0.002665, LR: 7.99e-05 [WARMUP]\n",
      "          Best: Train: 0.002549, Val: 0.001942, Total: 0.002473 (Epoch 449)\n",
      "Epoch  453: Train: 0.002720, Val: 0.002025, Total: 0.002633, LR: 8.01e-05 [WARMUP]\n",
      "          Best: Train: 0.002549, Val: 0.001942, Total: 0.002473 (Epoch 449)\n",
      "Epoch  454: Train: 0.002597, Val: 0.002042, Total: 0.002527, LR: 8.03e-05 [WARMUP]\n",
      "          Best: Train: 0.002549, Val: 0.001942, Total: 0.002473 (Epoch 449)\n",
      "Epoch  455: Train: 0.002463, Val: 0.002027, Total: 0.002408, LR: 8.04e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002463, Val: 0.002027, Total: 0.002408 (Epoch 455)\n",
      "Epoch  456: Train: 0.002430, Val: 0.001950, Total: 0.002370, LR: 8.06e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  457: Train: 0.002570, Val: 0.001891, Total: 0.002485, LR: 8.08e-05 [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  458: Train: 0.002693, Val: 0.001926, Total: 0.002598, LR: 8.10e-05 [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  459: Train: 0.002745, Val: 0.002046, Total: 0.002657, LR: 8.11e-05 [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  460: Train: 0.002606, Val: 0.002088, Total: 0.002541, LR: 8.13e-05 [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  461: Train: 0.002596, Val: 0.001949, Total: 0.002515, LR: 8.15e-05 [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  462: Train: 0.002456, Val: 0.001980, Total: 0.002396, LR: 8.17e-05 [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  463: Train: 0.002519, Val: 0.001955, Total: 0.002449, LR: 8.18e-05 [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  464: Train: 0.002460, Val: 0.001950, Total: 0.002396, LR: 8.20e-05 [WARMUP]\n",
      "          Best: Train: 0.002430, Val: 0.001950, Total: 0.002370 (Epoch 456)\n",
      "Epoch  465: Train: 0.002332, Val: 0.001926, Total: 0.002281, LR: 8.22e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  466: Train: 0.002339, Val: 0.001959, Total: 0.002291, LR: 8.24e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  467: Train: 0.002495, Val: 0.001832, Total: 0.002413, LR: 8.26e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  468: Train: 0.002547, Val: 0.001803, Total: 0.002454, LR: 8.27e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  469: Train: 0.002403, Val: 0.001782, Total: 0.002325, LR: 8.29e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  470: Train: 0.002403, Val: 0.001820, Total: 0.002330, LR: 8.31e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  471: Train: 0.002360, Val: 0.001870, Total: 0.002299, LR: 8.33e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  472: Train: 0.002456, Val: 0.001907, Total: 0.002387, LR: 8.34e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  473: Train: 0.002528, Val: 0.001936, Total: 0.002454, LR: 8.36e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  474: Train: 0.002534, Val: 0.001979, Total: 0.002464, LR: 8.38e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  475: Train: 0.002462, Val: 0.001928, Total: 0.002395, LR: 8.40e-05 [WARMUP]\n",
      "          Best: Train: 0.002332, Val: 0.001926, Total: 0.002281 (Epoch 465)\n",
      "Epoch  476: Train: 0.002335, Val: 0.001853, Total: 0.002275, LR: 8.41e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002335, Val: 0.001853, Total: 0.002275 (Epoch 476)\n",
      "Epoch  477: Train: 0.002368, Val: 0.001838, Total: 0.002302, LR: 8.43e-05 [WARMUP]\n",
      "          Best: Train: 0.002335, Val: 0.001853, Total: 0.002275 (Epoch 476)\n",
      "Epoch  478: Train: 0.002235, Val: 0.001852, Total: 0.002187, LR: 8.45e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002235, Val: 0.001852, Total: 0.002187 (Epoch 478)\n",
      "Epoch  479: Train: 0.002347, Val: 0.001882, Total: 0.002289, LR: 8.47e-05 [WARMUP]\n",
      "          Best: Train: 0.002235, Val: 0.001852, Total: 0.002187 (Epoch 478)\n",
      "Epoch  480: Train: 0.002332, Val: 0.001808, Total: 0.002266, LR: 8.49e-05 [WARMUP]\n",
      "          Best: Train: 0.002235, Val: 0.001852, Total: 0.002187 (Epoch 478)\n",
      "Epoch  481: Train: 0.002344, Val: 0.001837, Total: 0.002280, LR: 8.50e-05 [WARMUP]\n",
      "          Best: Train: 0.002235, Val: 0.001852, Total: 0.002187 (Epoch 478)\n",
      "Epoch  482: Train: 0.002215, Val: 0.001944, Total: 0.002181, LR: 8.52e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002215, Val: 0.001944, Total: 0.002181 (Epoch 482)\n",
      "Epoch  483: Train: 0.002233, Val: 0.002063, Total: 0.002212, LR: 8.54e-05 [WARMUP]\n",
      "          Best: Train: 0.002215, Val: 0.001944, Total: 0.002181 (Epoch 482)\n",
      "Epoch  484: Train: 0.002228, Val: 0.002024, Total: 0.002202, LR: 8.56e-05 [WARMUP]\n",
      "          Best: Train: 0.002215, Val: 0.001944, Total: 0.002181 (Epoch 482)\n",
      "Epoch  485: Train: 0.002320, Val: 0.002084, Total: 0.002291, LR: 8.57e-05 [WARMUP]\n",
      "          Best: Train: 0.002215, Val: 0.001944, Total: 0.002181 (Epoch 482)\n",
      "Epoch  486: Train: 0.002234, Val: 0.002028, Total: 0.002209, LR: 8.59e-05 [WARMUP]\n",
      "          Best: Train: 0.002215, Val: 0.001944, Total: 0.002181 (Epoch 482)\n",
      "Epoch  487: Train: 0.002199, Val: 0.001943, Total: 0.002167, LR: 8.61e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002199, Val: 0.001943, Total: 0.002167 (Epoch 487)\n",
      "Epoch  488: Train: 0.002229, Val: 0.001954, Total: 0.002194, LR: 8.63e-05 [WARMUP]\n",
      "          Best: Train: 0.002199, Val: 0.001943, Total: 0.002167 (Epoch 487)\n",
      "Epoch  489: Train: 0.002318, Val: 0.002002, Total: 0.002279, LR: 8.64e-05 [WARMUP]\n",
      "          Best: Train: 0.002199, Val: 0.001943, Total: 0.002167 (Epoch 487)\n",
      "Epoch  490: Train: 0.002266, Val: 0.001932, Total: 0.002225, LR: 8.66e-05 [WARMUP]\n",
      "          Best: Train: 0.002199, Val: 0.001943, Total: 0.002167 (Epoch 487)\n",
      "Epoch  491: Train: 0.002326, Val: 0.001779, Total: 0.002258, LR: 8.68e-05 [WARMUP]\n",
      "          Best: Train: 0.002199, Val: 0.001943, Total: 0.002167 (Epoch 487)\n",
      "Epoch  492: Train: 0.002173, Val: 0.001753, Total: 0.002121, LR: 8.70e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002173, Val: 0.001753, Total: 0.002121 (Epoch 492)\n",
      "Epoch  493: Train: 0.002240, Val: 0.001794, Total: 0.002184, LR: 8.72e-05 [WARMUP]\n",
      "          Best: Train: 0.002173, Val: 0.001753, Total: 0.002121 (Epoch 492)\n",
      "Epoch  494: Train: 0.002192, Val: 0.001849, Total: 0.002149, LR: 8.73e-05 [WARMUP]\n",
      "          Best: Train: 0.002173, Val: 0.001753, Total: 0.002121 (Epoch 492)\n",
      "Epoch  495: Train: 0.002143, Val: 0.001882, Total: 0.002110, LR: 8.75e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002143, Val: 0.001882, Total: 0.002110 (Epoch 495)\n",
      "Epoch  496: Train: 0.002234, Val: 0.002076, Total: 0.002214, LR: 8.77e-05 [WARMUP]\n",
      "          Best: Train: 0.002143, Val: 0.001882, Total: 0.002110 (Epoch 495)\n",
      "Epoch  497: Train: 0.002168, Val: 0.002112, Total: 0.002161, LR: 8.79e-05 [WARMUP]\n",
      "          Best: Train: 0.002143, Val: 0.001882, Total: 0.002110 (Epoch 495)\n",
      "Epoch  498: Train: 0.002197, Val: 0.001967, Total: 0.002168, LR: 8.80e-05 [WARMUP]\n",
      "          Best: Train: 0.002143, Val: 0.001882, Total: 0.002110 (Epoch 495)\n",
      "Epoch  499: Train: 0.002165, Val: 0.001853, Total: 0.002126, LR: 8.82e-05 [WARMUP]\n",
      "          Best: Train: 0.002143, Val: 0.001882, Total: 0.002110 (Epoch 495)\n",
      "Epoch  500: Train: 0.002056, Val: 0.001851, Total: 0.002030, LR: 8.84e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002056, Val: 0.001851, Total: 0.002030 (Epoch 500)\n",
      "Epoch  501: Train: 0.002150, Val: 0.001917, Total: 0.002121, LR: 8.86e-05 [WARMUP]\n",
      "          Best: Train: 0.002056, Val: 0.001851, Total: 0.002030 (Epoch 500)\n",
      "Epoch  502: Train: 0.002287, Val: 0.001869, Total: 0.002235, LR: 8.87e-05 [WARMUP]\n",
      "          Best: Train: 0.002056, Val: 0.001851, Total: 0.002030 (Epoch 500)\n",
      "Epoch  503: Train: 0.002027, Val: 0.001920, Total: 0.002013, LR: 8.89e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002027, Val: 0.001920, Total: 0.002013 (Epoch 503)\n",
      "Epoch  504: Train: 0.002197, Val: 0.002018, Total: 0.002175, LR: 8.91e-05 [WARMUP]\n",
      "          Best: Train: 0.002027, Val: 0.001920, Total: 0.002013 (Epoch 503)\n",
      "Epoch  505: Train: 0.002115, Val: 0.001991, Total: 0.002100, LR: 8.93e-05 [WARMUP]\n",
      "          Best: Train: 0.002027, Val: 0.001920, Total: 0.002013 (Epoch 503)\n",
      "Epoch  506: Train: 0.002129, Val: 0.001880, Total: 0.002098, LR: 8.94e-05 [WARMUP]\n",
      "          Best: Train: 0.002027, Val: 0.001920, Total: 0.002013 (Epoch 503)\n",
      "Epoch  507: Train: 0.002189, Val: 0.001799, Total: 0.002140, LR: 8.96e-05 [WARMUP]\n",
      "          Best: Train: 0.002027, Val: 0.001920, Total: 0.002013 (Epoch 503)\n",
      "Epoch  508: Train: 0.002162, Val: 0.001765, Total: 0.002113, LR: 8.98e-05 [WARMUP]\n",
      "          Best: Train: 0.002027, Val: 0.001920, Total: 0.002013 (Epoch 503)\n",
      "Epoch  509: Train: 0.002157, Val: 0.001848, Total: 0.002118, LR: 9.00e-05 [WARMUP]\n",
      "          Best: Train: 0.002027, Val: 0.001920, Total: 0.002013 (Epoch 503)\n",
      "Epoch  510: Train: 0.002006, Val: 0.001815, Total: 0.001982, LR: 9.02e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.002006, Val: 0.001815, Total: 0.001982 (Epoch 510)\n",
      "Epoch  511: Train: 0.002065, Val: 0.001874, Total: 0.002041, LR: 9.03e-05 [WARMUP]\n",
      "          Best: Train: 0.002006, Val: 0.001815, Total: 0.001982 (Epoch 510)\n",
      "Epoch  512: Train: 0.002035, Val: 0.001862, Total: 0.002013, LR: 9.05e-05 [WARMUP]\n",
      "          Best: Train: 0.002006, Val: 0.001815, Total: 0.001982 (Epoch 510)\n",
      "Epoch  513: Train: 0.002036, Val: 0.001913, Total: 0.002021, LR: 9.07e-05 [WARMUP]\n",
      "          Best: Train: 0.002006, Val: 0.001815, Total: 0.001982 (Epoch 510)\n",
      "Epoch  514: Train: 0.002040, Val: 0.001985, Total: 0.002033, LR: 9.09e-05 [WARMUP]\n",
      "          Best: Train: 0.002006, Val: 0.001815, Total: 0.001982 (Epoch 510)\n",
      "Epoch  515: Train: 0.001935, Val: 0.002021, Total: 0.001946, LR: 9.10e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001935, Val: 0.002021, Total: 0.001946 (Epoch 515)\n",
      "Epoch  516: Train: 0.001946, Val: 0.001997, Total: 0.001953, LR: 9.12e-05 [WARMUP]\n",
      "          Best: Train: 0.001935, Val: 0.002021, Total: 0.001946 (Epoch 515)\n",
      "Epoch  517: Train: 0.002135, Val: 0.001901, Total: 0.002106, LR: 9.14e-05 [WARMUP]\n",
      "          Best: Train: 0.001935, Val: 0.002021, Total: 0.001946 (Epoch 515)\n",
      "Epoch  518: Train: 0.001985, Val: 0.001876, Total: 0.001971, LR: 9.16e-05 [WARMUP]\n",
      "          Best: Train: 0.001935, Val: 0.002021, Total: 0.001946 (Epoch 515)\n",
      "Epoch  519: Train: 0.001984, Val: 0.001945, Total: 0.001979, LR: 9.17e-05 [WARMUP]\n",
      "          Best: Train: 0.001935, Val: 0.002021, Total: 0.001946 (Epoch 515)\n",
      "Epoch  520: Train: 0.001896, Val: 0.001901, Total: 0.001897, LR: 9.19e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001896, Val: 0.001901, Total: 0.001897 (Epoch 520)\n",
      "Epoch  521: Train: 0.001954, Val: 0.001899, Total: 0.001947, LR: 9.21e-05 [WARMUP]\n",
      "          Best: Train: 0.001896, Val: 0.001901, Total: 0.001897 (Epoch 520)\n",
      "Epoch  522: Train: 0.002032, Val: 0.001862, Total: 0.002011, LR: 9.23e-05 [WARMUP]\n",
      "          Best: Train: 0.001896, Val: 0.001901, Total: 0.001897 (Epoch 520)\n",
      "Epoch  523: Train: 0.001883, Val: 0.001832, Total: 0.001876, LR: 9.25e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001883, Val: 0.001832, Total: 0.001876 (Epoch 523)\n",
      "Epoch  524: Train: 0.001901, Val: 0.001877, Total: 0.001898, LR: 9.26e-05 [WARMUP]\n",
      "          Best: Train: 0.001883, Val: 0.001832, Total: 0.001876 (Epoch 523)\n",
      "Epoch  525: Train: 0.002053, Val: 0.001799, Total: 0.002021, LR: 9.28e-05 [WARMUP]\n",
      "          Best: Train: 0.001883, Val: 0.001832, Total: 0.001876 (Epoch 523)\n",
      "Epoch  526: Train: 0.002006, Val: 0.001884, Total: 0.001991, LR: 9.30e-05 [WARMUP]\n",
      "          Best: Train: 0.001883, Val: 0.001832, Total: 0.001876 (Epoch 523)\n",
      "Epoch  527: Train: 0.001933, Val: 0.001949, Total: 0.001935, LR: 9.32e-05 [WARMUP]\n",
      "          Best: Train: 0.001883, Val: 0.001832, Total: 0.001876 (Epoch 523)\n",
      "Epoch  528: Train: 0.001903, Val: 0.002011, Total: 0.001916, LR: 9.33e-05 [WARMUP]\n",
      "          Best: Train: 0.001883, Val: 0.001832, Total: 0.001876 (Epoch 523)\n",
      "Epoch  529: Train: 0.001957, Val: 0.001956, Total: 0.001957, LR: 9.35e-05 [WARMUP]\n",
      "          Best: Train: 0.001883, Val: 0.001832, Total: 0.001876 (Epoch 523)\n",
      "Epoch  530: Train: 0.001891, Val: 0.001858, Total: 0.001887, LR: 9.37e-05 [WARMUP]\n",
      "          Best: Train: 0.001883, Val: 0.001832, Total: 0.001876 (Epoch 523)\n",
      "Epoch  531: Train: 0.001854, Val: 0.001783, Total: 0.001846, LR: 9.39e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001854, Val: 0.001783, Total: 0.001846 (Epoch 531)\n",
      "Epoch  532: Train: 0.001978, Val: 0.001840, Total: 0.001961, LR: 9.40e-05 [WARMUP]\n",
      "          Best: Train: 0.001854, Val: 0.001783, Total: 0.001846 (Epoch 531)\n",
      "Epoch  533: Train: 0.001857, Val: 0.001988, Total: 0.001874, LR: 9.42e-05 [WARMUP]\n",
      "          Best: Train: 0.001854, Val: 0.001783, Total: 0.001846 (Epoch 531)\n",
      "Epoch  534: Train: 0.001913, Val: 0.001982, Total: 0.001922, LR: 9.44e-05 [WARMUP]\n",
      "          Best: Train: 0.001854, Val: 0.001783, Total: 0.001846 (Epoch 531)\n",
      "Epoch  535: Train: 0.001788, Val: 0.002017, Total: 0.001816, LR: 9.46e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001788, Val: 0.002017, Total: 0.001816 (Epoch 535)\n",
      "Epoch  536: Train: 0.001799, Val: 0.002001, Total: 0.001824, LR: 9.48e-05 [WARMUP]\n",
      "          Best: Train: 0.001788, Val: 0.002017, Total: 0.001816 (Epoch 535)\n",
      "Epoch  537: Train: 0.001802, Val: 0.001987, Total: 0.001825, LR: 9.49e-05 [WARMUP]\n",
      "          Best: Train: 0.001788, Val: 0.002017, Total: 0.001816 (Epoch 535)\n",
      "Epoch  538: Train: 0.001762, Val: 0.001873, Total: 0.001776, LR: 9.51e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  539: Train: 0.001868, Val: 0.001811, Total: 0.001861, LR: 9.53e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  540: Train: 0.001921, Val: 0.001955, Total: 0.001926, LR: 9.55e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  541: Train: 0.001775, Val: 0.001999, Total: 0.001803, LR: 9.56e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  542: Train: 0.001803, Val: 0.001956, Total: 0.001822, LR: 9.58e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  543: Train: 0.001958, Val: 0.001947, Total: 0.001957, LR: 9.60e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  544: Train: 0.001749, Val: 0.001977, Total: 0.001777, LR: 9.62e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  545: Train: 0.001794, Val: 0.001989, Total: 0.001818, LR: 9.63e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  546: Train: 0.001781, Val: 0.001966, Total: 0.001804, LR: 9.65e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  547: Train: 0.001828, Val: 0.001930, Total: 0.001841, LR: 9.67e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  548: Train: 0.001835, Val: 0.002000, Total: 0.001856, LR: 9.69e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  549: Train: 0.001806, Val: 0.002044, Total: 0.001836, LR: 9.71e-05 [WARMUP]\n",
      "          Best: Train: 0.001762, Val: 0.001873, Total: 0.001776 (Epoch 538)\n",
      "Epoch  550: Train: 0.001743, Val: 0.001923, Total: 0.001765, LR: 9.72e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001743, Val: 0.001923, Total: 0.001765 (Epoch 550)\n",
      "Epoch  551: Train: 0.001886, Val: 0.001935, Total: 0.001892, LR: 9.74e-05 [WARMUP]\n",
      "          Best: Train: 0.001743, Val: 0.001923, Total: 0.001765 (Epoch 550)\n",
      "Epoch  552: Train: 0.001790, Val: 0.001904, Total: 0.001804, LR: 9.76e-05 [WARMUP]\n",
      "          Best: Train: 0.001743, Val: 0.001923, Total: 0.001765 (Epoch 550)\n",
      "Epoch  553: Train: 0.001770, Val: 0.001882, Total: 0.001784, LR: 9.78e-05 [WARMUP]\n",
      "          Best: Train: 0.001743, Val: 0.001923, Total: 0.001765 (Epoch 550)\n",
      "Epoch  554: Train: 0.001867, Val: 0.002024, Total: 0.001887, LR: 9.79e-05 [WARMUP]\n",
      "          Best: Train: 0.001743, Val: 0.001923, Total: 0.001765 (Epoch 550)\n",
      "Epoch  555: Train: 0.001841, Val: 0.002168, Total: 0.001882, LR: 9.81e-05 [WARMUP]\n",
      "          Best: Train: 0.001743, Val: 0.001923, Total: 0.001765 (Epoch 550)\n",
      "Epoch  556: Train: 0.001698, Val: 0.002052, Total: 0.001742, LR: 9.83e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001698, Val: 0.002052, Total: 0.001742 (Epoch 556)\n",
      "Epoch  557: Train: 0.001726, Val: 0.001938, Total: 0.001753, LR: 9.85e-05 [WARMUP]\n",
      "          Best: Train: 0.001698, Val: 0.002052, Total: 0.001742 (Epoch 556)\n",
      "Epoch  558: Train: 0.001775, Val: 0.001999, Total: 0.001803, LR: 9.86e-05 [WARMUP]\n",
      "          Best: Train: 0.001698, Val: 0.002052, Total: 0.001742 (Epoch 556)\n",
      "Epoch  559: Train: 0.001855, Val: 0.001956, Total: 0.001867, LR: 9.88e-05 [WARMUP]\n",
      "          Best: Train: 0.001698, Val: 0.002052, Total: 0.001742 (Epoch 556)\n",
      "Epoch  560: Train: 0.001717, Val: 0.001977, Total: 0.001749, LR: 9.90e-05 [WARMUP]\n",
      "          Best: Train: 0.001698, Val: 0.002052, Total: 0.001742 (Epoch 556)\n",
      "Epoch  561: Train: 0.001683, Val: 0.002017, Total: 0.001725, LR: 9.92e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001683, Val: 0.002017, Total: 0.001725 (Epoch 561)\n",
      "Epoch  562: Train: 0.001707, Val: 0.001931, Total: 0.001735, LR: 9.93e-05 [WARMUP]\n",
      "          Best: Train: 0.001683, Val: 0.002017, Total: 0.001725 (Epoch 561)\n",
      "Epoch  563: Train: 0.001735, Val: 0.001956, Total: 0.001762, LR: 9.95e-05 [WARMUP]\n",
      "          Best: Train: 0.001683, Val: 0.002017, Total: 0.001725 (Epoch 561)\n",
      "Epoch  564: Train: 0.001643, Val: 0.001996, Total: 0.001687, LR: 9.97e-05 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001643, Val: 0.001996, Total: 0.001687 (Epoch 564)\n",
      "Epoch  565: Train: 0.001673, Val: 0.001989, Total: 0.001712, LR: 9.99e-05 [WARMUP]\n",
      "          Best: Train: 0.001643, Val: 0.001996, Total: 0.001687 (Epoch 564)\n",
      "Epoch  566: Train: 0.001677, Val: 0.001935, Total: 0.001709, LR: 1.00e-04 [WARMUP]\n",
      "          Best: Train: 0.001643, Val: 0.001996, Total: 0.001687 (Epoch 564)\n",
      "Epoch  567: Train: 0.001658, Val: 0.001945, Total: 0.001694, LR: 1.00e-04 [WARMUP]\n",
      "          Best: Train: 0.001643, Val: 0.001996, Total: 0.001687 (Epoch 564)\n",
      "Epoch  568: Train: 0.001721, Val: 0.002012, Total: 0.001757, LR: 1.00e-04 [WARMUP]\n",
      "          Best: Train: 0.001643, Val: 0.001996, Total: 0.001687 (Epoch 564)\n",
      "Epoch  569: Train: 0.001759, Val: 0.002146, Total: 0.001807, LR: 1.01e-04 [WARMUP]\n",
      "          Best: Train: 0.001643, Val: 0.001996, Total: 0.001687 (Epoch 564)\n",
      "Epoch  570: Train: 0.001590, Val: 0.002014, Total: 0.001643, LR: 1.01e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  571: Train: 0.001705, Val: 0.001976, Total: 0.001739, LR: 1.01e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  572: Train: 0.001616, Val: 0.001999, Total: 0.001664, LR: 1.01e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  573: Train: 0.001677, Val: 0.002075, Total: 0.001727, LR: 1.01e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  574: Train: 0.001661, Val: 0.002121, Total: 0.001718, LR: 1.01e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  575: Train: 0.001642, Val: 0.001982, Total: 0.001684, LR: 1.02e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  576: Train: 0.001634, Val: 0.002051, Total: 0.001686, LR: 1.02e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  577: Train: 0.001679, Val: 0.002119, Total: 0.001734, LR: 1.02e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  578: Train: 0.001682, Val: 0.002008, Total: 0.001723, LR: 1.02e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  579: Train: 0.001612, Val: 0.002039, Total: 0.001666, LR: 1.02e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  580: Train: 0.001726, Val: 0.002120, Total: 0.001775, LR: 1.03e-04 [WARMUP]\n",
      "          Best: Train: 0.001590, Val: 0.002014, Total: 0.001643 (Epoch 570)\n",
      "Epoch  581: Train: 0.001572, Val: 0.002081, Total: 0.001636, LR: 1.03e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001572, Val: 0.002081, Total: 0.001636 (Epoch 581)\n",
      "Epoch  582: Train: 0.001570, Val: 0.002169, Total: 0.001645, LR: 1.03e-04 [WARMUP]\n",
      "          Best: Train: 0.001572, Val: 0.002081, Total: 0.001636 (Epoch 581)\n",
      "Epoch  583: Train: 0.001586, Val: 0.002068, Total: 0.001646, LR: 1.03e-04 [WARMUP]\n",
      "          Best: Train: 0.001572, Val: 0.002081, Total: 0.001636 (Epoch 581)\n",
      "Epoch  584: Train: 0.001698, Val: 0.001940, Total: 0.001728, LR: 1.03e-04 [WARMUP]\n",
      "          Best: Train: 0.001572, Val: 0.002081, Total: 0.001636 (Epoch 581)\n",
      "Epoch  585: Train: 0.001606, Val: 0.001954, Total: 0.001649, LR: 1.03e-04 [WARMUP]\n",
      "          Best: Train: 0.001572, Val: 0.002081, Total: 0.001636 (Epoch 581)\n",
      "Epoch  586: Train: 0.001538, Val: 0.002002, Total: 0.001596, LR: 1.04e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001538, Val: 0.002002, Total: 0.001596 (Epoch 586)\n",
      "Epoch  587: Train: 0.001576, Val: 0.001985, Total: 0.001627, LR: 1.04e-04 [WARMUP]\n",
      "          Best: Train: 0.001538, Val: 0.002002, Total: 0.001596 (Epoch 586)\n",
      "Epoch  588: Train: 0.001643, Val: 0.002039, Total: 0.001693, LR: 1.04e-04 [WARMUP]\n",
      "          Best: Train: 0.001538, Val: 0.002002, Total: 0.001596 (Epoch 586)\n",
      "Epoch  589: Train: 0.001561, Val: 0.002068, Total: 0.001624, LR: 1.04e-04 [WARMUP]\n",
      "          Best: Train: 0.001538, Val: 0.002002, Total: 0.001596 (Epoch 586)\n",
      "Epoch  590: Train: 0.001515, Val: 0.002030, Total: 0.001579, LR: 1.04e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001515, Val: 0.002030, Total: 0.001579 (Epoch 590)\n",
      "Epoch  591: Train: 0.001549, Val: 0.002098, Total: 0.001618, LR: 1.04e-04 [WARMUP]\n",
      "          Best: Train: 0.001515, Val: 0.002030, Total: 0.001579 (Epoch 590)\n",
      "Epoch  592: Train: 0.001641, Val: 0.002124, Total: 0.001701, LR: 1.05e-04 [WARMUP]\n",
      "          Best: Train: 0.001515, Val: 0.002030, Total: 0.001579 (Epoch 590)\n",
      "Epoch  593: Train: 0.001697, Val: 0.002084, Total: 0.001745, LR: 1.05e-04 [WARMUP]\n",
      "          Best: Train: 0.001515, Val: 0.002030, Total: 0.001579 (Epoch 590)\n",
      "Epoch  594: Train: 0.001472, Val: 0.002019, Total: 0.001541, LR: 1.05e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001472, Val: 0.002019, Total: 0.001541 (Epoch 594)\n",
      "Epoch  595: Train: 0.001581, Val: 0.002068, Total: 0.001642, LR: 1.05e-04 [WARMUP]\n",
      "          Best: Train: 0.001472, Val: 0.002019, Total: 0.001541 (Epoch 594)\n",
      "Epoch  596: Train: 0.001548, Val: 0.002010, Total: 0.001605, LR: 1.05e-04 [WARMUP]\n",
      "          Best: Train: 0.001472, Val: 0.002019, Total: 0.001541 (Epoch 594)\n",
      "Epoch  597: Train: 0.001525, Val: 0.002039, Total: 0.001589, LR: 1.06e-04 [WARMUP]\n",
      "          Best: Train: 0.001472, Val: 0.002019, Total: 0.001541 (Epoch 594)\n",
      "Epoch  598: Train: 0.001480, Val: 0.002032, Total: 0.001549, LR: 1.06e-04 [WARMUP]\n",
      "          Best: Train: 0.001472, Val: 0.002019, Total: 0.001541 (Epoch 594)\n",
      "Epoch  599: Train: 0.001446, Val: 0.002079, Total: 0.001525, LR: 1.06e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  600: Train: 0.001640, Val: 0.002186, Total: 0.001708, LR: 1.06e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  601: Train: 0.001514, Val: 0.002096, Total: 0.001587, LR: 1.06e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  602: Train: 0.001488, Val: 0.001945, Total: 0.001546, LR: 1.06e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  603: Train: 0.001515, Val: 0.002086, Total: 0.001586, LR: 1.07e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  604: Train: 0.001536, Val: 0.002030, Total: 0.001598, LR: 1.07e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  605: Train: 0.001531, Val: 0.002125, Total: 0.001605, LR: 1.07e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  606: Train: 0.001567, Val: 0.002103, Total: 0.001634, LR: 1.07e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  607: Train: 0.001556, Val: 0.001988, Total: 0.001610, LR: 1.07e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  608: Train: 0.001530, Val: 0.002094, Total: 0.001601, LR: 1.07e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  609: Train: 0.001492, Val: 0.002170, Total: 0.001577, LR: 1.08e-04 [WARMUP]\n",
      "          Best: Train: 0.001446, Val: 0.002079, Total: 0.001525 (Epoch 599)\n",
      "Epoch  610: Train: 0.001422, Val: 0.002012, Total: 0.001496, LR: 1.08e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001422, Val: 0.002012, Total: 0.001496 (Epoch 610)\n",
      "Epoch  611: Train: 0.001573, Val: 0.002082, Total: 0.001637, LR: 1.08e-04 [WARMUP]\n",
      "          Best: Train: 0.001422, Val: 0.002012, Total: 0.001496 (Epoch 610)\n",
      "Epoch  612: Train: 0.001546, Val: 0.001986, Total: 0.001601, LR: 1.08e-04 [WARMUP]\n",
      "          Best: Train: 0.001422, Val: 0.002012, Total: 0.001496 (Epoch 610)\n",
      "Epoch  613: Train: 0.001503, Val: 0.002038, Total: 0.001570, LR: 1.08e-04 [WARMUP]\n",
      "          Best: Train: 0.001422, Val: 0.002012, Total: 0.001496 (Epoch 610)\n",
      "Epoch  614: Train: 0.001510, Val: 0.001978, Total: 0.001569, LR: 1.09e-04 [WARMUP]\n",
      "          Best: Train: 0.001422, Val: 0.002012, Total: 0.001496 (Epoch 610)\n",
      "Epoch  615: Train: 0.001480, Val: 0.002019, Total: 0.001547, LR: 1.09e-04 [WARMUP]\n",
      "          Best: Train: 0.001422, Val: 0.002012, Total: 0.001496 (Epoch 610)\n",
      "Epoch  616: Train: 0.001475, Val: 0.002057, Total: 0.001548, LR: 1.09e-04 [WARMUP]\n",
      "          Best: Train: 0.001422, Val: 0.002012, Total: 0.001496 (Epoch 610)\n",
      "Epoch  617: Train: 0.001367, Val: 0.002123, Total: 0.001462, LR: 1.09e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  618: Train: 0.001519, Val: 0.002049, Total: 0.001585, LR: 1.09e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  619: Train: 0.001407, Val: 0.002163, Total: 0.001502, LR: 1.09e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  620: Train: 0.001376, Val: 0.002173, Total: 0.001476, LR: 1.10e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  621: Train: 0.001489, Val: 0.002075, Total: 0.001562, LR: 1.10e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  622: Train: 0.001517, Val: 0.002083, Total: 0.001588, LR: 1.10e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  623: Train: 0.001553, Val: 0.002179, Total: 0.001631, LR: 1.10e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  624: Train: 0.001412, Val: 0.002072, Total: 0.001494, LR: 1.10e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  625: Train: 0.001426, Val: 0.002024, Total: 0.001501, LR: 1.10e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  626: Train: 0.001453, Val: 0.002168, Total: 0.001543, LR: 1.11e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  627: Train: 0.001368, Val: 0.002173, Total: 0.001469, LR: 1.11e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  628: Train: 0.001451, Val: 0.002084, Total: 0.001530, LR: 1.11e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  629: Train: 0.001439, Val: 0.002114, Total: 0.001524, LR: 1.11e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  630: Train: 0.001493, Val: 0.002198, Total: 0.001581, LR: 1.11e-04 [WARMUP]\n",
      "          Best: Train: 0.001367, Val: 0.002123, Total: 0.001462 (Epoch 617)\n",
      "Epoch  631: Train: 0.001350, Val: 0.002099, Total: 0.001443, LR: 1.12e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  632: Train: 0.001366, Val: 0.002087, Total: 0.001456, LR: 1.12e-04 [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  633: Train: 0.001428, Val: 0.002144, Total: 0.001518, LR: 1.12e-04 [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  634: Train: 0.001469, Val: 0.002128, Total: 0.001551, LR: 1.12e-04 [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  635: Train: 0.001377, Val: 0.002201, Total: 0.001480, LR: 1.12e-04 [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  636: Train: 0.001422, Val: 0.002197, Total: 0.001519, LR: 1.12e-04 [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  637: Train: 0.001447, Val: 0.002157, Total: 0.001536, LR: 1.13e-04 [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  638: Train: 0.001412, Val: 0.002131, Total: 0.001502, LR: 1.13e-04 [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  639: Train: 0.001370, Val: 0.002164, Total: 0.001469, LR: 1.13e-04 [WARMUP]\n",
      "          Best: Train: 0.001350, Val: 0.002099, Total: 0.001443 (Epoch 631)\n",
      "Epoch  640: Train: 0.001288, Val: 0.002125, Total: 0.001392, LR: 1.13e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  641: Train: 0.001310, Val: 0.002232, Total: 0.001426, LR: 1.13e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  642: Train: 0.001367, Val: 0.002235, Total: 0.001476, LR: 1.13e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  643: Train: 0.001395, Val: 0.002151, Total: 0.001490, LR: 1.14e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  644: Train: 0.001513, Val: 0.002090, Total: 0.001585, LR: 1.14e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  645: Train: 0.001408, Val: 0.002248, Total: 0.001513, LR: 1.14e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  646: Train: 0.001369, Val: 0.002247, Total: 0.001479, LR: 1.14e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  647: Train: 0.001473, Val: 0.002310, Total: 0.001577, LR: 1.14e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  648: Train: 0.001436, Val: 0.002206, Total: 0.001532, LR: 1.15e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  649: Train: 0.001360, Val: 0.002143, Total: 0.001458, LR: 1.15e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  650: Train: 0.001334, Val: 0.002233, Total: 0.001447, LR: 1.15e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  651: Train: 0.001286, Val: 0.002267, Total: 0.001409, LR: 1.15e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  652: Train: 0.001524, Val: 0.002234, Total: 0.001613, LR: 1.15e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  653: Train: 0.001429, Val: 0.002235, Total: 0.001530, LR: 1.15e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  654: Train: 0.001463, Val: 0.001959, Total: 0.001525, LR: 1.16e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  655: Train: 0.001351, Val: 0.002050, Total: 0.001438, LR: 1.16e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  656: Train: 0.001413, Val: 0.002079, Total: 0.001496, LR: 1.16e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  657: Train: 0.001309, Val: 0.002170, Total: 0.001417, LR: 1.16e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  658: Train: 0.001331, Val: 0.002225, Total: 0.001443, LR: 1.16e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  659: Train: 0.001300, Val: 0.002115, Total: 0.001402, LR: 1.16e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  660: Train: 0.001325, Val: 0.002174, Total: 0.001431, LR: 1.17e-04 [WARMUP]\n",
      "          Best: Train: 0.001288, Val: 0.002125, Total: 0.001392 (Epoch 640)\n",
      "Epoch  661: Train: 0.001228, Val: 0.002178, Total: 0.001346, LR: 1.17e-04 ★ NEW BEST [WARMUP]\n",
      "          Best: Train: 0.001228, Val: 0.002178, Total: 0.001346 (Epoch 661)\n",
      "Epoch  662: Train: 0.001360, Val: 0.002189, Total: 0.001464, LR: 1.17e-04 [WARMUP]\n",
      "          Best: Train: 0.001228, Val: 0.002178, Total: 0.001346 (Epoch 661)\n"
     ]
    }
   ],
   "source": [
    "# Training loop with Noam scheduler (epoch-based) - based on train_single_fold function\n",
    "print(\"Starting training with Noam scheduler (epoch-based)...\")\n",
    "\n",
    "# 실제 데이터셋 크기 기반 동적 가중치 계산\n",
    "train_samples = len(train_loader.dataset)\n",
    "val_samples = len(val_loader.dataset)\n",
    "total_samples = train_samples + val_samples\n",
    "\n",
    "train_weight = train_samples / total_samples\n",
    "val_weight = val_samples / total_samples\n",
    "\n",
    "print(f\"데이터 분포 - Train: {train_samples}({train_weight:.3f}), Val: {val_samples}({val_weight:.3f})\")\n",
    "\n",
    "best_total_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "total_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 에포크 시작 시 학습률 업데이트\n",
    "    current_lr = scheduler.step_epoch()\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for input_seq, seq_lengths in train_loader:\n",
    "        try:\n",
    "            input_seq = input_seq.to(device)\n",
    "            seq_lengths = seq_lengths.to(device)\n",
    "            \n",
    "            # Teacher forcing 데이터 준비 (전류 제외한 input, 전류 포함한 target)\n",
    "            inputs, targets, target_seq_lengths = prepare_teacher_forcing_data(input_seq, seq_lengths)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs, target_seq_lengths)\n",
    "            loss = masked_mse_loss(predictions, targets, target_seq_lengths)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 그래디언트 클리핑\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training batch error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if train_batches == 0:\n",
    "        print(\"No valid training batches\")\n",
    "        break\n",
    "    \n",
    "    train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_seq, seq_lengths in val_loader:\n",
    "            try:\n",
    "                input_seq = input_seq.to(device)\n",
    "                seq_lengths = seq_lengths.to(device)\n",
    "                \n",
    "                inputs, targets, target_seq_lengths = prepare_teacher_forcing_data(input_seq, seq_lengths)\n",
    "                predictions = model(inputs, target_seq_lengths)\n",
    "                loss = masked_mse_loss(predictions, targets, target_seq_lengths)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Validation batch error: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if val_batches == 0:\n",
    "        print(\"No valid validation batches\")\n",
    "        break\n",
    "    \n",
    "    val_loss = val_loss / val_batches\n",
    "    \n",
    "    # Total loss 계산 (실제 데이터 분포 기반 동적 가중치)\n",
    "    total_loss = train_weight * train_loss + val_weight * val_loss\n",
    "    \n",
    "    # 기록 저장\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    total_losses.append(total_loss)\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Early stopping (total_loss 기준)\n",
    "    if total_loss < best_total_loss:\n",
    "        best_total_loss = total_loss\n",
    "        best_train_loss = train_loss\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "        # 베스트 모델 저장\n",
    "        torch.save(model.state_dict(), 'best_bmed_noam_model.pth')\n",
    "        best_status = \" ★ NEW BEST\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        best_status = \"\"\n",
    "    \n",
    "    # Warmup 구간 표시\n",
    "    if epoch + 1 <= warmup_epochs:\n",
    "        warmup_status = \" [WARMUP]\"\n",
    "    else:\n",
    "        warmup_status = \"\"\n",
    "    \n",
    "    # 매 에포크마다 로깅 - best 성능 정보 포함\n",
    "    print(f\"Epoch {epoch+1:4d}: Train: {train_loss:.6f}, Val: {val_loss:.6f}, Total: {total_loss:.6f}, LR: {current_lr:.2e}{best_status}{warmup_status}\")\n",
    "    \n",
    "    # Best 성능 정보 추가 표시 (매 에포크)\n",
    "    if epoch == 0:\n",
    "        print(f\"          Best: Train: {best_train_loss:.6f}, Val: {best_val_loss:.6f}, Total: {best_total_loss:.6f} (Epoch {best_epoch})\")\n",
    "    elif total_loss < best_total_loss:\n",
    "        print(f\"          ✓ Updated Best: Train: {best_train_loss:.6f}, Val: {best_val_loss:.6f}, Total: {best_total_loss:.6f}\")\n",
    "    else:\n",
    "        print(f\"          Best: Train: {best_train_loss:.6f}, Val: {best_val_loss:.6f}, Total: {best_total_loss:.6f} (Epoch {best_epoch})\")\n",
    "    \n",
    "    # Early stopping 체크 (total_loss 기준)\n",
    "    if epoch >= min_epochs and patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Best Performance (Epoch {best_epoch}):\")\n",
    "print(f\"  Best Total Loss:  {best_total_loss:.6f}\")\n",
    "print(f\"  Best Train Loss:  {best_train_loss:.6f}\")\n",
    "print(f\"  Best Val Loss:    {best_val_loss:.6f}\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Final Performance (Epoch {len(train_losses)}):\")\n",
    "print(f\"  Final Total Loss: {total_losses[-1]:.6f}\")\n",
    "print(f\"  Final Train Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss:   {val_losses[-1]:.6f}\")\n",
    "print(f\"  Final LR:         {current_lr:.2e}\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Warmup completed at epoch {warmup_epochs} with peak LR: {max(learning_rates):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1m06cppxd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 시각화 - total_loss 포함\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. 손실 함수 변화 (train, val, total)\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "axes[0, 0].plot(epochs, train_losses, label='Train Loss', alpha=0.7)\n",
    "axes[0, 0].plot(epochs, val_losses, label='Validation Loss', alpha=0.7)\n",
    "axes[0, 0].plot(epochs, total_losses, label='Total Loss (Weighted)', alpha=0.7, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training, Validation, and Total Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 학습률 변화 (Noam scheduler)\n",
    "axes[0, 1].plot(epochs, learning_rates, label='Learning Rate', color='orange', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Learning Rate')\n",
    "axes[0, 1].set_title('Noam Scheduler Learning Rate')\n",
    "axes[0, 1].set_yscale('log')  # 로그 스케일로 표시\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 학습률과 Total 손실의 관계\n",
    "scatter = axes[1, 0].scatter(learning_rates, total_losses, alpha=0.6, c=epochs, cmap='viridis', s=30)\n",
    "axes[1, 0].set_xlabel('Learning Rate')\n",
    "axes[1, 0].set_ylabel('Total Loss')\n",
    "axes[1, 0].set_title('Learning Rate vs Total Loss')\n",
    "axes[1, 0].set_xscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "cbar1 = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "cbar1.set_label('Epoch')\n",
    "\n",
    "# 4. Train vs Validation Loss 산점도\n",
    "scatter2 = axes[1, 1].scatter(train_losses, val_losses, alpha=0.6, c=epochs, cmap='plasma', s=30)\n",
    "axes[1, 1].set_xlabel('Train Loss')\n",
    "axes[1, 1].set_ylabel('Validation Loss')\n",
    "axes[1, 1].set_title('Train vs Validation Loss')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "# 대각선 추가 (perfect correlation)\n",
    "min_loss = min(min(train_losses), min(val_losses))\n",
    "max_loss = max(max(train_losses), max(val_losses))\n",
    "axes[1, 1].plot([min_loss, max_loss], [min_loss, max_loss], 'r--', alpha=0.5, label='Perfect Correlation')\n",
    "axes[1, 1].legend()\n",
    "cbar2 = plt.colorbar(scatter2, ax=axes[1, 1])\n",
    "cbar2.set_label('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 학습 통계 출력\n",
    "print(\"\\n=== Training Statistics ===\")\n",
    "print(f\"Total epochs: {len(train_losses)}\")\n",
    "print(f\"Best total loss: {best_total_loss:.6f}\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"Final total loss: {total_losses[-1]:.6f}\")\n",
    "print(f\"Initial learning rate: {learning_rates[0]:.2e}\")\n",
    "print(f\"Final learning rate: {learning_rates[-1]:.2e}\")\n",
    "print(f\"Max learning rate: {max(learning_rates):.2e}\")\n",
    "print(f\"Min learning rate: {min(learning_rates):.2e}\")\n",
    "\n",
    "# Noam scheduler 특성 분석\n",
    "print(f\"\\n=== Noam Scheduler Analysis ===\")\n",
    "print(f\"Model size (hidden_size): {scheduler.model_size}\")\n",
    "print(f\"Warmup steps: {scheduler.warmup_steps}\")\n",
    "print(f\"Factor: {scheduler.factor}\")\n",
    "warmup_lr = scheduler.factor * (scheduler.model_size ** (-0.5) * scheduler.warmup_steps ** (-0.5))\n",
    "print(f\"Peak learning rate at warmup: {warmup_lr:.2e}\")\n",
    "\n",
    "# 데이터 분포 가중치 정보\n",
    "print(f\"\\n=== Data Distribution Weights ===\")\n",
    "print(f\"Train samples: {train_samples} (weight: {train_weight:.3f})\")\n",
    "print(f\"Validation samples: {val_samples} (weight: {val_weight:.3f})\")\n",
    "print(f\"Total loss formula: {train_weight:.3f} * train_loss + {val_weight:.3f} * val_loss\")\n",
    "\n",
    "# 최고 성능 에포크 찾기\n",
    "best_epoch = total_losses.index(min(total_losses)) + 1\n",
    "print(f\"\\n=== Best Performance ===\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best total loss: {min(total_losses):.6f}\")\n",
    "print(f\"Train loss at best epoch: {train_losses[best_epoch-1]:.6f}\")\n",
    "print(f\"Val loss at best epoch: {val_losses[best_epoch-1]:.6f}\")\n",
    "print(f\"Learning rate at best epoch: {learning_rates[best_epoch-1]:.2e}\")\n",
    "\n",
    "# 모델 저장 정보\n",
    "print(f\"\\n=== Model Saved ===\")\n",
    "print(f\"Best model saved as: best_bmed_noam_model.pth\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
