{
  "lstm_hidden_size": 64,
  "lstm_n_layers": 4,
  "lstm_dropout": 0.1,
  "decoder_hidden_size": 128,
  "decoder_n_layers": 4,
  "decoder_dropout": 0.1,
  "current_hidden_size": 96,
  "current_n_layers": 2,
  "current_dropout": 0.1,
  "noam_factor": 1.6,
  "warmup_ratio": 0.2,
  "batch_size": 8
}