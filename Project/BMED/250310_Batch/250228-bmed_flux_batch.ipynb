{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from optuna.samplers import GPSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "class RawDataLoader():\n",
    "    def __init__(self, path='BMED_train_data_v2.xlsx'):\n",
    "        self.path = path\n",
    "        self.X_data, self.Y_data = self.RawData()\n",
    "\n",
    "    def RawData(self):\n",
    "        df = pd.read_excel(self.path, sheet_name='Sheet2')\n",
    "        X_data = df[['T','V','E','CF_LA','CF_K','CA_LA','CB_K']].values\n",
    "        Y_data = df[['dNLA','dNK','dVF','dVA','dVB']].values\n",
    "        return X_data, Y_data\n",
    "    \n",
    "    def FoldData(self):\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        folds = []\n",
    "        for train_index, test_index in kf.split(self.X_data):\n",
    "            # Split the data into training and test sets\n",
    "            X_train, X_test = self.X_data[train_index], self.X_data[test_index]\n",
    "            Y_train, Y_test = self.Y_data[train_index], self.Y_data[test_index]\n",
    "            \n",
    "            # Normalize the data\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_Y = StandardScaler()\n",
    "            \n",
    "            X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "            X_test_scaled = scaler_X.transform(X_test)\n",
    "            \n",
    "            Y_train_scaled = scaler_Y.fit_transform(Y_train)\n",
    "            Y_test_scaled = scaler_Y.transform(Y_test)\n",
    "\n",
    "            X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "            X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "            Y_train_tensor = torch.FloatTensor(Y_train_scaled)\n",
    "            Y_test_tensor = torch.FloatTensor(Y_test_scaled)\n",
    "\n",
    "            folds.append((X_train_tensor, X_test_tensor, Y_train_tensor, Y_test_tensor, scaler_X, scaler_Y))\n",
    "        return folds\n",
    "    \n",
    "# Customize the NN architecture\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, hidden_layers=2, hidden_nodes = 8):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        nodes = 7\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(nodes, hidden_nodes))\n",
    "            layers.append(nn.ReLU())\n",
    "            nodes = hidden_nodes\n",
    "        layers.append(nn.Linear(hidden_nodes, 5))\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.hidden(x)\n",
    "\n",
    "# Hyperparameter optimization\n",
    "class NNOpt():\n",
    "    def __init__(self, hidden_layers=2, hidden_nodes = 8, learning_rate=0.001, num_epochs=500, batch_size=256, weight_decay=1e-5):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.hidden_layers= hidden_layers\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model = CustomModel(hidden_layers=self.hidden_layers, hidden_nodes=self.hidden_nodes).to(self.device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train(self, X_train, Y_train, X_test, Y_test):\n",
    "        X_train_gpu = X_train.to(self.device)\n",
    "        Y_train_gpu = Y_train.to(self.device)\n",
    "        X_test_gpu = X_test.to(self.device)\n",
    "        Y_test_gpu = Y_test.to(self.device)\n",
    "\n",
    "        dataset = TensorDataset(X_train_gpu, Y_train_gpu)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for _ in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            for X_batch, Y_batch in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                train_outputs = self.model(X_batch)\n",
    "                train_loss = self.criterion(train_outputs, Y_batch)\n",
    "                train_loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            test_outputs = self.model(X_test_gpu)\n",
    "            test_loss = self.criterion(test_outputs, Y_test_gpu)\n",
    "        \n",
    "        return test_loss.item() \n",
    "\n",
    "# Objective function\n",
    "def objective(trial):\n",
    "    hidden_layers = trial.suggest_int('hidden_layers', 1, 10)\n",
    "    hidden_nodes = trial.suggest_int('hidden_nodes', 8, 128)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.1, log=True)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 100, 10000)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 1024)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "\n",
    "    data = RawDataLoader()\n",
    "    folds = data.FoldData()\n",
    "\n",
    "    test_losses = []\n",
    "    for _, (X_train, X_test, Y_train, Y_test, _, _) in enumerate(folds):\n",
    "        model = NNOpt(\n",
    "            hidden_layers=hidden_layers, \n",
    "            hidden_nodes=hidden_nodes, \n",
    "            learning_rate=learning_rate, \n",
    "            num_epochs=num_epochs, \n",
    "            batch_size=batch_size, \n",
    "            weight_decay=weight_decay)\n",
    "        test_loss = model.train(X_train, Y_train, X_test, Y_test)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    return sum(test_losses) / len(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test Loss: 0.2547\n",
      "Fold 2 Test Loss: 0.5839\n",
      "Fold 3 Test Loss: 0.3486\n",
      "Fold 4 Test Loss: 0.2622\n",
      "Fold 5 Test Loss: 0.3354\n",
      "Average Test Loss: 0.3570\n"
     ]
    }
   ],
   "source": [
    "# Optuna Study 설정\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=20, interval_steps=5) # 학습 시 one epoch 내에서 20번은 그냥 학습하고, 이후에 pruning 시작 -> 5스텝마다 pruning 결정\n",
    "sampler = GPSampler() # gaussian process sampler, hyperparameter를 결정하는 surrogate model \n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler=sampler)\n",
    "\n",
    "# 최대 60번의 trial을 수행\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
