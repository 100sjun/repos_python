{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    물리적 법칙을 반영하는 레이어\n",
    "    몰 변화량 예측값을 받아 물질수지와 부피 변화를 계산\n",
    "    \"\"\"\n",
    "    def __init__(self, time_step=0.1):\n",
    "        super(PhysicsLayer, self).__init__()\n",
    "        self.time_step = time_step  # 시간 단위 [hour]\n",
    "\n",
    "    def forward(self, mol_changes, features):\n",
    "        # 현재 시간 단계 상태 추출\n",
    "        current_temp = features[..., 0:1]\n",
    "        current_voltage = features[..., 1:2]\n",
    "        current_electrolyte = features[..., 2:3]\n",
    "        \n",
    "        # 현재 부피 추출\n",
    "        current_feed_volume = features[..., 5:6]  # L\n",
    "        current_acid_volume = features[..., 6:7]  # L\n",
    "        current_base_volume = features[..., 7:8]  # L\n",
    "        \n",
    "        # 현재 농도 추출 (mol/L)\n",
    "        current_feed_la = features[..., 8:9]  \n",
    "        current_feed_k = features[..., 9:10]\n",
    "        current_acid_la = features[..., 10:11]\n",
    "        # current_acid_k는 항상 0 (Acid에는 K+가 들어갈 수 없음)\n",
    "        # current_base_la는 항상 0 (Base에는 LA가 들어갈 수 없음)\n",
    "        current_base_k = features[..., 11:12]\n",
    "        \n",
    "        # 몰 변화량 추출 (mol/h)\n",
    "        la_mol_change = mol_changes[..., 0:1]  # Feed->Acid 방향 LA 몰 변화량\n",
    "        k_mol_change = mol_changes[..., 1:2]   # Feed->Base 방향 K+ 몰 변화량\n",
    "        water_acid_change = mol_changes[..., 2:3]  # Feed->Acid 방향 물 부피 변화량 (L/h)\n",
    "        water_base_change = mol_changes[..., 3:4]  # Feed->Base 방향 물 부피 변화량 (L/h)\n",
    "        \n",
    "        # 한 시간 단계 동안의 물질 전달량 계산 (mol)\n",
    "        la_transfer = la_mol_change * self.time_step\n",
    "        k_transfer = k_mol_change * self.time_step\n",
    "        \n",
    "        # 한 시간 단계 동안의 물 전달량 계산 (L)\n",
    "        water_to_acid = water_acid_change * self.time_step\n",
    "        water_to_base = water_base_change * self.time_step\n",
    "        \n",
    "        # 부피 업데이트\n",
    "        new_feed_volume = current_feed_volume - water_to_acid - water_to_base\n",
    "        new_acid_volume = current_acid_volume + water_to_acid\n",
    "        new_base_volume = current_base_volume + water_to_base\n",
    "        \n",
    "        # 몰수 계산 및 업데이트\n",
    "        feed_la_mol = current_feed_la * current_feed_volume\n",
    "        feed_k_mol = current_feed_k * current_feed_volume\n",
    "        acid_la_mol = current_acid_la * current_acid_volume\n",
    "        acid_k_mol = torch.zeros_like(acid_la_mol)  # Acid에는 K+가 없음\n",
    "        base_la_mol = torch.zeros_like(current_base_k)  # Base에는 LA가 없음\n",
    "        base_k_mol = current_base_k * current_base_volume\n",
    "        \n",
    "        # 몰수 전달 적용\n",
    "        new_feed_la_mol = feed_la_mol - la_transfer\n",
    "        new_feed_k_mol = feed_k_mol - k_transfer\n",
    "        new_acid_la_mol = acid_la_mol + la_transfer\n",
    "        new_acid_k_mol = torch.zeros_like(acid_la_mol)  # 항상 0\n",
    "        new_base_la_mol = torch.zeros_like(current_base_k)  # 항상 0\n",
    "        new_base_k_mol = base_k_mol + k_transfer\n",
    "        \n",
    "        # 새로운 농도 계산 (0으로 나누기 방지)\n",
    "        eps = 1e-6  # 작은 값 추가하여 0으로 나누기 방지\n",
    "        new_feed_la_conc = new_feed_la_mol / (new_feed_volume + eps)\n",
    "        new_feed_k_conc = new_feed_k_mol / (new_feed_volume + eps)\n",
    "        new_acid_la_conc = new_acid_la_mol / (new_acid_volume + eps)\n",
    "        new_acid_k_conc = torch.zeros_like(new_acid_la_conc)  # 항상 0\n",
    "        new_base_la_conc = torch.zeros_like(new_base_k_mol)  # 항상 0\n",
    "        new_base_k_conc = new_base_k_mol / (new_base_volume + eps)\n",
    "        \n",
    "        # 결과 출력: 새로운 농도, 새로운 부피\n",
    "        new_states = torch.cat([\n",
    "            new_feed_la_conc, new_feed_k_conc,\n",
    "            new_acid_la_conc, new_acid_k_conc,\n",
    "            new_base_la_conc, new_base_k_conc,\n",
    "            new_feed_volume, new_acid_volume, new_base_volume\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MembraneSystemModel(nn.Module):\n",
    "    \"\"\"\n",
    "    멤브레인 시스템 모델링을 위한 Physics-Informed LSTM 모델\n",
    "    \"\"\"\n",
    "    def __init__(self, lstm_units=64, mlp_hidden_units=[128, 64], time_step=0.1, sequence_length=3):\n",
    "        super(MembraneSystemModel, self).__init__()\n",
    "        self.lstm_units = lstm_units\n",
    "        self.mlp_hidden_units = mlp_hidden_units\n",
    "        self.time_step = time_step\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # LSTM 층\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=12,  # [온도, 전압, 전해질, Ci, Ki, VF, VA, VB, CF_LA, CF_K, CA_LA, CB_K]\n",
    "            hidden_size=self.lstm_units,\n",
    "            batch_first=True,  # (batch, seq, feature) 형태 입력\n",
    "            num_layers=1\n",
    "        )\n",
    "        \n",
    "        # 몰 변화량 예측을 위한 다층 퍼셉트론 정의\n",
    "        mlp_layers = []\n",
    "        input_size = self.lstm_units\n",
    "        \n",
    "        # MLP 히든 레이어 구성\n",
    "        for hidden_size in self.mlp_hidden_units:\n",
    "            mlp_layers.append(nn.Linear(input_size, hidden_size))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        # 출력 레이어 (4개 값: LA몰 변화량, K+몰 변화량, 물->Acid 부피 변화량, 물->Base 부피 변화량)\n",
    "        mlp_layers.append(nn.Linear(input_size, 4))\n",
    "        \n",
    "        # MLP 레이어를 Sequential 컨테이너로 구성\n",
    "        self.mol_change_mlp = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        # 물리 법칙 층\n",
    "        self.physics_layer = PhysicsLayer(time_step=self.time_step)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM 처리\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 몰 변화량 예측 - 각 시간 단계별로 동일한 MLP 적용\n",
    "        batch_size, seq_len, _ = lstm_out.size()\n",
    "        mol_changes_list = []\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            # 각 시간 단계의 LSTM 출력을 MLP에 입력\n",
    "            step_lstm_out = lstm_out[:, i, :]\n",
    "            step_mol_changes = self.mol_change_mlp(step_lstm_out)\n",
    "            mol_changes_list.append(step_mol_changes.unsqueeze(1))\n",
    "        \n",
    "        # 모든 시간 단계의 몰 변화량 예측 결과 결합\n",
    "        mol_changes = torch.cat(mol_changes_list, dim=1)\n",
    "        \n",
    "        # 물리 법칙 적용 - 모든 시간 단계를 처리하기 위한 루프\n",
    "        new_states_list = []\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            new_state = self.physics_layer(mol_changes[:, i, :], x[:, i, :])\n",
    "            new_states_list.append(new_state.unsqueeze(1))\n",
    "        \n",
    "        # 모든 시간 단계의 결과를 결합\n",
    "        new_states = torch.cat(new_states_list, dim=1)\n",
    "        \n",
    "        return mol_changes, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDDataset(Dataset):\n",
    "    \"\"\"\n",
    "    BMED CSV 데이터를 위한 PyTorch 데이터셋\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, sequence_length=1, train=True, train_ratio=0.8):\n",
    "        # CSV 파일 로드\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # 실험 번호별로 데이터 분리\n",
    "        self.experiment_data = {}\n",
    "        for exp in self.df['exp'].unique():\n",
    "            self.experiment_data[exp] = self.df[self.df['exp'] == exp].sort_values('t')\n",
    "        \n",
    "        # 훈련/테스트 셋 분리 (실험 번호 기준)\n",
    "        all_exps = list(self.experiment_data.keys())\n",
    "        # 랜덤 시드 고정으로 동일한 분할 보장\n",
    "        np.random.seed(42)  \n",
    "        np.random.shuffle(all_exps)\n",
    "        np.random.seed(None)  # 랜덤 시드 초기화\n",
    "        \n",
    "        split_idx = int(len(all_exps) * train_ratio)\n",
    "        \n",
    "        if train:\n",
    "            self.exps_to_use = all_exps[:split_idx]\n",
    "        else:\n",
    "            self.exps_to_use = all_exps[split_idx:]\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.train = train\n",
    "        \n",
    "        # 데이터 준비\n",
    "        self.prepare_data()\n",
    "        \n",
    "        # 실험별 시작 인덱스 저장 - 배치 구성 시 사용\n",
    "        self.exp_indices = []\n",
    "        current_idx = 0\n",
    "        self.exp_id_to_indices = {}\n",
    "        \n",
    "        for exp_id in self.exps_to_use:\n",
    "            exp_data = self.experiment_data[exp_id]\n",
    "            \n",
    "            if len(exp_data) <= self.sequence_length:\n",
    "                continue\n",
    "                \n",
    "            n_samples = len(exp_data) - self.sequence_length\n",
    "            indices = list(range(current_idx, current_idx + n_samples))\n",
    "            self.exp_id_to_indices[exp_id] = indices\n",
    "            self.exp_indices.append((exp_id, indices))\n",
    "            current_idx += n_samples\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # 특성 및 목표 변수 데이터\n",
    "        features_list = []\n",
    "        mol_change_targets_list = []\n",
    "        state_targets_list = []\n",
    "        \n",
    "        for exp in self.exps_to_use:\n",
    "            exp_data = self.experiment_data[exp]\n",
    "            \n",
    "            # 시간 간격이 일정하지 않을 수 있으므로 시간 순으로 정렬된 데이터 사용\n",
    "            if len(exp_data) <= self.sequence_length:\n",
    "                continue\n",
    "                \n",
    "            for i in range(len(exp_data) - self.sequence_length):\n",
    "                # 시퀀스 길이만큼의 입력 데이터\n",
    "                seq_data = exp_data.iloc[i:i+self.sequence_length]\n",
    "                \n",
    "                # 특성 벡터 구성\n",
    "                features = []\n",
    "                for _, row in seq_data.iterrows():\n",
    "                    # [온도, 전압, 전해질, 초기LA농도, 초기K+농도, \n",
    "                    #  Feed부피, Acid부피, Base부피, \n",
    "                    #  Feed LA농도, Feed K+농도, Acid LA농도, Base K+농도]\n",
    "                    feature = [\n",
    "                        row['T'], row['V'], row['E'], \n",
    "                        row['Ci'], row['Ki'],\n",
    "                        row['VF'], row['VA'], row['VB'],\n",
    "                        row['CF_LA'], row['CF_K'], row['CA_LA'], row['CB_K']\n",
    "                    ]\n",
    "                    features.append(feature)\n",
    "                \n",
    "                # 시퀀스 내 각 시간 단계에 대한 목표 변수 준비\n",
    "                # (각 시간 단계의 t+1 데이터가 필요함)\n",
    "                mol_change_targets = []\n",
    "                state_targets = []\n",
    "                \n",
    "                for j in range(self.sequence_length-1):\n",
    "                    current = seq_data.iloc[j]  # 현재 시간 단계\n",
    "                    next_row = seq_data.iloc[j+1]  # 다음 시간 단계\n",
    "                    \n",
    "                    # 시간 간격\n",
    "                    dt = next_row['t'] - current['t']\n",
    "                    \n",
    "                    # LA 및 K+ 몰수 계산 (농도 * 부피)\n",
    "                    current_la_feed = current['CF_LA'] * current['VF']\n",
    "                    current_la_acid = current['CA_LA'] * current['VA']\n",
    "                    current_k_feed = current['CF_K'] * current['VF']\n",
    "                    current_k_base = current['CB_K'] * current['VB']\n",
    "                    \n",
    "                    next_la_feed = next_row['CF_LA'] * next_row['VF']\n",
    "                    next_la_acid = next_row['CA_LA'] * next_row['VA']\n",
    "                    next_k_feed = next_row['CF_K'] * next_row['VF']\n",
    "                    next_k_base = next_row['CB_K'] * next_row['VB']\n",
    "                    \n",
    "                    # 몰 변화량 계산 (mol/h)\n",
    "                    la_mol_change = (next_la_acid - current_la_acid) / dt\n",
    "                    k_mol_change = (next_k_base - current_k_base) / dt\n",
    "                    \n",
    "                    # 부피 변화량 계산 (L/h)\n",
    "                    water_acid_change = (next_row['VA'] - current['VA']) / dt\n",
    "                    water_base_change = (next_row['VB'] - current['VB']) / dt\n",
    "                    \n",
    "                    mol_change = [la_mol_change, k_mol_change, water_acid_change, water_base_change]\n",
    "                    mol_change_targets.append(mol_change)\n",
    "                    \n",
    "                    # 상태 변수: 농도와 부피\n",
    "                    state = [\n",
    "                        next_row['CF_LA'], next_row['CF_K'],\n",
    "                        next_row['CA_LA'], 0,  # Acid에는 K+가 없음\n",
    "                        0, next_row['CB_K'],  # Base에는 LA가 없음\n",
    "                        next_row['VF'], next_row['VA'], next_row['VB']\n",
    "                    ]\n",
    "                    state_targets.append(state)\n",
    "                \n",
    "                # 마지막 시간 단계와 그 다음 데이터\n",
    "                current = seq_data.iloc[-1]  # 시퀀스의 마지막 시간 단계\n",
    "                if i + self.sequence_length < len(exp_data):\n",
    "                    next_row = exp_data.iloc[i + self.sequence_length]  # 다음 시간 단계\n",
    "                    \n",
    "                    # 시간 간격\n",
    "                    dt = next_row['t'] - current['t']\n",
    "                    \n",
    "                    # LA 및 K+ 몰수 계산 (농도 * 부피)\n",
    "                    current_la_feed = current['CF_LA'] * current['VF']\n",
    "                    current_la_acid = current['CA_LA'] * current['VA']\n",
    "                    current_k_feed = current['CF_K'] * current['VF']\n",
    "                    current_k_base = current['CB_K'] * current['VB']\n",
    "                    \n",
    "                    next_la_feed = next_row['CF_LA'] * next_row['VF']\n",
    "                    next_la_acid = next_row['CA_LA'] * next_row['VA']\n",
    "                    next_k_feed = next_row['CF_K'] * next_row['VF']\n",
    "                    next_k_base = next_row['CB_K'] * next_row['VB']\n",
    "                    \n",
    "                    # 몰 변화량 계산 (mol/h)\n",
    "                    la_mol_change = (next_la_acid - current_la_acid) / dt\n",
    "                    k_mol_change = (next_k_base - current_k_base) / dt\n",
    "                    \n",
    "                    # 부피 변화량 계산 (L/h)\n",
    "                    water_acid_change = (next_row['VA'] - current['VA']) / dt\n",
    "                    water_base_change = (next_row['VB'] - current['VB']) / dt\n",
    "                    \n",
    "                    mol_change = [la_mol_change, k_mol_change, water_acid_change, water_base_change]\n",
    "                    mol_change_targets.append(mol_change)\n",
    "                    \n",
    "                    # 상태 변수: 농도와 부피\n",
    "                    state = [\n",
    "                        next_row['CF_LA'], next_row['CF_K'],\n",
    "                        next_row['CA_LA'], 0,  # Acid에는 K+가 없음\n",
    "                        0, next_row['CB_K'],  # Base에는 LA가 없음\n",
    "                        next_row['VF'], next_row['VA'], next_row['VB']\n",
    "                    ]\n",
    "                    state_targets.append(state)\n",
    "                else:\n",
    "                    # 시퀀스의 마지막 데이터가 데이터셋의 마지막이면, 이전 단계 변화량을 그대로 사용\n",
    "                    if len(mol_change_targets) > 0:\n",
    "                        mol_change_targets.append(mol_change_targets[-1])\n",
    "                        state_targets.append(state_targets[-1])\n",
    "                    else:\n",
    "                        # 시퀀스가 길이 1인 경우 처리\n",
    "                        mol_change_targets.append([0, 0, 0, 0])\n",
    "                        state = [\n",
    "                            current['CF_LA'], current['CF_K'],\n",
    "                            current['CA_LA'], 0,\n",
    "                            0, current['CB_K'],\n",
    "                            current['VF'], current['VA'], current['VB']\n",
    "                        ]\n",
    "                        state_targets.append(state)\n",
    "                \n",
    "                features_list.append(features)\n",
    "                mol_change_targets_list.append(mol_change_targets)\n",
    "                state_targets_list.append(state_targets)\n",
    "        \n",
    "        self.features = np.array(features_list, dtype=np.float32)\n",
    "        self.mol_change_targets = np.array(mol_change_targets_list, dtype=np.float32)\n",
    "        self.state_targets = np.array(state_targets_list, dtype=np.float32)\n",
    "        \n",
    "        print(f\"Data loaded: {len(self.features)} sequences, feature shape: {self.features.shape}\")\n",
    "        print(f\"Mol change targets shape: {self.mol_change_targets.shape}\")\n",
    "        print(f\"State targets shape: {self.state_targets.shape}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.features[idx]),\n",
    "            torch.FloatTensor(self.mol_change_targets[idx]),\n",
    "            torch.FloatTensor(self.state_targets[idx])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MembraneSystemTrainer:\n",
    "    \"\"\"\n",
    "    멤브레인 시스템 모델 훈련 및 시뮬레이션을 위한 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        # 몰 변화량과 상태 예측을 위한 손실 함수\n",
    "        self.mol_change_criterion = nn.MSELoss()\n",
    "        self.state_criterion = nn.MSELoss()\n",
    "    \n",
    "    def train(self, train_loader, val_loader=None, epochs=100, mol_change_weight=1.0, state_weight=1.0):\n",
    "        \"\"\"\n",
    "        모델 훈련\n",
    "        \n",
    "        입력:\n",
    "        - train_loader: 훈련 데이터 로더\n",
    "        - val_loader: 검증 데이터 로더 (옵션)\n",
    "        - epochs: 훈련 에폭 수\n",
    "        - mol_change_weight: 몰 변화량 손실 가중치\n",
    "        - state_weight: 상태 손실 가중치\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (features, mol_change_targets, state_targets) in enumerate(train_loader):\n",
    "                features = features.to(self.device)\n",
    "                mol_change_targets = mol_change_targets.to(self.device)\n",
    "                state_targets = state_targets.to(self.device)\n",
    "                \n",
    "                # 그래디언트 초기화\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # 순전파\n",
    "                mol_change_predictions, state_predictions = self.model(features)\n",
    "                \n",
    "                # 손실 계산 - 모든 시간 단계에 대한 손실\n",
    "                mol_change_loss = self.mol_change_criterion(mol_change_predictions, mol_change_targets)\n",
    "                state_loss = self.state_criterion(state_predictions, state_targets)\n",
    "                total_loss = mol_change_weight * mol_change_loss + state_weight * state_loss\n",
    "                \n",
    "                # 역전파 및 최적화\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += total_loss.item()\n",
    "            \n",
    "            # 에폭 평균 손실\n",
    "            avg_train_loss = epoch_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # 검증 손실 계산 (있는 경우)\n",
    "            if val_loader is not None:\n",
    "                self.model.eval()\n",
    "                val_loss = 0.0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for features, mol_change_targets, state_targets in val_loader:\n",
    "                        features = features.to(self.device)\n",
    "                        mol_change_targets = mol_change_targets.to(self.device)\n",
    "                        state_targets = state_targets.to(self.device)\n",
    "                        \n",
    "                        mol_change_predictions, state_predictions = self.model(features)\n",
    "                        \n",
    "                        mol_change_loss = self.mol_change_criterion(mol_change_predictions, mol_change_targets)\n",
    "                        state_loss = self.state_criterion(state_predictions, state_targets)\n",
    "                        total_loss = mol_change_weight * mol_change_loss + state_weight * state_loss\n",
    "                        \n",
    "                        val_loss += total_loss.item()\n",
    "                \n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                \n",
    "                print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')\n",
    "                \n",
    "                self.model.train()\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}')\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def evaluate_experiment(self, exp_data, initial_conditions, operation_params):\n",
    "        \"\"\"\n",
    "        특정 실험 데이터와 모델 예측 결과를 비교 평가\n",
    "        \n",
    "        입력:\n",
    "        - exp_data: 단일 실험 데이터프레임\n",
    "        - initial_conditions: 초기 조건\n",
    "        - operation_params: 운전 조건\n",
    "        \n",
    "        출력:\n",
    "        - 실제 데이터와 모델 예측의 비교 결과\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 실험 시간 포인트\n",
    "        time_points = exp_data['t'].values\n",
    "        max_time = time_points[-1]\n",
    "        \n",
    "        # 0.1시간 간격으로 시뮬레이션\n",
    "        sim_time_points = np.arange(0, max_time + self.model.time_step, self.model.time_step)\n",
    "        num_steps = len(sim_time_points) - 1\n",
    "        \n",
    "        # 시뮬레이션 실행\n",
    "        sim_results = self.simulate(initial_conditions, operation_params, num_steps)\n",
    "        \n",
    "        # 실제 데이터에 가장 가까운 시뮬레이션 시간점 찾기\n",
    "        comparison = {}\n",
    "        for i, t in enumerate(time_points):\n",
    "            # 가장 가까운 시뮬레이션 시간 인덱스 찾기\n",
    "            closest_idx = np.argmin(np.abs(sim_time_points - t))\n",
    "            \n",
    "            # 실제 값과 예측 값 비교\n",
    "            comparison[t] = {\n",
    "                'actual': {\n",
    "                    'CF_LA': exp_data.iloc[i]['CF_LA'],\n",
    "                    'CA_LA': exp_data.iloc[i]['CA_LA'],\n",
    "                    'CF_K': exp_data.iloc[i]['CF_K'],\n",
    "                    'CB_K': exp_data.iloc[i]['CB_K'],\n",
    "                    'VF': exp_data.iloc[i]['VF'],\n",
    "                    'VA': exp_data.iloc[i]['VA'],\n",
    "                    'VB': exp_data.iloc[i]['VB']\n",
    "                },\n",
    "                'predicted': {\n",
    "                    'CF_LA': sim_results['feed_la'][closest_idx],\n",
    "                    'CA_LA': sim_results['acid_la'][closest_idx],\n",
    "                    'CF_K': sim_results['feed_k'][closest_idx],\n",
    "                    'CB_K': sim_results['base_k'][closest_idx],\n",
    "                    'VF': sim_results['feed_volume'][closest_idx],\n",
    "                    'VA': sim_results['acid_volume'][closest_idx],\n",
    "                    'VB': sim_results['base_volume'][closest_idx]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 백분율 오차 계산\n",
    "            error = {}\n",
    "            for key in comparison[t]['actual'].keys():\n",
    "                actual = comparison[t]['actual'][key]\n",
    "                predicted = comparison[t]['predicted'][key]\n",
    "                if actual != 0:\n",
    "                    error[key] = abs((predicted - actual) / actual) * 100\n",
    "                else:\n",
    "                    error[key] = float('inf') if predicted != 0 else 0\n",
    "            \n",
    "            comparison[t]['error'] = error\n",
    "        \n",
    "        return {\n",
    "            'comparison': comparison,\n",
    "            'simulation': sim_results,\n",
    "            'exp_time_points': time_points,\n",
    "            'sim_time_points': sim_time_points\n",
    "        }\n",
    "    \n",
    "    def simulate(self, initial_conditions, operation_params, num_steps):\n",
    "        \"\"\"\n",
    "        시스템 시뮬레이션 실행\n",
    "        \n",
    "        입력:\n",
    "        - initial_conditions: 초기 농도 및 부피 조건 (딕셔너리)\n",
    "        - operation_params: 운전 조건 (온도, 전압, 전해질 농도) (딕셔너리)\n",
    "        - num_steps: 시뮬레이션 단계 수\n",
    "        \n",
    "        출력:\n",
    "        - 시뮬레이션 결과 (딕셔너리)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 초기 조건 설정\n",
    "        feed_la_init = initial_conditions['feed_la']\n",
    "        feed_k_init = initial_conditions['feed_k']\n",
    "        acid_la_init = initial_conditions.get('acid_la', 0)\n",
    "        acid_k_init = 0  # Acid에는 K+가 없음\n",
    "        base_la_init = 0  # Base에는 LA가 없음\n",
    "        base_k_init = initial_conditions.get('base_k', 0)\n",
    "        \n",
    "        feed_vol_init = initial_conditions['feed_volume']\n",
    "        acid_vol_init = initial_conditions['acid_volume']\n",
    "        base_vol_init = initial_conditions['base_volume']\n",
    "        \n",
    "        # 운전 조건\n",
    "        temperature = operation_params['temperature']\n",
    "        voltage = operation_params['voltage']\n",
    "        electrolyte = operation_params['electrolyte']\n",
    "        ci = operation_params['ci']  # 초기 LA 농도\n",
    "        ki = operation_params['ki']  # 초기 K+ 농도\n",
    "        \n",
    "        # 결과 저장 배열\n",
    "        results = {\n",
    "            'time': np.arange(num_steps + 1) * self.model.time_step,\n",
    "            'feed_la': np.zeros(num_steps + 1),\n",
    "            'feed_k': np.zeros(num_steps + 1),\n",
    "            'acid_la': np.zeros(num_steps + 1),\n",
    "            'base_k': np.zeros(num_steps + 1),\n",
    "            'feed_volume': np.zeros(num_steps + 1),\n",
    "            'acid_volume': np.zeros(num_steps + 1),\n",
    "            'base_volume': np.zeros(num_steps + 1),\n",
    "            'la_mol_change': np.zeros(num_steps),\n",
    "            'k_mol_change': np.zeros(num_steps),\n",
    "            'water_acid_change': np.zeros(num_steps),\n",
    "            'water_base_change': np.zeros(num_steps)\n",
    "        }\n",
    "        \n",
    "        # 초기 값 설정\n",
    "        results['feed_la'][0] = feed_la_init\n",
    "        results['feed_k'][0] = feed_k_init\n",
    "        results['acid_la'][0] = acid_la_init\n",
    "        results['base_k'][0] = base_k_init\n",
    "        results['feed_volume'][0] = feed_vol_init\n",
    "        results['acid_volume'][0] = acid_vol_init\n",
    "        results['base_volume'][0] = base_vol_init\n",
    "        \n",
    "        # 시계열 입력을 위한 버퍼 초기화 (sequence_length 크기의 슬라이딩 윈도우)\n",
    "        sequence_buffer = []\n",
    "        for _ in range(self.model.sequence_length):\n",
    "            sequence_buffer.append([\n",
    "                temperature, voltage, electrolyte, ci, ki,\n",
    "                feed_vol_init, acid_vol_init, base_vol_init,\n",
    "                feed_la_init, feed_k_init, acid_la_init, base_k_init\n",
    "            ])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 시뮬레이션 실행\n",
    "            for i in range(num_steps):\n",
    "                # NumPy 배열을 PyTorch 텐서로 변환\n",
    "                input_tensor = torch.FloatTensor([sequence_buffer]).to(self.device)  # [batch_size=1, seq_len, features]\n",
    "                \n",
    "                # 예측 실행\n",
    "                mol_change_pred, state_pred = self.model(input_tensor)\n",
    "                \n",
    "                # 마지막 시간 단계에 대한 예측만 사용\n",
    "                mol_change_np = mol_change_pred.squeeze()[-1].cpu().numpy() \n",
    "                state_np = state_pred.squeeze()[-1].cpu().numpy()\n",
    "                \n",
    "                # 결과 저장\n",
    "                results['la_mol_change'][i] = mol_change_np[0]\n",
    "                results['k_mol_change'][i] = mol_change_np[1]\n",
    "                results['water_acid_change'][i] = mol_change_np[2]\n",
    "                results['water_base_change'][i] = mol_change_np[3]\n",
    "                \n",
    "                # 다음 시간 단계의 상태 저장\n",
    "                results['feed_la'][i+1] = state_np[0]\n",
    "                results['feed_k'][i+1] = state_np[1]\n",
    "                results['acid_la'][i+1] = state_np[2]\n",
    "                # Acid K+ 및 Base LA는 항상 0\n",
    "                results['base_k'][i+1] = state_np[5]\n",
    "                results['feed_volume'][i+1] = state_np[6]\n",
    "                results['acid_volume'][i+1] = state_np[7]\n",
    "                results['base_volume'][i+1] = state_np[8]\n",
    "                \n",
    "                # 시퀀스 버퍼 업데이트 (가장 오래된 항목 제거, 새 항목 추가 - 슬라이딩 윈도우)\n",
    "                sequence_buffer.pop(0)\n",
    "                sequence_buffer.append([\n",
    "                    temperature, voltage, electrolyte, ci, ki,\n",
    "                    results['feed_volume'][i+1], results['acid_volume'][i+1], results['base_volume'][i+1],\n",
    "                    results['feed_la'][i+1], results['feed_k'][i+1],\n",
    "                    results['acid_la'][i+1], results['base_k'][i+1]\n",
    "                ])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_comparison(self, eval_results, exp_number):\n",
    "        \"\"\"\n",
    "        실험 데이터와 모델 예측 결과 비교 시각화\n",
    "        \"\"\"\n",
    "        comparison = eval_results['comparison']\n",
    "        simulation = eval_results['simulation']\n",
    "        exp_time_points = eval_results['exp_time_points']\n",
    "        sim_time_points = eval_results['sim_time_points']\n",
    "        \n",
    "        # 결과 추출\n",
    "        actual_times = list(comparison.keys())\n",
    "        actual_cf_la = [comparison[t]['actual']['CF_LA'] for t in actual_times]\n",
    "        actual_ca_la = [comparison[t]['actual']['CA_LA'] for t in actual_times]\n",
    "        actual_cf_k = [comparison[t]['actual']['CF_K'] for t in actual_times]\n",
    "        actual_cb_k = [comparison[t]['actual']['CB_K'] for t in actual_times]\n",
    "        actual_vf = [comparison[t]['actual']['VF'] for t in actual_times]\n",
    "        actual_va = [comparison[t]['actual']['VA'] for t in actual_times]\n",
    "        actual_vb = [comparison[t]['actual']['VB'] for t in actual_times]\n",
    "        \n",
    "        # 그래프 그리기\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        # LA 농도 그래프\n",
    "        plt.subplot(3, 2, 1)\n",
    "        plt.plot(sim_time_points, simulation['feed_la'], 'b-', label='Model Feed LA')\n",
    "        plt.plot(sim_time_points, simulation['acid_la'], 'r-', label='Model Acid LA')\n",
    "        plt.scatter(actual_times, actual_cf_la, marker='o', color='blue', label='Actual Feed LA')\n",
    "        plt.scatter(actual_times, actual_ca_la, marker='o', color='red', label='Actual Acid LA')\n",
    "        plt.xlabel('Time (hr)')\n",
    "        plt.ylabel('LA Concentration (mol/L)')\n",
    "        plt.title(f'Exp {exp_number}: LA Concentration')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # K+ 농도 그래프\n",
    "        plt.subplot(3, 2, 2)\n",
    "        plt.plot(sim_time_points, simulation['feed_k'], 'b-', label='Model Feed K+')\n",
    "        plt.plot(sim_time_points, simulation['base_k'], 'g-', label='Model Base K+')\n",
    "        plt.scatter(actual_times, actual_cf_k, marker='o', color='blue', label='Actual Feed K+')\n",
    "        plt.scatter(actual_times, actual_cb_k, marker='o', color='green', label='Actual Base K+')\n",
    "        plt.xlabel('Time (hr)')\n",
    "        plt.ylabel('K+ Concentration (mol/L)')\n",
    "        plt.title(f'Exp {exp_number}: K+ Concentration')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 부피 그래프\n",
    "        plt.subplot(3, 2, 3)\n",
    "        plt.plot(sim_time_points, simulation['feed_volume'], 'b-', label='Model Feed Volume')\n",
    "        plt.plot(sim_time_points, simulation['acid_volume'], 'r-', label='Model Acid Volume')\n",
    "        plt.plot(sim_time_points, simulation['base_volume'], 'g-', label='Model Base Volume')\n",
    "        plt.scatter(actual_times, actual_vf, marker='o', color='blue', label='Actual Feed Volume')\n",
    "        plt.scatter(actual_times, actual_va, marker='o', color='red', label='Actual Acid Volume')\n",
    "        plt.scatter(actual_times, actual_vb, marker='o', color='green', label='Actual Base Volume')\n",
    "        plt.xlabel('Time (hr)')\n",
    "        plt.ylabel('Volume (L)')\n",
    "        plt.title(f'Exp {exp_number}: Channel Volumes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 몰 변화량 그래프\n",
    "        plt.subplot(3, 2, 4)\n",
    "        plt.plot(sim_time_points[:-1], simulation['la_mol_change'], 'r-', label='LA Mol Change')\n",
    "        plt.plot(sim_time_points[:-1], simulation['k_mol_change'], 'g-', label='K+ Mol Change')\n",
    "        plt.xlabel('Time (hr)')\n",
    "        plt.ylabel('Mol Change Rate (mol/hr)')\n",
    "        plt.title(f'Exp {exp_number}: Substance Transfer Rates')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 물 변화량 그래프\n",
    "        plt.subplot(3, 2, 5)\n",
    "        plt.plot(sim_time_points[:-1], simulation['water_acid_change'], 'r-', label='Water to Acid')\n",
    "        plt.plot(sim_time_points[:-1], simulation['water_base_change'], 'g-', label='Water to Base')\n",
    "        plt.xlabel('Time (hr)')\n",
    "        plt.ylabel('Volume Change Rate (L/hr)')\n",
    "        plt.title(f'Exp {exp_number}: Water Transfer Rates')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 오차율 그래프\n",
    "        plt.subplot(3, 2, 6)\n",
    "        err_times = list(comparison.keys())\n",
    "        err_cf_la = [comparison[t]['error']['CF_LA'] if 'CF_LA' in comparison[t]['error'] and comparison[t]['error']['CF_LA'] != float('inf') else 0 for t in err_times]\n",
    "        err_ca_la = [comparison[t]['error']['CA_LA'] if 'CA_LA' in comparison[t]['error'] and comparison[t]['error']['CA_LA'] != float('inf') else 0 for t in err_times]\n",
    "        err_cf_k = [comparison[t]['error']['CF_K'] if 'CF_K' in comparison[t]['error'] and comparison[t]['error']['CF_K'] != float('inf') else 0 for t in err_times]\n",
    "        err_cb_k = [comparison[t]['error']['CB_K'] if 'CB_K' in comparison[t]['error'] and comparison[t]['error']['CB_K'] != float('inf') else 0 for t in err_times]\n",
    "        \n",
    "        plt.bar(np.array(range(len(err_times))) - 0.3, err_cf_la, width=0.15, label='Feed LA Error', color='blue')\n",
    "        plt.bar(np.array(range(len(err_times))) - 0.15, err_ca_la, width=0.15, label='Acid LA Error', color='red')\n",
    "        plt.bar(np.array(range(len(err_times))) + 0, err_cf_k, width=0.15, label='Feed K+ Error', color='cyan')\n",
    "        plt.bar(np.array(range(len(err_times))) + 0.15, err_cb_k, width=0.15, label='Base K+ Error', color='green')\n",
    "        plt.xticks(range(len(err_times)), [f'{t:.1f}' for t in err_times])\n",
    "        plt.xlabel('Time (hr)')\n",
    "        plt.ylabel('Error (%)')\n",
    "        plt.title(f'Exp {exp_number}: Prediction Error')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt\n",
    "        \n",
    "    def calculate_r2_scores(self, csv_path):\n",
    "        \"\"\"\n",
    "        전체 데이터에 대한 R² 스코어를 계산하는 메서드\n",
    "        \n",
    "        입력:\n",
    "        - csv_path: CSV 파일 경로\n",
    "        \n",
    "        출력:\n",
    "        - 각 변수별 R² 스코어와 평균 R² 스코어\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import r2_score\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        all_actual_values = {\n",
    "            'CF_LA': [],\n",
    "            'CA_LA': [],\n",
    "            'CF_K': [],\n",
    "            'CB_K': [],\n",
    "            'VF': [],\n",
    "            'VA': [],\n",
    "            'VB': []\n",
    "        }\n",
    "        all_predicted_values = {\n",
    "            'CF_LA': [],\n",
    "            'CA_LA': [],\n",
    "            'CF_K': [],\n",
    "            'CB_K': [],\n",
    "            'VF': [],\n",
    "            'VA': [],\n",
    "            'VB': []\n",
    "        }\n",
    "        \n",
    "        # 각 실험별로 평가\n",
    "        for exp in df['exp'].unique():\n",
    "            exp_data = df[df['exp'] == exp].sort_values('t')\n",
    "            \n",
    "            # 첫 번째 행 데이터로 초기 조건 설정\n",
    "            first_row = exp_data.iloc[0]\n",
    "            \n",
    "            initial_conditions = {\n",
    "                'feed_la': first_row['CF_LA'],\n",
    "                'feed_k': first_row['CF_K'],\n",
    "                'acid_la': first_row['CA_LA'],\n",
    "                'base_k': first_row['CB_K'],\n",
    "                'feed_volume': first_row['VF'],\n",
    "                'acid_volume': first_row['VA'],\n",
    "                'base_volume': first_row['VB']\n",
    "            }\n",
    "            \n",
    "            operation_params = {\n",
    "                'temperature': first_row['T'],\n",
    "                'voltage': first_row['V'],\n",
    "                'electrolyte': first_row['E'],\n",
    "                'ci': first_row['Ci'],\n",
    "                'ki': first_row['Ki']\n",
    "            }\n",
    "            \n",
    "            # 실험 평가\n",
    "            eval_result = self.evaluate_experiment(exp_data, initial_conditions, operation_params)\n",
    "            comparison = eval_result['comparison']\n",
    "            \n",
    "            # 실제 값과 예측 값 수집\n",
    "            for t in comparison:\n",
    "                for key in all_actual_values:\n",
    "                    if key in comparison[t]['actual']:\n",
    "                        all_actual_values[key].append(comparison[t]['actual'][key])\n",
    "                        all_predicted_values[key].append(comparison[t]['predicted'][key])\n",
    "        \n",
    "        # R² 스코어 계산\n",
    "        r2_scores = {}\n",
    "        total_score = 0\n",
    "        valid_count = 0\n",
    "        \n",
    "        for key in all_actual_values:\n",
    "            if len(all_actual_values[key]) > 0:\n",
    "                score = r2_score(all_actual_values[key], all_predicted_values[key])\n",
    "                r2_scores[key] = score\n",
    "                total_score += score\n",
    "                valid_count += 1\n",
    "        \n",
    "        # 평균 R² 스코어\n",
    "        average_r2 = total_score / valid_count if valid_count > 0 else 0\n",
    "        r2_scores['average'] = average_r2\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(\"\\n=== R² Scores ===\")\n",
    "        for key, score in r2_scores.items():\n",
    "            print(f\"{key}: {score:.4f}\")\n",
    "        print(f\"Average R² Score: {average_r2:.4f}\")\n",
    "        \n",
    "        return r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentBatchSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"\n",
    "    실험 단위로 배치를 구성하는 샘플러\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # 실험별로 인덱스를 섞어 배치 구성\n",
    "        all_indices = []\n",
    "        for exp_id, indices in self.dataset.exp_indices:\n",
    "            # 각 실험 내 인덱스는 순서대로 유지\n",
    "            all_indices.extend(indices)\n",
    "            \n",
    "        # 배치 크기에 맞게 인덱스 그룹화\n",
    "        batches = [all_indices[i:i + self.batch_size] \n",
    "                   for i in range(0, len(all_indices), self.batch_size)]\n",
    "        \n",
    "        # 배치 순서는 섞음\n",
    "        np.random.shuffle(batches)\n",
    "        \n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.dataset) + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loaders_from_csv(csv_path, sequence_length=1, batch_size=32, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    CSV 파일에서 훈련 및 검증 데이터 로더 생성\n",
    "    \"\"\"\n",
    "    train_dataset = BMEDDataset(csv_path, sequence_length=sequence_length, train=True, train_ratio=train_ratio)\n",
    "    test_dataset = BMEDDataset(csv_path, sequence_length=sequence_length, train=False, train_ratio=train_ratio)\n",
    "    \n",
    "    # 실험 단위 배치 샘플러 사용\n",
    "    train_batch_sampler = ExperimentBatchSampler(train_dataset, batch_size)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_sampler=train_batch_sampler\n",
    "    )\n",
    "    \n",
    "    # 테스트 데이터는 순서대로 (shuffle=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_model_from_csv(csv_path, lstm_units=64, sequence_length=1, time_step=0.1, \n",
    "                        batch_size=32, epochs=100, train_ratio=0.8,\n",
    "                        mol_change_weight=1.0, state_weight=1.0):\n",
    "    \"\"\"\n",
    "    CSV 파일에서 모델 훈련을 위한 편의 함수\n",
    "    \"\"\"\n",
    "    # 데이터 로더 생성\n",
    "    train_loader, test_loader = data_loaders_from_csv(\n",
    "        csv_path, sequence_length, batch_size, train_ratio\n",
    "    )\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = MembraneSystemModel(lstm_units=lstm_units, time_step=time_step, sequence_length=sequence_length)\n",
    "    trainer = MembraneSystemTrainer(model)\n",
    "    \n",
    "    # 모델 훈련\n",
    "    train_losses, val_losses = trainer.train(\n",
    "        train_loader, \n",
    "        test_loader, \n",
    "        epochs=epochs,\n",
    "        mol_change_weight=mol_change_weight,\n",
    "        state_weight=state_weight\n",
    "    )\n",
    "    \n",
    "    return model, trainer, train_losses, val_losses\n",
    "\n",
    "def evaluate_model_on_experiments(csv_path, trainer, output_dir=None):\n",
    "    \"\"\"\n",
    "    모든 실험에 대해 모델 평가 및 결과 시각화\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results = {}\n",
    "    \n",
    "    # 각 실험별로 평가\n",
    "    for exp in df['exp'].unique():\n",
    "        exp_data = df[df['exp'] == exp].sort_values('t')\n",
    "        \n",
    "        # 첫 번째 행 데이터로 초기 조건 설정\n",
    "        first_row = exp_data.iloc[0]\n",
    "        \n",
    "        initial_conditions = {\n",
    "            'feed_la': first_row['CF_LA'],\n",
    "            'feed_k': first_row['CF_K'],\n",
    "            'acid_la': first_row['CA_LA'],\n",
    "            'base_k': first_row['CB_K'],\n",
    "            'feed_volume': first_row['VF'],\n",
    "            'acid_volume': first_row['VA'],\n",
    "            'base_volume': first_row['VB']\n",
    "        }\n",
    "        \n",
    "        operation_params = {\n",
    "            'temperature': first_row['T'],\n",
    "            'voltage': first_row['V'],\n",
    "            'electrolyte': first_row['E'],\n",
    "            'ci': first_row['Ci'],\n",
    "            'ki': first_row['Ki']\n",
    "        }\n",
    "        \n",
    "        # 실험 평가\n",
    "        eval_result = trainer.evaluate_experiment(exp_data, initial_conditions, operation_params)\n",
    "        results[exp] = eval_result\n",
    "        \n",
    "        # 결과 시각화 및 저장\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            plt_fig = trainer.plot_comparison(eval_result, exp)\n",
    "            plt.savefig(os.path.join(output_dir, f'experiment_{exp}_comparison.png'))\n",
    "            plt.close()\n",
    "    \n",
    "    return results   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 91 sequences, feature shape: (91, 3, 12)\n",
      "Mol change targets shape: (91, 3, 4)\n",
      "State targets shape: (91, 3, 9)\n",
      "Data loaded: 22 sequences, feature shape: (22, 3, 12)\n",
      "Mol change targets shape: (22, 3, 4)\n",
      "State targets shape: (22, 3, 9)\n",
      "Epoch 1/2000, Train Loss: 0.146859, Val Loss: 0.195766\n",
      "Epoch 2/2000, Train Loss: 0.111160, Val Loss: 0.182833\n",
      "Epoch 3/2000, Train Loss: 0.108559, Val Loss: 0.178082\n",
      "Epoch 4/2000, Train Loss: 0.101743, Val Loss: 0.174084\n",
      "Epoch 5/2000, Train Loss: 0.100182, Val Loss: 0.169976\n",
      "Epoch 6/2000, Train Loss: 0.095755, Val Loss: 0.165331\n",
      "Epoch 7/2000, Train Loss: 0.092290, Val Loss: 0.162452\n",
      "Epoch 8/2000, Train Loss: 0.091040, Val Loss: 0.160923\n",
      "Epoch 9/2000, Train Loss: 0.090033, Val Loss: 0.159057\n",
      "Epoch 10/2000, Train Loss: 0.088799, Val Loss: 0.157345\n",
      "Epoch 11/2000, Train Loss: 0.087753, Val Loss: 0.156489\n",
      "Epoch 12/2000, Train Loss: 0.087228, Val Loss: 0.156337\n",
      "Epoch 13/2000, Train Loss: 0.086697, Val Loss: 0.156095\n",
      "Epoch 14/2000, Train Loss: 0.086274, Val Loss: 0.155628\n",
      "Epoch 15/2000, Train Loss: 0.086153, Val Loss: 0.156412\n",
      "Epoch 16/2000, Train Loss: 0.085619, Val Loss: 0.154162\n",
      "Epoch 17/2000, Train Loss: 0.084812, Val Loss: 0.155164\n",
      "Epoch 18/2000, Train Loss: 0.085116, Val Loss: 0.153248\n",
      "Epoch 19/2000, Train Loss: 0.084593, Val Loss: 0.153079\n",
      "Epoch 20/2000, Train Loss: 0.084177, Val Loss: 0.151963\n",
      "Epoch 21/2000, Train Loss: 0.083673, Val Loss: 0.151602\n",
      "Epoch 22/2000, Train Loss: 0.083632, Val Loss: 0.153728\n",
      "Epoch 23/2000, Train Loss: 0.084100, Val Loss: 0.152657\n",
      "Epoch 24/2000, Train Loss: 0.084595, Val Loss: 0.155093\n",
      "Epoch 25/2000, Train Loss: 0.084328, Val Loss: 0.153163\n",
      "Epoch 26/2000, Train Loss: 0.083663, Val Loss: 0.153733\n",
      "Epoch 27/2000, Train Loss: 0.084157, Val Loss: 0.150264\n",
      "Epoch 28/2000, Train Loss: 0.083040, Val Loss: 0.150471\n",
      "Epoch 29/2000, Train Loss: 0.082469, Val Loss: 0.149657\n",
      "Epoch 30/2000, Train Loss: 0.082195, Val Loss: 0.149435\n",
      "Epoch 31/2000, Train Loss: 0.081761, Val Loss: 0.148732\n",
      "Epoch 32/2000, Train Loss: 0.081656, Val Loss: 0.148392\n",
      "Epoch 33/2000, Train Loss: 0.081422, Val Loss: 0.148573\n",
      "Epoch 34/2000, Train Loss: 0.081481, Val Loss: 0.148131\n",
      "Epoch 35/2000, Train Loss: 0.081722, Val Loss: 0.148660\n",
      "Epoch 36/2000, Train Loss: 0.081366, Val Loss: 0.147757\n",
      "Epoch 37/2000, Train Loss: 0.081051, Val Loss: 0.147578\n",
      "Epoch 38/2000, Train Loss: 0.081022, Val Loss: 0.147483\n",
      "Epoch 39/2000, Train Loss: 0.080878, Val Loss: 0.147742\n",
      "Epoch 40/2000, Train Loss: 0.080744, Val Loss: 0.147781\n",
      "Epoch 41/2000, Train Loss: 0.080392, Val Loss: 0.147252\n",
      "Epoch 42/2000, Train Loss: 0.080542, Val Loss: 0.147931\n",
      "Epoch 43/2000, Train Loss: 0.080538, Val Loss: 0.146785\n",
      "Epoch 44/2000, Train Loss: 0.080847, Val Loss: 0.146902\n",
      "Epoch 45/2000, Train Loss: 0.080375, Val Loss: 0.148151\n",
      "Epoch 46/2000, Train Loss: 0.080330, Val Loss: 0.146719\n",
      "Epoch 47/2000, Train Loss: 0.080143, Val Loss: 0.147199\n",
      "Epoch 48/2000, Train Loss: 0.080152, Val Loss: 0.147357\n",
      "Epoch 49/2000, Train Loss: 0.080066, Val Loss: 0.147522\n",
      "Epoch 50/2000, Train Loss: 0.079870, Val Loss: 0.146628\n",
      "Epoch 51/2000, Train Loss: 0.079859, Val Loss: 0.146915\n",
      "Epoch 52/2000, Train Loss: 0.079804, Val Loss: 0.147647\n",
      "Epoch 53/2000, Train Loss: 0.080036, Val Loss: 0.148880\n",
      "Epoch 54/2000, Train Loss: 0.080325, Val Loss: 0.146836\n",
      "Epoch 55/2000, Train Loss: 0.080054, Val Loss: 0.146957\n",
      "Epoch 56/2000, Train Loss: 0.079718, Val Loss: 0.148240\n",
      "Epoch 57/2000, Train Loss: 0.079827, Val Loss: 0.147107\n",
      "Epoch 58/2000, Train Loss: 0.080791, Val Loss: 0.147310\n",
      "Epoch 59/2000, Train Loss: 0.080876, Val Loss: 0.150597\n",
      "Epoch 60/2000, Train Loss: 0.080553, Val Loss: 0.147228\n",
      "Epoch 61/2000, Train Loss: 0.079969, Val Loss: 0.147887\n",
      "Epoch 62/2000, Train Loss: 0.079597, Val Loss: 0.148631\n",
      "Epoch 63/2000, Train Loss: 0.079538, Val Loss: 0.146483\n",
      "Epoch 64/2000, Train Loss: 0.079324, Val Loss: 0.146313\n",
      "Epoch 65/2000, Train Loss: 0.079253, Val Loss: 0.147038\n",
      "Epoch 66/2000, Train Loss: 0.079161, Val Loss: 0.146787\n",
      "Epoch 67/2000, Train Loss: 0.079017, Val Loss: 0.146490\n",
      "Epoch 68/2000, Train Loss: 0.079092, Val Loss: 0.147062\n",
      "Epoch 69/2000, Train Loss: 0.079005, Val Loss: 0.147323\n",
      "Epoch 70/2000, Train Loss: 0.079056, Val Loss: 0.146091\n",
      "Epoch 71/2000, Train Loss: 0.079230, Val Loss: 0.147010\n",
      "Epoch 72/2000, Train Loss: 0.078857, Val Loss: 0.146937\n",
      "Epoch 73/2000, Train Loss: 0.078967, Val Loss: 0.146417\n",
      "Epoch 74/2000, Train Loss: 0.078816, Val Loss: 0.146674\n",
      "Epoch 75/2000, Train Loss: 0.078886, Val Loss: 0.147483\n",
      "Epoch 76/2000, Train Loss: 0.078665, Val Loss: 0.146791\n",
      "Epoch 77/2000, Train Loss: 0.078555, Val Loss: 0.146419\n",
      "Epoch 78/2000, Train Loss: 0.078584, Val Loss: 0.146704\n",
      "Epoch 79/2000, Train Loss: 0.078409, Val Loss: 0.146850\n",
      "Epoch 80/2000, Train Loss: 0.078471, Val Loss: 0.147326\n",
      "Epoch 81/2000, Train Loss: 0.078519, Val Loss: 0.146253\n",
      "Epoch 82/2000, Train Loss: 0.078475, Val Loss: 0.146535\n",
      "Epoch 83/2000, Train Loss: 0.078310, Val Loss: 0.146997\n",
      "Epoch 84/2000, Train Loss: 0.078348, Val Loss: 0.146316\n",
      "Epoch 85/2000, Train Loss: 0.078316, Val Loss: 0.146884\n",
      "Epoch 86/2000, Train Loss: 0.078379, Val Loss: 0.148665\n",
      "Epoch 87/2000, Train Loss: 0.078730, Val Loss: 0.147102\n",
      "Epoch 88/2000, Train Loss: 0.078590, Val Loss: 0.146594\n",
      "Epoch 89/2000, Train Loss: 0.078837, Val Loss: 0.148951\n",
      "Epoch 90/2000, Train Loss: 0.078785, Val Loss: 0.149249\n",
      "Epoch 91/2000, Train Loss: 0.078994, Val Loss: 0.146598\n",
      "Epoch 92/2000, Train Loss: 0.078713, Val Loss: 0.147247\n",
      "Epoch 93/2000, Train Loss: 0.078600, Val Loss: 0.147978\n",
      "Epoch 94/2000, Train Loss: 0.078154, Val Loss: 0.146137\n",
      "Epoch 95/2000, Train Loss: 0.078156, Val Loss: 0.146206\n",
      "Epoch 96/2000, Train Loss: 0.078494, Val Loss: 0.148622\n",
      "Epoch 97/2000, Train Loss: 0.078649, Val Loss: 0.147696\n",
      "Epoch 98/2000, Train Loss: 0.079036, Val Loss: 0.146194\n",
      "Epoch 99/2000, Train Loss: 0.078271, Val Loss: 0.146701\n",
      "Epoch 100/2000, Train Loss: 0.078465, Val Loss: 0.149804\n",
      "Epoch 101/2000, Train Loss: 0.079103, Val Loss: 0.148883\n",
      "Epoch 102/2000, Train Loss: 0.078815, Val Loss: 0.146057\n",
      "Epoch 103/2000, Train Loss: 0.078321, Val Loss: 0.148287\n",
      "Epoch 104/2000, Train Loss: 0.078110, Val Loss: 0.147911\n",
      "Epoch 105/2000, Train Loss: 0.078128, Val Loss: 0.146543\n",
      "Epoch 106/2000, Train Loss: 0.078485, Val Loss: 0.147185\n",
      "Epoch 107/2000, Train Loss: 0.078396, Val Loss: 0.147739\n",
      "Epoch 108/2000, Train Loss: 0.078228, Val Loss: 0.146560\n",
      "Epoch 109/2000, Train Loss: 0.078038, Val Loss: 0.146144\n",
      "Epoch 110/2000, Train Loss: 0.078415, Val Loss: 0.147652\n",
      "Epoch 111/2000, Train Loss: 0.078551, Val Loss: 0.148154\n",
      "Epoch 112/2000, Train Loss: 0.079173, Val Loss: 0.147396\n",
      "Epoch 113/2000, Train Loss: 0.078972, Val Loss: 0.149521\n",
      "Epoch 114/2000, Train Loss: 0.078117, Val Loss: 0.146733\n",
      "Epoch 115/2000, Train Loss: 0.078208, Val Loss: 0.146329\n",
      "Epoch 116/2000, Train Loss: 0.077922, Val Loss: 0.148318\n",
      "Epoch 117/2000, Train Loss: 0.078088, Val Loss: 0.147154\n",
      "Epoch 118/2000, Train Loss: 0.077926, Val Loss: 0.146691\n",
      "Epoch 119/2000, Train Loss: 0.078234, Val Loss: 0.147706\n",
      "Epoch 120/2000, Train Loss: 0.077813, Val Loss: 0.146893\n",
      "Epoch 121/2000, Train Loss: 0.077857, Val Loss: 0.146499\n",
      "Epoch 122/2000, Train Loss: 0.077575, Val Loss: 0.147291\n",
      "Epoch 123/2000, Train Loss: 0.077371, Val Loss: 0.146702\n",
      "Epoch 124/2000, Train Loss: 0.077394, Val Loss: 0.146665\n",
      "Epoch 125/2000, Train Loss: 0.077337, Val Loss: 0.147229\n",
      "Epoch 126/2000, Train Loss: 0.077213, Val Loss: 0.146547\n",
      "Epoch 127/2000, Train Loss: 0.077399, Val Loss: 0.146378\n",
      "Epoch 128/2000, Train Loss: 0.077507, Val Loss: 0.147365\n",
      "Epoch 129/2000, Train Loss: 0.077193, Val Loss: 0.146917\n",
      "Epoch 130/2000, Train Loss: 0.077296, Val Loss: 0.146644\n",
      "Epoch 131/2000, Train Loss: 0.077528, Val Loss: 0.147470\n",
      "Epoch 132/2000, Train Loss: 0.077474, Val Loss: 0.146586\n",
      "Epoch 133/2000, Train Loss: 0.077354, Val Loss: 0.148021\n",
      "Epoch 134/2000, Train Loss: 0.077302, Val Loss: 0.147342\n",
      "Epoch 135/2000, Train Loss: 0.077485, Val Loss: 0.147908\n",
      "Epoch 136/2000, Train Loss: 0.077647, Val Loss: 0.146634\n",
      "Epoch 137/2000, Train Loss: 0.078124, Val Loss: 0.146812\n",
      "Epoch 138/2000, Train Loss: 0.078444, Val Loss: 0.147156\n",
      "Epoch 139/2000, Train Loss: 0.077996, Val Loss: 0.146917\n",
      "Epoch 140/2000, Train Loss: 0.077724, Val Loss: 0.147297\n",
      "Epoch 141/2000, Train Loss: 0.077181, Val Loss: 0.146972\n",
      "Epoch 142/2000, Train Loss: 0.077368, Val Loss: 0.146733\n",
      "Epoch 143/2000, Train Loss: 0.077271, Val Loss: 0.147218\n",
      "Epoch 144/2000, Train Loss: 0.077187, Val Loss: 0.146615\n",
      "Epoch 145/2000, Train Loss: 0.077139, Val Loss: 0.147882\n",
      "Epoch 146/2000, Train Loss: 0.077064, Val Loss: 0.146605\n",
      "Epoch 147/2000, Train Loss: 0.077064, Val Loss: 0.146859\n",
      "Epoch 148/2000, Train Loss: 0.076961, Val Loss: 0.147297\n",
      "Epoch 149/2000, Train Loss: 0.077239, Val Loss: 0.147647\n",
      "Epoch 150/2000, Train Loss: 0.077552, Val Loss: 0.146617\n",
      "Epoch 151/2000, Train Loss: 0.078660, Val Loss: 0.148747\n",
      "Epoch 152/2000, Train Loss: 0.077447, Val Loss: 0.148712\n",
      "Epoch 153/2000, Train Loss: 0.077284, Val Loss: 0.148120\n",
      "Epoch 154/2000, Train Loss: 0.077021, Val Loss: 0.147336\n",
      "Epoch 155/2000, Train Loss: 0.077023, Val Loss: 0.146673\n",
      "Epoch 156/2000, Train Loss: 0.076836, Val Loss: 0.146830\n",
      "Epoch 157/2000, Train Loss: 0.076665, Val Loss: 0.146912\n",
      "Epoch 158/2000, Train Loss: 0.076707, Val Loss: 0.147302\n",
      "Epoch 159/2000, Train Loss: 0.076990, Val Loss: 0.147541\n",
      "Epoch 160/2000, Train Loss: 0.077665, Val Loss: 0.148488\n",
      "Epoch 161/2000, Train Loss: 0.077384, Val Loss: 0.147552\n",
      "Epoch 162/2000, Train Loss: 0.076985, Val Loss: 0.147344\n",
      "Epoch 163/2000, Train Loss: 0.076576, Val Loss: 0.147551\n",
      "Epoch 164/2000, Train Loss: 0.076491, Val Loss: 0.147144\n",
      "Epoch 165/2000, Train Loss: 0.076798, Val Loss: 0.147192\n",
      "Epoch 166/2000, Train Loss: 0.076949, Val Loss: 0.148115\n",
      "Epoch 167/2000, Train Loss: 0.076509, Val Loss: 0.146757\n",
      "Epoch 168/2000, Train Loss: 0.077893, Val Loss: 0.149395\n",
      "Epoch 169/2000, Train Loss: 0.078446, Val Loss: 0.151426\n",
      "Epoch 170/2000, Train Loss: 0.078579, Val Loss: 0.147182\n",
      "Epoch 171/2000, Train Loss: 0.077166, Val Loss: 0.147783\n",
      "Epoch 172/2000, Train Loss: 0.076816, Val Loss: 0.149731\n",
      "Epoch 173/2000, Train Loss: 0.076870, Val Loss: 0.147676\n",
      "Epoch 174/2000, Train Loss: 0.076772, Val Loss: 0.147491\n",
      "Epoch 175/2000, Train Loss: 0.076917, Val Loss: 0.148278\n",
      "Epoch 176/2000, Train Loss: 0.076837, Val Loss: 0.146980\n",
      "Epoch 177/2000, Train Loss: 0.077163, Val Loss: 0.147409\n",
      "Epoch 178/2000, Train Loss: 0.077286, Val Loss: 0.150465\n",
      "Epoch 179/2000, Train Loss: 0.077566, Val Loss: 0.146957\n",
      "Epoch 180/2000, Train Loss: 0.077314, Val Loss: 0.147268\n",
      "Epoch 181/2000, Train Loss: 0.077089, Val Loss: 0.149578\n",
      "Epoch 182/2000, Train Loss: 0.077176, Val Loss: 0.147812\n",
      "Epoch 183/2000, Train Loss: 0.077707, Val Loss: 0.148256\n",
      "Epoch 184/2000, Train Loss: 0.077531, Val Loss: 0.152207\n",
      "Epoch 185/2000, Train Loss: 0.077683, Val Loss: 0.147964\n",
      "Epoch 186/2000, Train Loss: 0.077783, Val Loss: 0.149059\n",
      "Epoch 187/2000, Train Loss: 0.078084, Val Loss: 0.148085\n",
      "Epoch 188/2000, Train Loss: 0.077365, Val Loss: 0.146830\n",
      "Epoch 189/2000, Train Loss: 0.076499, Val Loss: 0.147196\n",
      "Epoch 190/2000, Train Loss: 0.076358, Val Loss: 0.148246\n",
      "Epoch 191/2000, Train Loss: 0.076555, Val Loss: 0.149211\n",
      "Epoch 192/2000, Train Loss: 0.076221, Val Loss: 0.146821\n",
      "Epoch 193/2000, Train Loss: 0.076294, Val Loss: 0.148894\n",
      "Epoch 194/2000, Train Loss: 0.076636, Val Loss: 0.147595\n",
      "Epoch 195/2000, Train Loss: 0.077116, Val Loss: 0.149088\n",
      "Epoch 196/2000, Train Loss: 0.076560, Val Loss: 0.148761\n",
      "Epoch 197/2000, Train Loss: 0.076354, Val Loss: 0.147028\n",
      "Epoch 198/2000, Train Loss: 0.077123, Val Loss: 0.150032\n",
      "Epoch 199/2000, Train Loss: 0.076569, Val Loss: 0.149187\n",
      "Epoch 200/2000, Train Loss: 0.075994, Val Loss: 0.147552\n",
      "Epoch 201/2000, Train Loss: 0.076012, Val Loss: 0.149124\n",
      "Epoch 202/2000, Train Loss: 0.076792, Val Loss: 0.149829\n",
      "Epoch 203/2000, Train Loss: 0.076456, Val Loss: 0.147247\n",
      "Epoch 204/2000, Train Loss: 0.077211, Val Loss: 0.149355\n",
      "Epoch 205/2000, Train Loss: 0.077016, Val Loss: 0.148136\n",
      "Epoch 206/2000, Train Loss: 0.076669, Val Loss: 0.148306\n",
      "Epoch 207/2000, Train Loss: 0.076489, Val Loss: 0.149142\n",
      "Epoch 208/2000, Train Loss: 0.077643, Val Loss: 0.148316\n",
      "Epoch 209/2000, Train Loss: 0.077012, Val Loss: 0.149789\n",
      "Epoch 210/2000, Train Loss: 0.077563, Val Loss: 0.149452\n",
      "Epoch 211/2000, Train Loss: 0.077258, Val Loss: 0.147498\n",
      "Epoch 212/2000, Train Loss: 0.076343, Val Loss: 0.150010\n",
      "Epoch 213/2000, Train Loss: 0.076457, Val Loss: 0.148416\n",
      "Epoch 214/2000, Train Loss: 0.076518, Val Loss: 0.148238\n",
      "Epoch 215/2000, Train Loss: 0.076271, Val Loss: 0.148389\n",
      "Epoch 216/2000, Train Loss: 0.076089, Val Loss: 0.148553\n",
      "Epoch 217/2000, Train Loss: 0.075938, Val Loss: 0.147711\n",
      "Epoch 218/2000, Train Loss: 0.075825, Val Loss: 0.147734\n",
      "Epoch 219/2000, Train Loss: 0.075938, Val Loss: 0.148040\n",
      "Epoch 220/2000, Train Loss: 0.075706, Val Loss: 0.148495\n",
      "Epoch 221/2000, Train Loss: 0.075946, Val Loss: 0.148006\n",
      "Epoch 222/2000, Train Loss: 0.076043, Val Loss: 0.148066\n",
      "Epoch 223/2000, Train Loss: 0.076231, Val Loss: 0.149384\n",
      "Epoch 224/2000, Train Loss: 0.075667, Val Loss: 0.148085\n",
      "Epoch 225/2000, Train Loss: 0.075741, Val Loss: 0.148581\n",
      "Epoch 226/2000, Train Loss: 0.075648, Val Loss: 0.148478\n",
      "Epoch 227/2000, Train Loss: 0.075558, Val Loss: 0.148571\n",
      "Epoch 228/2000, Train Loss: 0.075525, Val Loss: 0.148462\n",
      "Epoch 229/2000, Train Loss: 0.075723, Val Loss: 0.147631\n",
      "Epoch 230/2000, Train Loss: 0.075810, Val Loss: 0.150262\n",
      "Epoch 231/2000, Train Loss: 0.075579, Val Loss: 0.148824\n",
      "Epoch 232/2000, Train Loss: 0.075960, Val Loss: 0.147917\n",
      "Epoch 233/2000, Train Loss: 0.075918, Val Loss: 0.147687\n",
      "Epoch 234/2000, Train Loss: 0.076055, Val Loss: 0.149653\n",
      "Epoch 235/2000, Train Loss: 0.075904, Val Loss: 0.147099\n",
      "Epoch 236/2000, Train Loss: 0.075880, Val Loss: 0.150108\n",
      "Epoch 237/2000, Train Loss: 0.075874, Val Loss: 0.148496\n",
      "Epoch 238/2000, Train Loss: 0.075549, Val Loss: 0.148642\n",
      "Epoch 239/2000, Train Loss: 0.075607, Val Loss: 0.147603\n",
      "Epoch 240/2000, Train Loss: 0.075985, Val Loss: 0.149131\n",
      "Epoch 241/2000, Train Loss: 0.075463, Val Loss: 0.147171\n",
      "Epoch 242/2000, Train Loss: 0.075610, Val Loss: 0.149595\n",
      "Epoch 243/2000, Train Loss: 0.075558, Val Loss: 0.147798\n",
      "Epoch 244/2000, Train Loss: 0.075852, Val Loss: 0.147514\n",
      "Epoch 245/2000, Train Loss: 0.075725, Val Loss: 0.148197\n",
      "Epoch 246/2000, Train Loss: 0.075567, Val Loss: 0.149243\n",
      "Epoch 247/2000, Train Loss: 0.075289, Val Loss: 0.147991\n",
      "Epoch 248/2000, Train Loss: 0.075132, Val Loss: 0.147377\n",
      "Epoch 249/2000, Train Loss: 0.074993, Val Loss: 0.148389\n",
      "Epoch 250/2000, Train Loss: 0.075126, Val Loss: 0.148870\n",
      "Epoch 251/2000, Train Loss: 0.075122, Val Loss: 0.147868\n",
      "Epoch 252/2000, Train Loss: 0.074958, Val Loss: 0.147612\n",
      "Epoch 253/2000, Train Loss: 0.075005, Val Loss: 0.149502\n",
      "Epoch 254/2000, Train Loss: 0.075196, Val Loss: 0.148155\n",
      "Epoch 255/2000, Train Loss: 0.075049, Val Loss: 0.148350\n",
      "Epoch 256/2000, Train Loss: 0.074932, Val Loss: 0.148176\n",
      "Epoch 257/2000, Train Loss: 0.074873, Val Loss: 0.147272\n",
      "Epoch 258/2000, Train Loss: 0.074878, Val Loss: 0.148888\n",
      "Epoch 259/2000, Train Loss: 0.074958, Val Loss: 0.147348\n",
      "Epoch 260/2000, Train Loss: 0.074909, Val Loss: 0.148196\n",
      "Epoch 261/2000, Train Loss: 0.074715, Val Loss: 0.147870\n",
      "Epoch 262/2000, Train Loss: 0.074682, Val Loss: 0.147755\n",
      "Epoch 263/2000, Train Loss: 0.074639, Val Loss: 0.148041\n",
      "Epoch 264/2000, Train Loss: 0.074557, Val Loss: 0.148460\n",
      "Epoch 265/2000, Train Loss: 0.074582, Val Loss: 0.148611\n",
      "Epoch 266/2000, Train Loss: 0.074774, Val Loss: 0.147508\n",
      "Epoch 267/2000, Train Loss: 0.074786, Val Loss: 0.147950\n",
      "Epoch 268/2000, Train Loss: 0.074871, Val Loss: 0.148009\n",
      "Epoch 269/2000, Train Loss: 0.074668, Val Loss: 0.148782\n",
      "Epoch 270/2000, Train Loss: 0.075316, Val Loss: 0.149478\n",
      "Epoch 271/2000, Train Loss: 0.075292, Val Loss: 0.148109\n",
      "Epoch 272/2000, Train Loss: 0.076048, Val Loss: 0.149141\n",
      "Epoch 273/2000, Train Loss: 0.077631, Val Loss: 0.150241\n",
      "Epoch 274/2000, Train Loss: 0.077053, Val Loss: 0.147306\n",
      "Epoch 275/2000, Train Loss: 0.076304, Val Loss: 0.151234\n",
      "Epoch 276/2000, Train Loss: 0.076806, Val Loss: 0.147774\n",
      "Epoch 277/2000, Train Loss: 0.075955, Val Loss: 0.146864\n",
      "Epoch 278/2000, Train Loss: 0.075838, Val Loss: 0.151545\n",
      "Epoch 279/2000, Train Loss: 0.075312, Val Loss: 0.149894\n",
      "Epoch 280/2000, Train Loss: 0.075188, Val Loss: 0.147414\n",
      "Epoch 281/2000, Train Loss: 0.075002, Val Loss: 0.147917\n",
      "Epoch 282/2000, Train Loss: 0.074695, Val Loss: 0.148382\n",
      "Epoch 283/2000, Train Loss: 0.075080, Val Loss: 0.147602\n",
      "Epoch 284/2000, Train Loss: 0.074914, Val Loss: 0.148015\n",
      "Epoch 285/2000, Train Loss: 0.074984, Val Loss: 0.148903\n",
      "Epoch 286/2000, Train Loss: 0.075035, Val Loss: 0.147709\n",
      "Epoch 287/2000, Train Loss: 0.074960, Val Loss: 0.148894\n",
      "Epoch 288/2000, Train Loss: 0.074877, Val Loss: 0.148554\n",
      "Epoch 289/2000, Train Loss: 0.074879, Val Loss: 0.147630\n",
      "Epoch 290/2000, Train Loss: 0.075275, Val Loss: 0.148561\n",
      "Epoch 291/2000, Train Loss: 0.075003, Val Loss: 0.147949\n",
      "Epoch 292/2000, Train Loss: 0.074542, Val Loss: 0.148948\n",
      "Epoch 293/2000, Train Loss: 0.074701, Val Loss: 0.148226\n",
      "Epoch 294/2000, Train Loss: 0.074816, Val Loss: 0.148669\n",
      "Epoch 295/2000, Train Loss: 0.074835, Val Loss: 0.147829\n",
      "Epoch 296/2000, Train Loss: 0.074657, Val Loss: 0.148166\n",
      "Epoch 297/2000, Train Loss: 0.074591, Val Loss: 0.148061\n",
      "Epoch 298/2000, Train Loss: 0.074434, Val Loss: 0.149815\n",
      "Epoch 299/2000, Train Loss: 0.074375, Val Loss: 0.148428\n",
      "Epoch 300/2000, Train Loss: 0.074377, Val Loss: 0.148316\n",
      "Epoch 301/2000, Train Loss: 0.074916, Val Loss: 0.148264\n",
      "Epoch 302/2000, Train Loss: 0.074736, Val Loss: 0.148623\n",
      "Epoch 303/2000, Train Loss: 0.075150, Val Loss: 0.147842\n",
      "Epoch 304/2000, Train Loss: 0.075166, Val Loss: 0.148331\n",
      "Epoch 305/2000, Train Loss: 0.074580, Val Loss: 0.149218\n",
      "Epoch 306/2000, Train Loss: 0.075107, Val Loss: 0.148165\n",
      "Epoch 307/2000, Train Loss: 0.075611, Val Loss: 0.147293\n",
      "Epoch 308/2000, Train Loss: 0.076275, Val Loss: 0.152857\n",
      "Epoch 309/2000, Train Loss: 0.076289, Val Loss: 0.149217\n",
      "Epoch 310/2000, Train Loss: 0.075781, Val Loss: 0.148827\n",
      "Epoch 311/2000, Train Loss: 0.075984, Val Loss: 0.147833\n",
      "Epoch 312/2000, Train Loss: 0.075442, Val Loss: 0.148851\n",
      "Epoch 313/2000, Train Loss: 0.075058, Val Loss: 0.148614\n",
      "Epoch 314/2000, Train Loss: 0.075032, Val Loss: 0.149835\n",
      "Epoch 315/2000, Train Loss: 0.075380, Val Loss: 0.148784\n",
      "Epoch 316/2000, Train Loss: 0.074872, Val Loss: 0.148999\n",
      "Epoch 317/2000, Train Loss: 0.074861, Val Loss: 0.149436\n",
      "Epoch 318/2000, Train Loss: 0.075058, Val Loss: 0.147697\n",
      "Epoch 319/2000, Train Loss: 0.075360, Val Loss: 0.148345\n",
      "Epoch 320/2000, Train Loss: 0.075417, Val Loss: 0.148128\n",
      "Epoch 321/2000, Train Loss: 0.074498, Val Loss: 0.148503\n",
      "Epoch 322/2000, Train Loss: 0.074846, Val Loss: 0.147427\n",
      "Epoch 323/2000, Train Loss: 0.074590, Val Loss: 0.150403\n",
      "Epoch 324/2000, Train Loss: 0.074734, Val Loss: 0.147872\n",
      "Epoch 325/2000, Train Loss: 0.074665, Val Loss: 0.149295\n",
      "Epoch 326/2000, Train Loss: 0.074147, Val Loss: 0.148128\n",
      "Epoch 327/2000, Train Loss: 0.074690, Val Loss: 0.148974\n",
      "Epoch 328/2000, Train Loss: 0.074594, Val Loss: 0.148105\n",
      "Epoch 329/2000, Train Loss: 0.074287, Val Loss: 0.148784\n",
      "Epoch 330/2000, Train Loss: 0.074321, Val Loss: 0.148346\n",
      "Epoch 331/2000, Train Loss: 0.074538, Val Loss: 0.148541\n",
      "Epoch 332/2000, Train Loss: 0.074724, Val Loss: 0.147938\n",
      "Epoch 333/2000, Train Loss: 0.074956, Val Loss: 0.150301\n",
      "Epoch 334/2000, Train Loss: 0.074502, Val Loss: 0.147316\n",
      "Epoch 335/2000, Train Loss: 0.074637, Val Loss: 0.148944\n",
      "Epoch 336/2000, Train Loss: 0.074632, Val Loss: 0.148229\n",
      "Epoch 337/2000, Train Loss: 0.074740, Val Loss: 0.147842\n",
      "Epoch 338/2000, Train Loss: 0.074602, Val Loss: 0.148552\n",
      "Epoch 339/2000, Train Loss: 0.074346, Val Loss: 0.149194\n",
      "Epoch 340/2000, Train Loss: 0.074359, Val Loss: 0.148749\n",
      "Epoch 341/2000, Train Loss: 0.074633, Val Loss: 0.148785\n",
      "Epoch 342/2000, Train Loss: 0.074742, Val Loss: 0.148502\n",
      "Epoch 343/2000, Train Loss: 0.074671, Val Loss: 0.148217\n",
      "Epoch 344/2000, Train Loss: 0.074269, Val Loss: 0.148936\n",
      "Epoch 345/2000, Train Loss: 0.074325, Val Loss: 0.148594\n",
      "Epoch 346/2000, Train Loss: 0.074282, Val Loss: 0.148217\n",
      "Epoch 347/2000, Train Loss: 0.073853, Val Loss: 0.148608\n",
      "Epoch 348/2000, Train Loss: 0.074092, Val Loss: 0.148800\n",
      "Epoch 349/2000, Train Loss: 0.074129, Val Loss: 0.148307\n",
      "Epoch 350/2000, Train Loss: 0.074455, Val Loss: 0.148226\n",
      "Epoch 351/2000, Train Loss: 0.074429, Val Loss: 0.150536\n",
      "Epoch 352/2000, Train Loss: 0.074744, Val Loss: 0.148408\n",
      "Epoch 353/2000, Train Loss: 0.074649, Val Loss: 0.149501\n",
      "Epoch 354/2000, Train Loss: 0.074925, Val Loss: 0.147382\n",
      "Epoch 355/2000, Train Loss: 0.075227, Val Loss: 0.149858\n",
      "Epoch 356/2000, Train Loss: 0.075269, Val Loss: 0.150881\n",
      "Epoch 357/2000, Train Loss: 0.075102, Val Loss: 0.148463\n",
      "Epoch 358/2000, Train Loss: 0.074727, Val Loss: 0.147622\n",
      "Epoch 359/2000, Train Loss: 0.074216, Val Loss: 0.151017\n",
      "Epoch 360/2000, Train Loss: 0.074613, Val Loss: 0.148842\n",
      "Epoch 361/2000, Train Loss: 0.074281, Val Loss: 0.147549\n",
      "Epoch 362/2000, Train Loss: 0.075010, Val Loss: 0.149254\n",
      "Epoch 363/2000, Train Loss: 0.074836, Val Loss: 0.148127\n",
      "Epoch 364/2000, Train Loss: 0.074129, Val Loss: 0.149237\n",
      "Epoch 365/2000, Train Loss: 0.074937, Val Loss: 0.148276\n",
      "Epoch 366/2000, Train Loss: 0.074237, Val Loss: 0.147243\n",
      "Epoch 367/2000, Train Loss: 0.074902, Val Loss: 0.149329\n",
      "Epoch 368/2000, Train Loss: 0.075374, Val Loss: 0.148868\n",
      "Epoch 369/2000, Train Loss: 0.074690, Val Loss: 0.146867\n",
      "Epoch 370/2000, Train Loss: 0.075138, Val Loss: 0.152310\n",
      "Epoch 371/2000, Train Loss: 0.075025, Val Loss: 0.147749\n",
      "Epoch 372/2000, Train Loss: 0.074874, Val Loss: 0.148851\n",
      "Epoch 373/2000, Train Loss: 0.074459, Val Loss: 0.148501\n",
      "Epoch 374/2000, Train Loss: 0.075254, Val Loss: 0.148839\n",
      "Epoch 375/2000, Train Loss: 0.074836, Val Loss: 0.148520\n",
      "Epoch 376/2000, Train Loss: 0.075480, Val Loss: 0.149494\n",
      "Epoch 377/2000, Train Loss: 0.076069, Val Loss: 0.148305\n",
      "Epoch 378/2000, Train Loss: 0.075757, Val Loss: 0.147686\n",
      "Epoch 379/2000, Train Loss: 0.075514, Val Loss: 0.149193\n",
      "Epoch 380/2000, Train Loss: 0.075117, Val Loss: 0.148062\n",
      "Epoch 381/2000, Train Loss: 0.074826, Val Loss: 0.148459\n",
      "Epoch 382/2000, Train Loss: 0.074844, Val Loss: 0.150702\n",
      "Epoch 383/2000, Train Loss: 0.074935, Val Loss: 0.148801\n",
      "Epoch 384/2000, Train Loss: 0.074298, Val Loss: 0.149036\n",
      "Epoch 385/2000, Train Loss: 0.074451, Val Loss: 0.147761\n",
      "Epoch 386/2000, Train Loss: 0.074538, Val Loss: 0.149573\n",
      "Epoch 387/2000, Train Loss: 0.074190, Val Loss: 0.148334\n",
      "Epoch 388/2000, Train Loss: 0.074139, Val Loss: 0.147883\n",
      "Epoch 389/2000, Train Loss: 0.074027, Val Loss: 0.149254\n",
      "Epoch 390/2000, Train Loss: 0.073995, Val Loss: 0.148007\n",
      "Epoch 391/2000, Train Loss: 0.073953, Val Loss: 0.147234\n",
      "Epoch 392/2000, Train Loss: 0.073932, Val Loss: 0.148541\n",
      "Epoch 393/2000, Train Loss: 0.073732, Val Loss: 0.147493\n",
      "Epoch 394/2000, Train Loss: 0.074096, Val Loss: 0.147483\n",
      "Epoch 395/2000, Train Loss: 0.074111, Val Loss: 0.149033\n",
      "Epoch 396/2000, Train Loss: 0.074042, Val Loss: 0.147796\n",
      "Epoch 397/2000, Train Loss: 0.073838, Val Loss: 0.148922\n",
      "Epoch 398/2000, Train Loss: 0.074166, Val Loss: 0.147974\n",
      "Epoch 399/2000, Train Loss: 0.074103, Val Loss: 0.148649\n",
      "Epoch 400/2000, Train Loss: 0.074030, Val Loss: 0.147480\n",
      "Epoch 401/2000, Train Loss: 0.074120, Val Loss: 0.148673\n",
      "Epoch 402/2000, Train Loss: 0.073932, Val Loss: 0.148328\n",
      "Epoch 403/2000, Train Loss: 0.073773, Val Loss: 0.147880\n",
      "Epoch 404/2000, Train Loss: 0.073879, Val Loss: 0.148645\n",
      "Epoch 405/2000, Train Loss: 0.073681, Val Loss: 0.148259\n",
      "Epoch 406/2000, Train Loss: 0.073665, Val Loss: 0.148342\n",
      "Epoch 407/2000, Train Loss: 0.073558, Val Loss: 0.148403\n",
      "Epoch 408/2000, Train Loss: 0.073607, Val Loss: 0.148966\n",
      "Epoch 409/2000, Train Loss: 0.073790, Val Loss: 0.147187\n",
      "Epoch 410/2000, Train Loss: 0.073830, Val Loss: 0.148687\n",
      "Epoch 411/2000, Train Loss: 0.074476, Val Loss: 0.148276\n",
      "Epoch 412/2000, Train Loss: 0.074026, Val Loss: 0.149049\n",
      "Epoch 413/2000, Train Loss: 0.074286, Val Loss: 0.148119\n",
      "Epoch 414/2000, Train Loss: 0.074533, Val Loss: 0.147400\n",
      "Epoch 415/2000, Train Loss: 0.074607, Val Loss: 0.150035\n",
      "Epoch 416/2000, Train Loss: 0.074604, Val Loss: 0.147656\n",
      "Epoch 417/2000, Train Loss: 0.074568, Val Loss: 0.148181\n",
      "Epoch 418/2000, Train Loss: 0.074276, Val Loss: 0.148571\n",
      "Epoch 419/2000, Train Loss: 0.074461, Val Loss: 0.149349\n",
      "Epoch 420/2000, Train Loss: 0.074793, Val Loss: 0.147059\n",
      "Epoch 421/2000, Train Loss: 0.074484, Val Loss: 0.148915\n",
      "Epoch 422/2000, Train Loss: 0.074835, Val Loss: 0.147719\n",
      "Epoch 423/2000, Train Loss: 0.074684, Val Loss: 0.148953\n",
      "Epoch 424/2000, Train Loss: 0.074544, Val Loss: 0.147847\n",
      "Epoch 425/2000, Train Loss: 0.074678, Val Loss: 0.148739\n",
      "Epoch 426/2000, Train Loss: 0.074071, Val Loss: 0.147871\n",
      "Epoch 427/2000, Train Loss: 0.074268, Val Loss: 0.147503\n",
      "Epoch 428/2000, Train Loss: 0.074774, Val Loss: 0.149892\n",
      "Epoch 429/2000, Train Loss: 0.074274, Val Loss: 0.147795\n",
      "Epoch 430/2000, Train Loss: 0.074279, Val Loss: 0.147301\n",
      "Epoch 431/2000, Train Loss: 0.074931, Val Loss: 0.149025\n",
      "Epoch 432/2000, Train Loss: 0.074078, Val Loss: 0.148751\n",
      "Epoch 433/2000, Train Loss: 0.073966, Val Loss: 0.147242\n",
      "Epoch 434/2000, Train Loss: 0.073950, Val Loss: 0.147894\n",
      "Epoch 435/2000, Train Loss: 0.073719, Val Loss: 0.148048\n",
      "Epoch 436/2000, Train Loss: 0.073797, Val Loss: 0.147628\n",
      "Epoch 437/2000, Train Loss: 0.073774, Val Loss: 0.148417\n",
      "Epoch 438/2000, Train Loss: 0.073666, Val Loss: 0.148099\n",
      "Epoch 439/2000, Train Loss: 0.073453, Val Loss: 0.147565\n",
      "Epoch 440/2000, Train Loss: 0.073460, Val Loss: 0.148451\n",
      "Epoch 441/2000, Train Loss: 0.073470, Val Loss: 0.147585\n",
      "Epoch 442/2000, Train Loss: 0.073372, Val Loss: 0.147797\n",
      "Epoch 443/2000, Train Loss: 0.073379, Val Loss: 0.148339\n",
      "Epoch 444/2000, Train Loss: 0.073303, Val Loss: 0.147828\n",
      "Epoch 445/2000, Train Loss: 0.073335, Val Loss: 0.147789\n",
      "Epoch 446/2000, Train Loss: 0.073386, Val Loss: 0.148290\n",
      "Epoch 447/2000, Train Loss: 0.073700, Val Loss: 0.147638\n",
      "Epoch 448/2000, Train Loss: 0.073367, Val Loss: 0.148323\n",
      "Epoch 449/2000, Train Loss: 0.073479, Val Loss: 0.148513\n",
      "Epoch 450/2000, Train Loss: 0.073305, Val Loss: 0.147970\n",
      "Epoch 451/2000, Train Loss: 0.073322, Val Loss: 0.148553\n",
      "Epoch 452/2000, Train Loss: 0.073498, Val Loss: 0.147921\n",
      "Epoch 453/2000, Train Loss: 0.073583, Val Loss: 0.147865\n",
      "Epoch 454/2000, Train Loss: 0.073514, Val Loss: 0.148861\n",
      "Epoch 455/2000, Train Loss: 0.073512, Val Loss: 0.148127\n",
      "Epoch 456/2000, Train Loss: 0.073561, Val Loss: 0.149243\n",
      "Epoch 457/2000, Train Loss: 0.073907, Val Loss: 0.149272\n",
      "Epoch 458/2000, Train Loss: 0.073472, Val Loss: 0.147551\n",
      "Epoch 459/2000, Train Loss: 0.073575, Val Loss: 0.147895\n",
      "Epoch 460/2000, Train Loss: 0.073368, Val Loss: 0.147700\n",
      "Epoch 461/2000, Train Loss: 0.073338, Val Loss: 0.147525\n",
      "Epoch 462/2000, Train Loss: 0.073534, Val Loss: 0.148376\n",
      "Epoch 463/2000, Train Loss: 0.073296, Val Loss: 0.148316\n",
      "Epoch 464/2000, Train Loss: 0.073264, Val Loss: 0.148099\n",
      "Epoch 465/2000, Train Loss: 0.073394, Val Loss: 0.148847\n",
      "Epoch 466/2000, Train Loss: 0.073915, Val Loss: 0.147567\n",
      "Epoch 467/2000, Train Loss: 0.073409, Val Loss: 0.147783\n",
      "Epoch 468/2000, Train Loss: 0.073475, Val Loss: 0.148883\n",
      "Epoch 469/2000, Train Loss: 0.073807, Val Loss: 0.148074\n",
      "Epoch 470/2000, Train Loss: 0.073624, Val Loss: 0.147984\n",
      "Epoch 471/2000, Train Loss: 0.073912, Val Loss: 0.149139\n",
      "Epoch 472/2000, Train Loss: 0.074839, Val Loss: 0.147170\n",
      "Epoch 473/2000, Train Loss: 0.074383, Val Loss: 0.148094\n",
      "Epoch 474/2000, Train Loss: 0.074541, Val Loss: 0.148442\n",
      "Epoch 475/2000, Train Loss: 0.074079, Val Loss: 0.148674\n",
      "Epoch 476/2000, Train Loss: 0.074585, Val Loss: 0.149032\n",
      "Epoch 477/2000, Train Loss: 0.073905, Val Loss: 0.147457\n",
      "Epoch 478/2000, Train Loss: 0.074210, Val Loss: 0.146857\n",
      "Epoch 479/2000, Train Loss: 0.074393, Val Loss: 0.148694\n",
      "Epoch 480/2000, Train Loss: 0.074076, Val Loss: 0.148025\n",
      "Epoch 481/2000, Train Loss: 0.073724, Val Loss: 0.147946\n",
      "Epoch 482/2000, Train Loss: 0.074095, Val Loss: 0.147935\n",
      "Epoch 483/2000, Train Loss: 0.073685, Val Loss: 0.148291\n",
      "Epoch 484/2000, Train Loss: 0.073766, Val Loss: 0.147566\n",
      "Epoch 485/2000, Train Loss: 0.074036, Val Loss: 0.148782\n",
      "Epoch 486/2000, Train Loss: 0.074240, Val Loss: 0.148633\n",
      "Epoch 487/2000, Train Loss: 0.073458, Val Loss: 0.148861\n",
      "Epoch 488/2000, Train Loss: 0.073616, Val Loss: 0.149259\n",
      "Epoch 489/2000, Train Loss: 0.073682, Val Loss: 0.148191\n",
      "Epoch 490/2000, Train Loss: 0.073757, Val Loss: 0.147821\n",
      "Epoch 491/2000, Train Loss: 0.073597, Val Loss: 0.148059\n",
      "Epoch 492/2000, Train Loss: 0.073368, Val Loss: 0.148552\n",
      "Epoch 493/2000, Train Loss: 0.073637, Val Loss: 0.149003\n",
      "Epoch 494/2000, Train Loss: 0.073471, Val Loss: 0.147575\n",
      "Epoch 495/2000, Train Loss: 0.073447, Val Loss: 0.148587\n",
      "Epoch 496/2000, Train Loss: 0.073732, Val Loss: 0.147741\n",
      "Epoch 497/2000, Train Loss: 0.073515, Val Loss: 0.148488\n",
      "Epoch 498/2000, Train Loss: 0.073868, Val Loss: 0.147852\n",
      "Epoch 499/2000, Train Loss: 0.073443, Val Loss: 0.148568\n",
      "Epoch 500/2000, Train Loss: 0.073645, Val Loss: 0.148443\n",
      "Epoch 501/2000, Train Loss: 0.073465, Val Loss: 0.147882\n",
      "Epoch 502/2000, Train Loss: 0.073481, Val Loss: 0.148290\n",
      "Epoch 503/2000, Train Loss: 0.073419, Val Loss: 0.147926\n",
      "Epoch 504/2000, Train Loss: 0.073360, Val Loss: 0.147879\n",
      "Epoch 505/2000, Train Loss: 0.073546, Val Loss: 0.148751\n",
      "Epoch 506/2000, Train Loss: 0.073284, Val Loss: 0.148307\n",
      "Epoch 507/2000, Train Loss: 0.073409, Val Loss: 0.147910\n",
      "Epoch 508/2000, Train Loss: 0.073180, Val Loss: 0.148908\n",
      "Epoch 509/2000, Train Loss: 0.073329, Val Loss: 0.147962\n",
      "Epoch 510/2000, Train Loss: 0.073563, Val Loss: 0.147856\n",
      "Epoch 511/2000, Train Loss: 0.073089, Val Loss: 0.148402\n",
      "Epoch 512/2000, Train Loss: 0.073245, Val Loss: 0.147878\n",
      "Epoch 513/2000, Train Loss: 0.073559, Val Loss: 0.148507\n",
      "Epoch 514/2000, Train Loss: 0.073209, Val Loss: 0.147983\n",
      "Epoch 515/2000, Train Loss: 0.073424, Val Loss: 0.147815\n",
      "Epoch 516/2000, Train Loss: 0.073021, Val Loss: 0.148273\n",
      "Epoch 517/2000, Train Loss: 0.073812, Val Loss: 0.148890\n",
      "Epoch 518/2000, Train Loss: 0.073825, Val Loss: 0.149058\n",
      "Epoch 519/2000, Train Loss: 0.073787, Val Loss: 0.147640\n",
      "Epoch 520/2000, Train Loss: 0.073788, Val Loss: 0.148369\n",
      "Epoch 521/2000, Train Loss: 0.074086, Val Loss: 0.149436\n",
      "Epoch 522/2000, Train Loss: 0.073631, Val Loss: 0.148091\n",
      "Epoch 523/2000, Train Loss: 0.073776, Val Loss: 0.148268\n",
      "Epoch 524/2000, Train Loss: 0.073829, Val Loss: 0.148540\n",
      "Epoch 525/2000, Train Loss: 0.073364, Val Loss: 0.147947\n",
      "Epoch 526/2000, Train Loss: 0.073205, Val Loss: 0.148913\n",
      "Epoch 527/2000, Train Loss: 0.073818, Val Loss: 0.148453\n",
      "Epoch 528/2000, Train Loss: 0.073434, Val Loss: 0.148143\n",
      "Epoch 529/2000, Train Loss: 0.073368, Val Loss: 0.150227\n",
      "Epoch 530/2000, Train Loss: 0.073593, Val Loss: 0.147507\n",
      "Epoch 531/2000, Train Loss: 0.074298, Val Loss: 0.147939\n",
      "Epoch 532/2000, Train Loss: 0.073432, Val Loss: 0.147631\n",
      "Epoch 533/2000, Train Loss: 0.073315, Val Loss: 0.149732\n",
      "Epoch 534/2000, Train Loss: 0.073723, Val Loss: 0.148741\n",
      "Epoch 535/2000, Train Loss: 0.073716, Val Loss: 0.148248\n",
      "Epoch 536/2000, Train Loss: 0.073971, Val Loss: 0.149036\n",
      "Epoch 537/2000, Train Loss: 0.073408, Val Loss: 0.148486\n",
      "Epoch 538/2000, Train Loss: 0.073356, Val Loss: 0.148457\n",
      "Epoch 539/2000, Train Loss: 0.073983, Val Loss: 0.150695\n",
      "Epoch 540/2000, Train Loss: 0.073853, Val Loss: 0.148815\n",
      "Epoch 541/2000, Train Loss: 0.073897, Val Loss: 0.148653\n",
      "Epoch 542/2000, Train Loss: 0.073869, Val Loss: 0.148601\n",
      "Epoch 543/2000, Train Loss: 0.073964, Val Loss: 0.148588\n",
      "Epoch 544/2000, Train Loss: 0.073938, Val Loss: 0.150134\n",
      "Epoch 545/2000, Train Loss: 0.073627, Val Loss: 0.147798\n",
      "Epoch 546/2000, Train Loss: 0.073818, Val Loss: 0.148835\n",
      "Epoch 547/2000, Train Loss: 0.073895, Val Loss: 0.148168\n",
      "Epoch 548/2000, Train Loss: 0.073749, Val Loss: 0.147097\n",
      "Epoch 549/2000, Train Loss: 0.073747, Val Loss: 0.149067\n",
      "Epoch 550/2000, Train Loss: 0.074520, Val Loss: 0.147279\n",
      "Epoch 551/2000, Train Loss: 0.074806, Val Loss: 0.148956\n",
      "Epoch 552/2000, Train Loss: 0.075174, Val Loss: 0.150389\n",
      "Epoch 553/2000, Train Loss: 0.074458, Val Loss: 0.148234\n",
      "Epoch 554/2000, Train Loss: 0.074271, Val Loss: 0.149805\n",
      "Epoch 555/2000, Train Loss: 0.074725, Val Loss: 0.147386\n",
      "Epoch 556/2000, Train Loss: 0.074713, Val Loss: 0.147912\n",
      "Epoch 557/2000, Train Loss: 0.074609, Val Loss: 0.150634\n",
      "Epoch 558/2000, Train Loss: 0.073803, Val Loss: 0.146985\n",
      "Epoch 559/2000, Train Loss: 0.073264, Val Loss: 0.149298\n",
      "Epoch 560/2000, Train Loss: 0.073734, Val Loss: 0.149008\n",
      "Epoch 561/2000, Train Loss: 0.073830, Val Loss: 0.148735\n",
      "Epoch 562/2000, Train Loss: 0.074777, Val Loss: 0.148135\n",
      "Epoch 563/2000, Train Loss: 0.075554, Val Loss: 0.148831\n",
      "Epoch 564/2000, Train Loss: 0.076628, Val Loss: 0.152757\n",
      "Epoch 565/2000, Train Loss: 0.075478, Val Loss: 0.148979\n",
      "Epoch 566/2000, Train Loss: 0.074267, Val Loss: 0.148803\n",
      "Epoch 567/2000, Train Loss: 0.073574, Val Loss: 0.147017\n",
      "Epoch 568/2000, Train Loss: 0.073517, Val Loss: 0.147004\n",
      "Epoch 569/2000, Train Loss: 0.073670, Val Loss: 0.148252\n",
      "Epoch 570/2000, Train Loss: 0.075000, Val Loss: 0.147283\n",
      "Epoch 571/2000, Train Loss: 0.073345, Val Loss: 0.148403\n",
      "Epoch 572/2000, Train Loss: 0.073683, Val Loss: 0.148348\n",
      "Epoch 573/2000, Train Loss: 0.073490, Val Loss: 0.148134\n",
      "Epoch 574/2000, Train Loss: 0.075306, Val Loss: 0.152466\n",
      "Epoch 575/2000, Train Loss: 0.076048, Val Loss: 0.148050\n",
      "Epoch 576/2000, Train Loss: 0.076173, Val Loss: 0.147845\n",
      "Epoch 577/2000, Train Loss: 0.077181, Val Loss: 0.149140\n",
      "Epoch 578/2000, Train Loss: 0.076273, Val Loss: 0.152853\n",
      "Epoch 579/2000, Train Loss: 0.077851, Val Loss: 0.152726\n",
      "Epoch 580/2000, Train Loss: 0.077468, Val Loss: 0.148208\n",
      "Epoch 581/2000, Train Loss: 0.075895, Val Loss: 0.153885\n",
      "Epoch 582/2000, Train Loss: 0.077416, Val Loss: 0.149564\n",
      "Epoch 583/2000, Train Loss: 0.076777, Val Loss: 0.149614\n",
      "Epoch 584/2000, Train Loss: 0.075878, Val Loss: 0.147570\n",
      "Epoch 585/2000, Train Loss: 0.075225, Val Loss: 0.150063\n",
      "Epoch 586/2000, Train Loss: 0.075208, Val Loss: 0.150470\n",
      "Epoch 587/2000, Train Loss: 0.074985, Val Loss: 0.149143\n",
      "Epoch 588/2000, Train Loss: 0.074863, Val Loss: 0.147197\n",
      "Epoch 589/2000, Train Loss: 0.074657, Val Loss: 0.148466\n",
      "Epoch 590/2000, Train Loss: 0.074696, Val Loss: 0.147047\n",
      "Epoch 591/2000, Train Loss: 0.073664, Val Loss: 0.147731\n",
      "Epoch 592/2000, Train Loss: 0.073647, Val Loss: 0.147979\n",
      "Epoch 593/2000, Train Loss: 0.073733, Val Loss: 0.148343\n",
      "Epoch 594/2000, Train Loss: 0.074342, Val Loss: 0.147091\n",
      "Epoch 595/2000, Train Loss: 0.074176, Val Loss: 0.152262\n",
      "Epoch 596/2000, Train Loss: 0.074826, Val Loss: 0.149014\n",
      "Epoch 597/2000, Train Loss: 0.073491, Val Loss: 0.149822\n",
      "Epoch 598/2000, Train Loss: 0.073161, Val Loss: 0.146711\n",
      "Epoch 599/2000, Train Loss: 0.073339, Val Loss: 0.148536\n",
      "Epoch 600/2000, Train Loss: 0.072950, Val Loss: 0.147954\n",
      "Epoch 601/2000, Train Loss: 0.072927, Val Loss: 0.148164\n",
      "Epoch 602/2000, Train Loss: 0.072741, Val Loss: 0.146956\n",
      "Epoch 603/2000, Train Loss: 0.072750, Val Loss: 0.148171\n",
      "Epoch 604/2000, Train Loss: 0.072861, Val Loss: 0.148051\n",
      "Epoch 605/2000, Train Loss: 0.072202, Val Loss: 0.147374\n",
      "Epoch 606/2000, Train Loss: 0.072882, Val Loss: 0.147885\n",
      "Epoch 607/2000, Train Loss: 0.075203, Val Loss: 0.147494\n",
      "Epoch 608/2000, Train Loss: 0.074267, Val Loss: 0.147292\n",
      "Epoch 609/2000, Train Loss: 0.073839, Val Loss: 0.151085\n",
      "Epoch 610/2000, Train Loss: 0.075434, Val Loss: 0.148270\n",
      "Epoch 611/2000, Train Loss: 0.073994, Val Loss: 0.148787\n",
      "Epoch 612/2000, Train Loss: 0.075317, Val Loss: 0.147456\n",
      "Epoch 613/2000, Train Loss: 0.073466, Val Loss: 0.149276\n",
      "Epoch 614/2000, Train Loss: 0.073463, Val Loss: 0.148759\n",
      "Epoch 615/2000, Train Loss: 0.073312, Val Loss: 0.148352\n",
      "Epoch 616/2000, Train Loss: 0.073199, Val Loss: 0.147602\n",
      "Epoch 617/2000, Train Loss: 0.073187, Val Loss: 0.149303\n",
      "Epoch 618/2000, Train Loss: 0.073773, Val Loss: 0.147317\n",
      "Epoch 619/2000, Train Loss: 0.072249, Val Loss: 0.149022\n",
      "Epoch 620/2000, Train Loss: 0.073241, Val Loss: 0.147539\n",
      "Epoch 621/2000, Train Loss: 0.072555, Val Loss: 0.147772\n",
      "Epoch 622/2000, Train Loss: 0.072646, Val Loss: 0.147556\n",
      "Epoch 623/2000, Train Loss: 0.072352, Val Loss: 0.147179\n",
      "Epoch 624/2000, Train Loss: 0.072850, Val Loss: 0.147943\n",
      "Epoch 625/2000, Train Loss: 0.072874, Val Loss: 0.148131\n",
      "Epoch 626/2000, Train Loss: 0.072320, Val Loss: 0.147911\n",
      "Epoch 627/2000, Train Loss: 0.072435, Val Loss: 0.147499\n",
      "Epoch 628/2000, Train Loss: 0.072714, Val Loss: 0.147628\n",
      "Epoch 629/2000, Train Loss: 0.072180, Val Loss: 0.147795\n",
      "Epoch 630/2000, Train Loss: 0.072602, Val Loss: 0.147856\n",
      "Epoch 631/2000, Train Loss: 0.073573, Val Loss: 0.148391\n",
      "Epoch 632/2000, Train Loss: 0.073153, Val Loss: 0.148333\n",
      "Epoch 633/2000, Train Loss: 0.071716, Val Loss: 0.147802\n",
      "Epoch 634/2000, Train Loss: 0.072767, Val Loss: 0.148037\n",
      "Epoch 635/2000, Train Loss: 0.072437, Val Loss: 0.147564\n",
      "Epoch 636/2000, Train Loss: 0.072222, Val Loss: 0.148009\n",
      "Epoch 637/2000, Train Loss: 0.072662, Val Loss: 0.148553\n",
      "Epoch 638/2000, Train Loss: 0.072516, Val Loss: 0.147786\n",
      "Epoch 639/2000, Train Loss: 0.071804, Val Loss: 0.148184\n",
      "Epoch 640/2000, Train Loss: 0.072381, Val Loss: 0.148258\n",
      "Epoch 641/2000, Train Loss: 0.072279, Val Loss: 0.147752\n",
      "Epoch 642/2000, Train Loss: 0.072636, Val Loss: 0.148055\n",
      "Epoch 643/2000, Train Loss: 0.072020, Val Loss: 0.148001\n",
      "Epoch 644/2000, Train Loss: 0.072290, Val Loss: 0.147391\n",
      "Epoch 645/2000, Train Loss: 0.071848, Val Loss: 0.148125\n",
      "Epoch 646/2000, Train Loss: 0.072815, Val Loss: 0.149093\n",
      "Epoch 647/2000, Train Loss: 0.072356, Val Loss: 0.147292\n",
      "Epoch 648/2000, Train Loss: 0.071903, Val Loss: 0.148023\n",
      "Epoch 649/2000, Train Loss: 0.072520, Val Loss: 0.148773\n",
      "Epoch 650/2000, Train Loss: 0.073147, Val Loss: 0.148079\n",
      "Epoch 651/2000, Train Loss: 0.072992, Val Loss: 0.148483\n",
      "Epoch 652/2000, Train Loss: 0.073136, Val Loss: 0.148963\n",
      "Epoch 653/2000, Train Loss: 0.073566, Val Loss: 0.148067\n",
      "Epoch 654/2000, Train Loss: 0.074999, Val Loss: 0.147582\n",
      "Epoch 655/2000, Train Loss: 0.074157, Val Loss: 0.148371\n",
      "Epoch 656/2000, Train Loss: 0.072872, Val Loss: 0.148001\n",
      "Epoch 657/2000, Train Loss: 0.072791, Val Loss: 0.147774\n",
      "Epoch 658/2000, Train Loss: 0.072028, Val Loss: 0.147898\n",
      "Epoch 659/2000, Train Loss: 0.072017, Val Loss: 0.148535\n",
      "Epoch 660/2000, Train Loss: 0.072616, Val Loss: 0.147754\n",
      "Epoch 661/2000, Train Loss: 0.073610, Val Loss: 0.148831\n",
      "Epoch 662/2000, Train Loss: 0.071859, Val Loss: 0.147978\n",
      "Epoch 663/2000, Train Loss: 0.073406, Val Loss: 0.147531\n",
      "Epoch 664/2000, Train Loss: 0.072724, Val Loss: 0.148869\n",
      "Epoch 665/2000, Train Loss: 0.072792, Val Loss: 0.147961\n",
      "Epoch 666/2000, Train Loss: 0.072090, Val Loss: 0.147119\n",
      "Epoch 667/2000, Train Loss: 0.072083, Val Loss: 0.148566\n",
      "Epoch 668/2000, Train Loss: 0.071561, Val Loss: 0.147612\n",
      "Epoch 669/2000, Train Loss: 0.071896, Val Loss: 0.148473\n",
      "Epoch 670/2000, Train Loss: 0.071778, Val Loss: 0.147311\n",
      "Epoch 671/2000, Train Loss: 0.071308, Val Loss: 0.147922\n",
      "Epoch 672/2000, Train Loss: 0.071887, Val Loss: 0.147670\n",
      "Epoch 673/2000, Train Loss: 0.072335, Val Loss: 0.148694\n",
      "Epoch 674/2000, Train Loss: 0.073069, Val Loss: 0.147920\n",
      "Epoch 675/2000, Train Loss: 0.073930, Val Loss: 0.149658\n",
      "Epoch 676/2000, Train Loss: 0.075297, Val Loss: 0.148719\n",
      "Epoch 677/2000, Train Loss: 0.075065, Val Loss: 0.148713\n",
      "Epoch 678/2000, Train Loss: 0.073902, Val Loss: 0.147926\n",
      "Epoch 679/2000, Train Loss: 0.073556, Val Loss: 0.148699\n",
      "Epoch 680/2000, Train Loss: 0.072768, Val Loss: 0.146717\n",
      "Epoch 681/2000, Train Loss: 0.072667, Val Loss: 0.148498\n",
      "Epoch 682/2000, Train Loss: 0.071909, Val Loss: 0.149195\n",
      "Epoch 683/2000, Train Loss: 0.071809, Val Loss: 0.148460\n",
      "Epoch 684/2000, Train Loss: 0.071586, Val Loss: 0.148573\n",
      "Epoch 685/2000, Train Loss: 0.071691, Val Loss: 0.146922\n",
      "Epoch 686/2000, Train Loss: 0.071417, Val Loss: 0.148203\n",
      "Epoch 687/2000, Train Loss: 0.071617, Val Loss: 0.147781\n",
      "Epoch 688/2000, Train Loss: 0.071344, Val Loss: 0.148190\n",
      "Epoch 689/2000, Train Loss: 0.071293, Val Loss: 0.148035\n",
      "Epoch 690/2000, Train Loss: 0.071299, Val Loss: 0.147395\n",
      "Epoch 691/2000, Train Loss: 0.071206, Val Loss: 0.148568\n",
      "Epoch 692/2000, Train Loss: 0.071194, Val Loss: 0.147968\n",
      "Epoch 693/2000, Train Loss: 0.071268, Val Loss: 0.147530\n",
      "Epoch 694/2000, Train Loss: 0.071109, Val Loss: 0.148905\n",
      "Epoch 695/2000, Train Loss: 0.071074, Val Loss: 0.147973\n",
      "Epoch 696/2000, Train Loss: 0.071266, Val Loss: 0.147621\n",
      "Epoch 697/2000, Train Loss: 0.071217, Val Loss: 0.147962\n",
      "Epoch 698/2000, Train Loss: 0.071126, Val Loss: 0.147909\n",
      "Epoch 699/2000, Train Loss: 0.071079, Val Loss: 0.148565\n",
      "Epoch 700/2000, Train Loss: 0.070955, Val Loss: 0.148044\n",
      "Epoch 701/2000, Train Loss: 0.070951, Val Loss: 0.148330\n",
      "Epoch 702/2000, Train Loss: 0.071028, Val Loss: 0.147881\n",
      "Epoch 703/2000, Train Loss: 0.071300, Val Loss: 0.148074\n",
      "Epoch 704/2000, Train Loss: 0.071082, Val Loss: 0.148191\n",
      "Epoch 705/2000, Train Loss: 0.071087, Val Loss: 0.148340\n",
      "Epoch 706/2000, Train Loss: 0.071223, Val Loss: 0.147443\n",
      "Epoch 707/2000, Train Loss: 0.071195, Val Loss: 0.148767\n",
      "Epoch 708/2000, Train Loss: 0.071348, Val Loss: 0.147401\n",
      "Epoch 709/2000, Train Loss: 0.071069, Val Loss: 0.147961\n",
      "Epoch 710/2000, Train Loss: 0.071343, Val Loss: 0.148386\n",
      "Epoch 711/2000, Train Loss: 0.071541, Val Loss: 0.147825\n",
      "Epoch 712/2000, Train Loss: 0.071120, Val Loss: 0.148012\n",
      "Epoch 713/2000, Train Loss: 0.071066, Val Loss: 0.148758\n",
      "Epoch 714/2000, Train Loss: 0.071446, Val Loss: 0.148596\n",
      "Epoch 715/2000, Train Loss: 0.071751, Val Loss: 0.147959\n",
      "Epoch 716/2000, Train Loss: 0.071717, Val Loss: 0.149648\n",
      "Epoch 717/2000, Train Loss: 0.072611, Val Loss: 0.148858\n",
      "Epoch 718/2000, Train Loss: 0.072027, Val Loss: 0.149597\n",
      "Epoch 719/2000, Train Loss: 0.071706, Val Loss: 0.147183\n",
      "Epoch 720/2000, Train Loss: 0.071217, Val Loss: 0.147538\n",
      "Epoch 721/2000, Train Loss: 0.071230, Val Loss: 0.150086\n",
      "Epoch 722/2000, Train Loss: 0.071302, Val Loss: 0.147105\n",
      "Epoch 723/2000, Train Loss: 0.071339, Val Loss: 0.148177\n",
      "Epoch 724/2000, Train Loss: 0.071022, Val Loss: 0.148421\n",
      "Epoch 725/2000, Train Loss: 0.070937, Val Loss: 0.147329\n",
      "Epoch 726/2000, Train Loss: 0.070913, Val Loss: 0.148356\n",
      "Epoch 727/2000, Train Loss: 0.070956, Val Loss: 0.147623\n",
      "Epoch 728/2000, Train Loss: 0.070907, Val Loss: 0.148836\n",
      "Epoch 729/2000, Train Loss: 0.071016, Val Loss: 0.148072\n",
      "Epoch 730/2000, Train Loss: 0.070889, Val Loss: 0.147483\n",
      "Epoch 731/2000, Train Loss: 0.070940, Val Loss: 0.147990\n",
      "Epoch 732/2000, Train Loss: 0.070944, Val Loss: 0.147771\n",
      "Epoch 733/2000, Train Loss: 0.070866, Val Loss: 0.147857\n",
      "Epoch 734/2000, Train Loss: 0.070863, Val Loss: 0.148068\n",
      "Epoch 735/2000, Train Loss: 0.070834, Val Loss: 0.147535\n",
      "Epoch 736/2000, Train Loss: 0.070809, Val Loss: 0.148363\n",
      "Epoch 737/2000, Train Loss: 0.070935, Val Loss: 0.148281\n",
      "Epoch 738/2000, Train Loss: 0.070896, Val Loss: 0.147686\n",
      "Epoch 739/2000, Train Loss: 0.070905, Val Loss: 0.147728\n",
      "Epoch 740/2000, Train Loss: 0.070893, Val Loss: 0.147838\n",
      "Epoch 741/2000, Train Loss: 0.070862, Val Loss: 0.148424\n",
      "Epoch 742/2000, Train Loss: 0.070855, Val Loss: 0.147985\n",
      "Epoch 743/2000, Train Loss: 0.070824, Val Loss: 0.147999\n",
      "Epoch 744/2000, Train Loss: 0.070875, Val Loss: 0.148085\n",
      "Epoch 745/2000, Train Loss: 0.070883, Val Loss: 0.147858\n",
      "Epoch 746/2000, Train Loss: 0.070901, Val Loss: 0.148602\n",
      "Epoch 747/2000, Train Loss: 0.070905, Val Loss: 0.147655\n",
      "Epoch 748/2000, Train Loss: 0.070823, Val Loss: 0.148465\n",
      "Epoch 749/2000, Train Loss: 0.070866, Val Loss: 0.147609\n",
      "Epoch 750/2000, Train Loss: 0.071054, Val Loss: 0.147391\n",
      "Epoch 751/2000, Train Loss: 0.071238, Val Loss: 0.149341\n",
      "Epoch 752/2000, Train Loss: 0.071457, Val Loss: 0.146510\n",
      "Epoch 753/2000, Train Loss: 0.071171, Val Loss: 0.148138\n",
      "Epoch 754/2000, Train Loss: 0.071700, Val Loss: 0.147959\n",
      "Epoch 755/2000, Train Loss: 0.072756, Val Loss: 0.147931\n",
      "Epoch 756/2000, Train Loss: 0.074124, Val Loss: 0.152038\n",
      "Epoch 757/2000, Train Loss: 0.072923, Val Loss: 0.149901\n",
      "Epoch 758/2000, Train Loss: 0.075790, Val Loss: 0.148767\n",
      "Epoch 759/2000, Train Loss: 0.076796, Val Loss: 0.150501\n",
      "Epoch 760/2000, Train Loss: 0.076753, Val Loss: 0.148320\n",
      "Epoch 761/2000, Train Loss: 0.075435, Val Loss: 0.149367\n",
      "Epoch 762/2000, Train Loss: 0.074696, Val Loss: 0.149273\n",
      "Epoch 763/2000, Train Loss: 0.074179, Val Loss: 0.148176\n",
      "Epoch 764/2000, Train Loss: 0.073690, Val Loss: 0.147972\n",
      "Epoch 765/2000, Train Loss: 0.073433, Val Loss: 0.147430\n",
      "Epoch 766/2000, Train Loss: 0.073274, Val Loss: 0.148609\n",
      "Epoch 767/2000, Train Loss: 0.073477, Val Loss: 0.148497\n",
      "Epoch 768/2000, Train Loss: 0.073458, Val Loss: 0.148206\n",
      "Epoch 769/2000, Train Loss: 0.073012, Val Loss: 0.148285\n",
      "Epoch 770/2000, Train Loss: 0.072949, Val Loss: 0.148023\n",
      "Epoch 771/2000, Train Loss: 0.072785, Val Loss: 0.148218\n",
      "Epoch 772/2000, Train Loss: 0.072684, Val Loss: 0.148045\n",
      "Epoch 773/2000, Train Loss: 0.072565, Val Loss: 0.148037\n",
      "Epoch 774/2000, Train Loss: 0.072516, Val Loss: 0.148145\n",
      "Epoch 775/2000, Train Loss: 0.072336, Val Loss: 0.147617\n",
      "Epoch 776/2000, Train Loss: 0.071842, Val Loss: 0.149827\n",
      "Epoch 777/2000, Train Loss: 0.071746, Val Loss: 0.147475\n",
      "Epoch 778/2000, Train Loss: 0.071451, Val Loss: 0.147992\n",
      "Epoch 779/2000, Train Loss: 0.071356, Val Loss: 0.149131\n",
      "Epoch 780/2000, Train Loss: 0.071096, Val Loss: 0.148991\n",
      "Epoch 781/2000, Train Loss: 0.071425, Val Loss: 0.148262\n",
      "Epoch 782/2000, Train Loss: 0.071513, Val Loss: 0.150101\n",
      "Epoch 783/2000, Train Loss: 0.072619, Val Loss: 0.148263\n",
      "Epoch 784/2000, Train Loss: 0.071462, Val Loss: 0.150052\n",
      "Epoch 785/2000, Train Loss: 0.071690, Val Loss: 0.149736\n",
      "Epoch 786/2000, Train Loss: 0.071819, Val Loss: 0.148844\n",
      "Epoch 787/2000, Train Loss: 0.071628, Val Loss: 0.150010\n",
      "Epoch 788/2000, Train Loss: 0.071746, Val Loss: 0.148774\n",
      "Epoch 789/2000, Train Loss: 0.071385, Val Loss: 0.148117\n",
      "Epoch 790/2000, Train Loss: 0.071759, Val Loss: 0.150447\n",
      "Epoch 791/2000, Train Loss: 0.071528, Val Loss: 0.149537\n",
      "Epoch 792/2000, Train Loss: 0.073188, Val Loss: 0.148343\n",
      "Epoch 793/2000, Train Loss: 0.073169, Val Loss: 0.147771\n",
      "Epoch 794/2000, Train Loss: 0.073203, Val Loss: 0.149795\n",
      "Epoch 795/2000, Train Loss: 0.073186, Val Loss: 0.150878\n",
      "Epoch 796/2000, Train Loss: 0.075760, Val Loss: 0.151180\n",
      "Epoch 797/2000, Train Loss: 0.076002, Val Loss: 0.149824\n",
      "Epoch 798/2000, Train Loss: 0.073972, Val Loss: 0.149062\n",
      "Epoch 799/2000, Train Loss: 0.072749, Val Loss: 0.148458\n",
      "Epoch 800/2000, Train Loss: 0.072702, Val Loss: 0.148454\n",
      "Epoch 801/2000, Train Loss: 0.072163, Val Loss: 0.148693\n",
      "Epoch 802/2000, Train Loss: 0.071664, Val Loss: 0.147888\n",
      "Epoch 803/2000, Train Loss: 0.071466, Val Loss: 0.147946\n",
      "Epoch 804/2000, Train Loss: 0.071392, Val Loss: 0.148548\n",
      "Epoch 805/2000, Train Loss: 0.071490, Val Loss: 0.149308\n",
      "Epoch 806/2000, Train Loss: 0.072005, Val Loss: 0.146826\n",
      "Epoch 807/2000, Train Loss: 0.071700, Val Loss: 0.148933\n",
      "Epoch 808/2000, Train Loss: 0.071229, Val Loss: 0.148404\n",
      "Epoch 809/2000, Train Loss: 0.071269, Val Loss: 0.148869\n",
      "Epoch 810/2000, Train Loss: 0.070997, Val Loss: 0.148645\n",
      "Epoch 811/2000, Train Loss: 0.070932, Val Loss: 0.148092\n",
      "Epoch 812/2000, Train Loss: 0.070855, Val Loss: 0.148764\n",
      "Epoch 813/2000, Train Loss: 0.070803, Val Loss: 0.148293\n",
      "Epoch 814/2000, Train Loss: 0.070773, Val Loss: 0.148433\n",
      "Epoch 815/2000, Train Loss: 0.070750, Val Loss: 0.148446\n",
      "Epoch 816/2000, Train Loss: 0.070755, Val Loss: 0.148316\n",
      "Epoch 817/2000, Train Loss: 0.070741, Val Loss: 0.148060\n",
      "Epoch 818/2000, Train Loss: 0.070724, Val Loss: 0.148542\n",
      "Epoch 819/2000, Train Loss: 0.070729, Val Loss: 0.148402\n",
      "Epoch 820/2000, Train Loss: 0.070715, Val Loss: 0.148608\n",
      "Epoch 821/2000, Train Loss: 0.070715, Val Loss: 0.148414\n",
      "Epoch 822/2000, Train Loss: 0.070706, Val Loss: 0.148191\n",
      "Epoch 823/2000, Train Loss: 0.070703, Val Loss: 0.148583\n",
      "Epoch 824/2000, Train Loss: 0.070703, Val Loss: 0.148578\n",
      "Epoch 825/2000, Train Loss: 0.070695, Val Loss: 0.148449\n",
      "Epoch 826/2000, Train Loss: 0.070691, Val Loss: 0.148524\n",
      "Epoch 827/2000, Train Loss: 0.070690, Val Loss: 0.148555\n",
      "Epoch 828/2000, Train Loss: 0.070689, Val Loss: 0.148550\n",
      "Epoch 829/2000, Train Loss: 0.070693, Val Loss: 0.148469\n",
      "Epoch 830/2000, Train Loss: 0.070683, Val Loss: 0.148555\n",
      "Epoch 831/2000, Train Loss: 0.070684, Val Loss: 0.148470\n",
      "Epoch 832/2000, Train Loss: 0.070680, Val Loss: 0.148607\n",
      "Epoch 833/2000, Train Loss: 0.070678, Val Loss: 0.148549\n",
      "Epoch 834/2000, Train Loss: 0.070677, Val Loss: 0.148489\n",
      "Epoch 835/2000, Train Loss: 0.070677, Val Loss: 0.148642\n",
      "Epoch 836/2000, Train Loss: 0.070675, Val Loss: 0.148515\n",
      "Epoch 837/2000, Train Loss: 0.070678, Val Loss: 0.148558\n",
      "Epoch 838/2000, Train Loss: 0.070676, Val Loss: 0.148649\n",
      "Epoch 839/2000, Train Loss: 0.070674, Val Loss: 0.148367\n",
      "Epoch 840/2000, Train Loss: 0.070677, Val Loss: 0.148674\n",
      "Epoch 841/2000, Train Loss: 0.070678, Val Loss: 0.148501\n",
      "Epoch 842/2000, Train Loss: 0.070669, Val Loss: 0.148731\n",
      "Epoch 843/2000, Train Loss: 0.070666, Val Loss: 0.148451\n",
      "Epoch 844/2000, Train Loss: 0.070666, Val Loss: 0.148619\n",
      "Epoch 845/2000, Train Loss: 0.070666, Val Loss: 0.148654\n",
      "Epoch 846/2000, Train Loss: 0.070667, Val Loss: 0.148717\n",
      "Epoch 847/2000, Train Loss: 0.070661, Val Loss: 0.148440\n",
      "Epoch 848/2000, Train Loss: 0.070667, Val Loss: 0.148679\n",
      "Epoch 849/2000, Train Loss: 0.070663, Val Loss: 0.148643\n",
      "Epoch 850/2000, Train Loss: 0.070674, Val Loss: 0.148614\n",
      "Epoch 851/2000, Train Loss: 0.070665, Val Loss: 0.148792\n",
      "Epoch 852/2000, Train Loss: 0.070669, Val Loss: 0.148445\n",
      "Epoch 853/2000, Train Loss: 0.070670, Val Loss: 0.148854\n",
      "Epoch 854/2000, Train Loss: 0.070682, Val Loss: 0.148584\n",
      "Epoch 855/2000, Train Loss: 0.070668, Val Loss: 0.148594\n",
      "Epoch 856/2000, Train Loss: 0.070665, Val Loss: 0.148757\n",
      "Epoch 857/2000, Train Loss: 0.070675, Val Loss: 0.148516\n",
      "Epoch 858/2000, Train Loss: 0.070682, Val Loss: 0.148813\n",
      "Epoch 859/2000, Train Loss: 0.070679, Val Loss: 0.148212\n",
      "Epoch 860/2000, Train Loss: 0.070675, Val Loss: 0.148556\n",
      "Epoch 861/2000, Train Loss: 0.070680, Val Loss: 0.148842\n",
      "Epoch 862/2000, Train Loss: 0.070702, Val Loss: 0.148737\n",
      "Epoch 863/2000, Train Loss: 0.070735, Val Loss: 0.149166\n",
      "Epoch 864/2000, Train Loss: 0.070716, Val Loss: 0.148431\n",
      "Epoch 865/2000, Train Loss: 0.070778, Val Loss: 0.149825\n",
      "Epoch 866/2000, Train Loss: 0.070776, Val Loss: 0.147670\n",
      "Epoch 867/2000, Train Loss: 0.070759, Val Loss: 0.148814\n",
      "Epoch 868/2000, Train Loss: 0.070747, Val Loss: 0.148980\n",
      "Epoch 869/2000, Train Loss: 0.070746, Val Loss: 0.148328\n",
      "Epoch 870/2000, Train Loss: 0.070695, Val Loss: 0.149323\n",
      "Epoch 871/2000, Train Loss: 0.070673, Val Loss: 0.148694\n",
      "Epoch 872/2000, Train Loss: 0.070661, Val Loss: 0.148781\n",
      "Epoch 873/2000, Train Loss: 0.070652, Val Loss: 0.148498\n",
      "Epoch 874/2000, Train Loss: 0.070649, Val Loss: 0.148643\n",
      "Epoch 875/2000, Train Loss: 0.070666, Val Loss: 0.148679\n",
      "Epoch 876/2000, Train Loss: 0.070673, Val Loss: 0.148945\n",
      "Epoch 877/2000, Train Loss: 0.070677, Val Loss: 0.148536\n",
      "Epoch 878/2000, Train Loss: 0.070696, Val Loss: 0.148551\n",
      "Epoch 879/2000, Train Loss: 0.070742, Val Loss: 0.148959\n",
      "Epoch 880/2000, Train Loss: 0.070808, Val Loss: 0.148770\n",
      "Epoch 881/2000, Train Loss: 0.070829, Val Loss: 0.149130\n",
      "Epoch 882/2000, Train Loss: 0.070831, Val Loss: 0.149502\n",
      "Epoch 883/2000, Train Loss: 0.070847, Val Loss: 0.147973\n",
      "Epoch 884/2000, Train Loss: 0.070855, Val Loss: 0.149817\n",
      "Epoch 885/2000, Train Loss: 0.070736, Val Loss: 0.148546\n",
      "Epoch 886/2000, Train Loss: 0.070767, Val Loss: 0.149946\n",
      "Epoch 887/2000, Train Loss: 0.070844, Val Loss: 0.148216\n",
      "Epoch 888/2000, Train Loss: 0.070841, Val Loss: 0.148154\n",
      "Epoch 889/2000, Train Loss: 0.070856, Val Loss: 0.149108\n",
      "Epoch 890/2000, Train Loss: 0.070939, Val Loss: 0.148162\n",
      "Epoch 891/2000, Train Loss: 0.070993, Val Loss: 0.149338\n",
      "Epoch 892/2000, Train Loss: 0.070963, Val Loss: 0.148800\n",
      "Epoch 893/2000, Train Loss: 0.070889, Val Loss: 0.148621\n",
      "Epoch 894/2000, Train Loss: 0.071097, Val Loss: 0.147524\n",
      "Epoch 895/2000, Train Loss: 0.070857, Val Loss: 0.149354\n",
      "Epoch 896/2000, Train Loss: 0.070871, Val Loss: 0.148537\n",
      "Epoch 897/2000, Train Loss: 0.070888, Val Loss: 0.149114\n",
      "Epoch 898/2000, Train Loss: 0.070823, Val Loss: 0.148362\n",
      "Epoch 899/2000, Train Loss: 0.070746, Val Loss: 0.148813\n",
      "Epoch 900/2000, Train Loss: 0.070762, Val Loss: 0.148958\n",
      "Epoch 901/2000, Train Loss: 0.070763, Val Loss: 0.148799\n",
      "Epoch 902/2000, Train Loss: 0.070819, Val Loss: 0.149172\n",
      "Epoch 903/2000, Train Loss: 0.070788, Val Loss: 0.148869\n",
      "Epoch 904/2000, Train Loss: 0.070946, Val Loss: 0.148659\n",
      "Epoch 905/2000, Train Loss: 0.070872, Val Loss: 0.148768\n",
      "Epoch 906/2000, Train Loss: 0.070850, Val Loss: 0.148545\n",
      "Epoch 907/2000, Train Loss: 0.070811, Val Loss: 0.148765\n",
      "Epoch 908/2000, Train Loss: 0.070779, Val Loss: 0.148990\n",
      "Epoch 909/2000, Train Loss: 0.070809, Val Loss: 0.148427\n",
      "Epoch 910/2000, Train Loss: 0.070815, Val Loss: 0.148028\n",
      "Epoch 911/2000, Train Loss: 0.071025, Val Loss: 0.150200\n",
      "Epoch 912/2000, Train Loss: 0.070957, Val Loss: 0.148892\n",
      "Epoch 913/2000, Train Loss: 0.071126, Val Loss: 0.148920\n",
      "Epoch 914/2000, Train Loss: 0.071129, Val Loss: 0.150257\n",
      "Epoch 915/2000, Train Loss: 0.071575, Val Loss: 0.148311\n",
      "Epoch 916/2000, Train Loss: 0.071254, Val Loss: 0.148520\n",
      "Epoch 917/2000, Train Loss: 0.071806, Val Loss: 0.152528\n",
      "Epoch 918/2000, Train Loss: 0.072280, Val Loss: 0.147712\n",
      "Epoch 919/2000, Train Loss: 0.072654, Val Loss: 0.152484\n",
      "Epoch 920/2000, Train Loss: 0.072069, Val Loss: 0.149671\n",
      "Epoch 921/2000, Train Loss: 0.073095, Val Loss: 0.148715\n",
      "Epoch 922/2000, Train Loss: 0.071818, Val Loss: 0.148337\n",
      "Epoch 923/2000, Train Loss: 0.072925, Val Loss: 0.150148\n",
      "Epoch 924/2000, Train Loss: 0.074260, Val Loss: 0.148583\n",
      "Epoch 925/2000, Train Loss: 0.073657, Val Loss: 0.149066\n",
      "Epoch 926/2000, Train Loss: 0.073211, Val Loss: 0.147135\n",
      "Epoch 927/2000, Train Loss: 0.073269, Val Loss: 0.149523\n",
      "Epoch 928/2000, Train Loss: 0.072789, Val Loss: 0.147688\n",
      "Epoch 929/2000, Train Loss: 0.072772, Val Loss: 0.148042\n",
      "Epoch 930/2000, Train Loss: 0.071271, Val Loss: 0.149235\n",
      "Epoch 931/2000, Train Loss: 0.071298, Val Loss: 0.148362\n",
      "Epoch 932/2000, Train Loss: 0.070989, Val Loss: 0.149176\n",
      "Epoch 933/2000, Train Loss: 0.070927, Val Loss: 0.148393\n",
      "Epoch 934/2000, Train Loss: 0.070839, Val Loss: 0.148309\n",
      "Epoch 935/2000, Train Loss: 0.070806, Val Loss: 0.149255\n",
      "Epoch 936/2000, Train Loss: 0.070765, Val Loss: 0.147988\n",
      "Epoch 937/2000, Train Loss: 0.070747, Val Loss: 0.148238\n",
      "Epoch 938/2000, Train Loss: 0.070702, Val Loss: 0.148980\n",
      "Epoch 939/2000, Train Loss: 0.070698, Val Loss: 0.148171\n",
      "Epoch 940/2000, Train Loss: 0.070699, Val Loss: 0.148881\n",
      "Epoch 941/2000, Train Loss: 0.070667, Val Loss: 0.148704\n",
      "Epoch 942/2000, Train Loss: 0.070665, Val Loss: 0.148585\n",
      "Epoch 943/2000, Train Loss: 0.070653, Val Loss: 0.148708\n",
      "Epoch 944/2000, Train Loss: 0.070644, Val Loss: 0.148447\n",
      "Epoch 945/2000, Train Loss: 0.070641, Val Loss: 0.148564\n",
      "Epoch 946/2000, Train Loss: 0.070651, Val Loss: 0.148755\n",
      "Epoch 947/2000, Train Loss: 0.070632, Val Loss: 0.148685\n",
      "Epoch 948/2000, Train Loss: 0.070635, Val Loss: 0.148797\n",
      "Epoch 949/2000, Train Loss: 0.070628, Val Loss: 0.148613\n",
      "Epoch 950/2000, Train Loss: 0.070630, Val Loss: 0.148500\n",
      "Epoch 951/2000, Train Loss: 0.070628, Val Loss: 0.148644\n",
      "Epoch 952/2000, Train Loss: 0.070632, Val Loss: 0.148628\n",
      "Epoch 953/2000, Train Loss: 0.070637, Val Loss: 0.149193\n",
      "Epoch 954/2000, Train Loss: 0.070630, Val Loss: 0.148661\n",
      "Epoch 955/2000, Train Loss: 0.070625, Val Loss: 0.148379\n",
      "Epoch 956/2000, Train Loss: 0.070622, Val Loss: 0.148742\n",
      "Epoch 957/2000, Train Loss: 0.070615, Val Loss: 0.148453\n",
      "Epoch 958/2000, Train Loss: 0.070622, Val Loss: 0.149013\n",
      "Epoch 959/2000, Train Loss: 0.070644, Val Loss: 0.148829\n",
      "Epoch 960/2000, Train Loss: 0.070627, Val Loss: 0.148323\n",
      "Epoch 961/2000, Train Loss: 0.070638, Val Loss: 0.148999\n",
      "Epoch 962/2000, Train Loss: 0.070617, Val Loss: 0.148503\n",
      "Epoch 963/2000, Train Loss: 0.070624, Val Loss: 0.148619\n",
      "Epoch 964/2000, Train Loss: 0.070616, Val Loss: 0.148814\n",
      "Epoch 965/2000, Train Loss: 0.070619, Val Loss: 0.148675\n",
      "Epoch 966/2000, Train Loss: 0.070617, Val Loss: 0.148598\n",
      "Epoch 967/2000, Train Loss: 0.070624, Val Loss: 0.148881\n",
      "Epoch 968/2000, Train Loss: 0.070631, Val Loss: 0.148964\n",
      "Epoch 969/2000, Train Loss: 0.070628, Val Loss: 0.148538\n",
      "Epoch 970/2000, Train Loss: 0.070676, Val Loss: 0.148583\n",
      "Epoch 971/2000, Train Loss: 0.070658, Val Loss: 0.148761\n",
      "Epoch 972/2000, Train Loss: 0.070629, Val Loss: 0.148829\n",
      "Epoch 973/2000, Train Loss: 0.070631, Val Loss: 0.148878\n",
      "Epoch 974/2000, Train Loss: 0.070647, Val Loss: 0.148399\n",
      "Epoch 975/2000, Train Loss: 0.070638, Val Loss: 0.149368\n",
      "Epoch 976/2000, Train Loss: 0.070632, Val Loss: 0.148705\n",
      "Epoch 977/2000, Train Loss: 0.070640, Val Loss: 0.148805\n",
      "Epoch 978/2000, Train Loss: 0.070654, Val Loss: 0.148984\n",
      "Epoch 979/2000, Train Loss: 0.070643, Val Loss: 0.148458\n",
      "Epoch 980/2000, Train Loss: 0.070641, Val Loss: 0.149144\n",
      "Epoch 981/2000, Train Loss: 0.070646, Val Loss: 0.148555\n",
      "Epoch 982/2000, Train Loss: 0.070634, Val Loss: 0.149286\n",
      "Epoch 983/2000, Train Loss: 0.070645, Val Loss: 0.148500\n",
      "Epoch 984/2000, Train Loss: 0.070704, Val Loss: 0.148783\n",
      "Epoch 985/2000, Train Loss: 0.070775, Val Loss: 0.149068\n",
      "Epoch 986/2000, Train Loss: 0.070867, Val Loss: 0.149230\n",
      "Epoch 987/2000, Train Loss: 0.070832, Val Loss: 0.148608\n",
      "Epoch 988/2000, Train Loss: 0.070907, Val Loss: 0.149155\n",
      "Epoch 989/2000, Train Loss: 0.070871, Val Loss: 0.148447\n",
      "Epoch 990/2000, Train Loss: 0.070734, Val Loss: 0.149098\n",
      "Epoch 991/2000, Train Loss: 0.070697, Val Loss: 0.148727\n",
      "Epoch 992/2000, Train Loss: 0.070647, Val Loss: 0.148863\n",
      "Epoch 993/2000, Train Loss: 0.070677, Val Loss: 0.149042\n",
      "Epoch 994/2000, Train Loss: 0.070671, Val Loss: 0.148444\n",
      "Epoch 995/2000, Train Loss: 0.070621, Val Loss: 0.148944\n",
      "Epoch 996/2000, Train Loss: 0.070634, Val Loss: 0.148471\n",
      "Epoch 997/2000, Train Loss: 0.070635, Val Loss: 0.149109\n",
      "Epoch 998/2000, Train Loss: 0.070636, Val Loss: 0.148443\n",
      "Epoch 999/2000, Train Loss: 0.070638, Val Loss: 0.148924\n",
      "Epoch 1000/2000, Train Loss: 0.070641, Val Loss: 0.149172\n",
      "Epoch 1001/2000, Train Loss: 0.070654, Val Loss: 0.148569\n",
      "Epoch 1002/2000, Train Loss: 0.070722, Val Loss: 0.148764\n",
      "Epoch 1003/2000, Train Loss: 0.070755, Val Loss: 0.148942\n",
      "Epoch 1004/2000, Train Loss: 0.070885, Val Loss: 0.149050\n",
      "Epoch 1005/2000, Train Loss: 0.070836, Val Loss: 0.149717\n",
      "Epoch 1006/2000, Train Loss: 0.070907, Val Loss: 0.148938\n",
      "Epoch 1007/2000, Train Loss: 0.070822, Val Loss: 0.148726\n",
      "Epoch 1008/2000, Train Loss: 0.070761, Val Loss: 0.148761\n",
      "Epoch 1009/2000, Train Loss: 0.070726, Val Loss: 0.148974\n",
      "Epoch 1010/2000, Train Loss: 0.070666, Val Loss: 0.148596\n",
      "Epoch 1011/2000, Train Loss: 0.070677, Val Loss: 0.148905\n",
      "Epoch 1012/2000, Train Loss: 0.070658, Val Loss: 0.149288\n",
      "Epoch 1013/2000, Train Loss: 0.070641, Val Loss: 0.148418\n",
      "Epoch 1014/2000, Train Loss: 0.070652, Val Loss: 0.149178\n",
      "Epoch 1015/2000, Train Loss: 0.070643, Val Loss: 0.148472\n",
      "Epoch 1016/2000, Train Loss: 0.070642, Val Loss: 0.148758\n",
      "Epoch 1017/2000, Train Loss: 0.070649, Val Loss: 0.148854\n",
      "Epoch 1018/2000, Train Loss: 0.070686, Val Loss: 0.149750\n",
      "Epoch 1019/2000, Train Loss: 0.070692, Val Loss: 0.148344\n",
      "Epoch 1020/2000, Train Loss: 0.070662, Val Loss: 0.149760\n",
      "Epoch 1021/2000, Train Loss: 0.070766, Val Loss: 0.149309\n",
      "Epoch 1022/2000, Train Loss: 0.070939, Val Loss: 0.147386\n",
      "Epoch 1023/2000, Train Loss: 0.071390, Val Loss: 0.152289\n",
      "Epoch 1024/2000, Train Loss: 0.071644, Val Loss: 0.149316\n",
      "Epoch 1025/2000, Train Loss: 0.071283, Val Loss: 0.148133\n",
      "Epoch 1026/2000, Train Loss: 0.071232, Val Loss: 0.148962\n",
      "Epoch 1027/2000, Train Loss: 0.071400, Val Loss: 0.149747\n",
      "Epoch 1028/2000, Train Loss: 0.071700, Val Loss: 0.147628\n",
      "Epoch 1029/2000, Train Loss: 0.072555, Val Loss: 0.156282\n",
      "Epoch 1030/2000, Train Loss: 0.075912, Val Loss: 0.149748\n",
      "Epoch 1031/2000, Train Loss: 0.080232, Val Loss: 0.154568\n",
      "Epoch 1032/2000, Train Loss: 0.076732, Val Loss: 0.150649\n",
      "Epoch 1033/2000, Train Loss: 0.075517, Val Loss: 0.148625\n",
      "Epoch 1034/2000, Train Loss: 0.074325, Val Loss: 0.149740\n",
      "Epoch 1035/2000, Train Loss: 0.075262, Val Loss: 0.149650\n",
      "Epoch 1036/2000, Train Loss: 0.076748, Val Loss: 0.150378\n",
      "Epoch 1037/2000, Train Loss: 0.076894, Val Loss: 0.151364\n",
      "Epoch 1038/2000, Train Loss: 0.076433, Val Loss: 0.149125\n",
      "Epoch 1039/2000, Train Loss: 0.074788, Val Loss: 0.148470\n",
      "Epoch 1040/2000, Train Loss: 0.073037, Val Loss: 0.147741\n",
      "Epoch 1041/2000, Train Loss: 0.072113, Val Loss: 0.148431\n",
      "Epoch 1042/2000, Train Loss: 0.071710, Val Loss: 0.149101\n",
      "Epoch 1043/2000, Train Loss: 0.071347, Val Loss: 0.148064\n",
      "Epoch 1044/2000, Train Loss: 0.071159, Val Loss: 0.147805\n",
      "Epoch 1045/2000, Train Loss: 0.070980, Val Loss: 0.147459\n",
      "Epoch 1046/2000, Train Loss: 0.070927, Val Loss: 0.148397\n",
      "Epoch 1047/2000, Train Loss: 0.070884, Val Loss: 0.148362\n",
      "Epoch 1048/2000, Train Loss: 0.070802, Val Loss: 0.148515\n",
      "Epoch 1049/2000, Train Loss: 0.070786, Val Loss: 0.148242\n",
      "Epoch 1050/2000, Train Loss: 0.070752, Val Loss: 0.148176\n",
      "Epoch 1051/2000, Train Loss: 0.070732, Val Loss: 0.148317\n",
      "Epoch 1052/2000, Train Loss: 0.070725, Val Loss: 0.148220\n",
      "Epoch 1053/2000, Train Loss: 0.070701, Val Loss: 0.148514\n",
      "Epoch 1054/2000, Train Loss: 0.070697, Val Loss: 0.148421\n",
      "Epoch 1055/2000, Train Loss: 0.070687, Val Loss: 0.148367\n",
      "Epoch 1056/2000, Train Loss: 0.070686, Val Loss: 0.148373\n",
      "Epoch 1057/2000, Train Loss: 0.070674, Val Loss: 0.148300\n",
      "Epoch 1058/2000, Train Loss: 0.070666, Val Loss: 0.148162\n",
      "Epoch 1059/2000, Train Loss: 0.070665, Val Loss: 0.148437\n",
      "Epoch 1060/2000, Train Loss: 0.070650, Val Loss: 0.148265\n",
      "Epoch 1061/2000, Train Loss: 0.070653, Val Loss: 0.148444\n",
      "Epoch 1062/2000, Train Loss: 0.070645, Val Loss: 0.148576\n",
      "Epoch 1063/2000, Train Loss: 0.070639, Val Loss: 0.148154\n",
      "Epoch 1064/2000, Train Loss: 0.070637, Val Loss: 0.148591\n",
      "Epoch 1065/2000, Train Loss: 0.070635, Val Loss: 0.148490\n",
      "Epoch 1066/2000, Train Loss: 0.070630, Val Loss: 0.148506\n",
      "Epoch 1067/2000, Train Loss: 0.070630, Val Loss: 0.148512\n",
      "Epoch 1068/2000, Train Loss: 0.070622, Val Loss: 0.148462\n",
      "Epoch 1069/2000, Train Loss: 0.070622, Val Loss: 0.148536\n",
      "Epoch 1070/2000, Train Loss: 0.070623, Val Loss: 0.148514\n",
      "Epoch 1071/2000, Train Loss: 0.070625, Val Loss: 0.148720\n",
      "Epoch 1072/2000, Train Loss: 0.070616, Val Loss: 0.148336\n",
      "Epoch 1073/2000, Train Loss: 0.070620, Val Loss: 0.148692\n",
      "Epoch 1074/2000, Train Loss: 0.070621, Val Loss: 0.148617\n",
      "Epoch 1075/2000, Train Loss: 0.070618, Val Loss: 0.148600\n",
      "Epoch 1076/2000, Train Loss: 0.070625, Val Loss: 0.148626\n",
      "Epoch 1077/2000, Train Loss: 0.070627, Val Loss: 0.148834\n",
      "Epoch 1078/2000, Train Loss: 0.070634, Val Loss: 0.148448\n",
      "Epoch 1079/2000, Train Loss: 0.070633, Val Loss: 0.148835\n",
      "Epoch 1080/2000, Train Loss: 0.070625, Val Loss: 0.148116\n",
      "Epoch 1081/2000, Train Loss: 0.070624, Val Loss: 0.148777\n",
      "Epoch 1082/2000, Train Loss: 0.070637, Val Loss: 0.148708\n",
      "Epoch 1083/2000, Train Loss: 0.070622, Val Loss: 0.148415\n",
      "Epoch 1084/2000, Train Loss: 0.070613, Val Loss: 0.148900\n",
      "Epoch 1085/2000, Train Loss: 0.070608, Val Loss: 0.148671\n",
      "Epoch 1086/2000, Train Loss: 0.070604, Val Loss: 0.148792\n",
      "Epoch 1087/2000, Train Loss: 0.070597, Val Loss: 0.148867\n",
      "Epoch 1088/2000, Train Loss: 0.070593, Val Loss: 0.148704\n",
      "Epoch 1089/2000, Train Loss: 0.070590, Val Loss: 0.148739\n",
      "Epoch 1090/2000, Train Loss: 0.070589, Val Loss: 0.148719\n",
      "Epoch 1091/2000, Train Loss: 0.070589, Val Loss: 0.148880\n",
      "Epoch 1092/2000, Train Loss: 0.070587, Val Loss: 0.148781\n",
      "Epoch 1093/2000, Train Loss: 0.070586, Val Loss: 0.148789\n",
      "Epoch 1094/2000, Train Loss: 0.070585, Val Loss: 0.148764\n",
      "Epoch 1095/2000, Train Loss: 0.070586, Val Loss: 0.148922\n",
      "Epoch 1096/2000, Train Loss: 0.070594, Val Loss: 0.148845\n",
      "Epoch 1097/2000, Train Loss: 0.070606, Val Loss: 0.148632\n",
      "Epoch 1098/2000, Train Loss: 0.070615, Val Loss: 0.148907\n",
      "Epoch 1099/2000, Train Loss: 0.070619, Val Loss: 0.148623\n",
      "Epoch 1100/2000, Train Loss: 0.070606, Val Loss: 0.148982\n",
      "Epoch 1101/2000, Train Loss: 0.070597, Val Loss: 0.149014\n",
      "Epoch 1102/2000, Train Loss: 0.070593, Val Loss: 0.148685\n",
      "Epoch 1103/2000, Train Loss: 0.070592, Val Loss: 0.148954\n",
      "Epoch 1104/2000, Train Loss: 0.070604, Val Loss: 0.148807\n",
      "Epoch 1105/2000, Train Loss: 0.070607, Val Loss: 0.148910\n",
      "Epoch 1106/2000, Train Loss: 0.070597, Val Loss: 0.149049\n",
      "Epoch 1107/2000, Train Loss: 0.070600, Val Loss: 0.148860\n",
      "Epoch 1108/2000, Train Loss: 0.070607, Val Loss: 0.148479\n",
      "Epoch 1109/2000, Train Loss: 0.070603, Val Loss: 0.149290\n",
      "Epoch 1110/2000, Train Loss: 0.070601, Val Loss: 0.149000\n",
      "Epoch 1111/2000, Train Loss: 0.070584, Val Loss: 0.148846\n",
      "Epoch 1112/2000, Train Loss: 0.070581, Val Loss: 0.148901\n",
      "Epoch 1113/2000, Train Loss: 0.070577, Val Loss: 0.148843\n",
      "Epoch 1114/2000, Train Loss: 0.070575, Val Loss: 0.148918\n",
      "Epoch 1115/2000, Train Loss: 0.070579, Val Loss: 0.148921\n",
      "Epoch 1116/2000, Train Loss: 0.070576, Val Loss: 0.149013\n",
      "Epoch 1117/2000, Train Loss: 0.070575, Val Loss: 0.148991\n",
      "Epoch 1118/2000, Train Loss: 0.070574, Val Loss: 0.148836\n",
      "Epoch 1119/2000, Train Loss: 0.070573, Val Loss: 0.148919\n",
      "Epoch 1120/2000, Train Loss: 0.070568, Val Loss: 0.148989\n",
      "Epoch 1121/2000, Train Loss: 0.070566, Val Loss: 0.148891\n",
      "Epoch 1122/2000, Train Loss: 0.070569, Val Loss: 0.149086\n",
      "Epoch 1123/2000, Train Loss: 0.070570, Val Loss: 0.149104\n",
      "Epoch 1124/2000, Train Loss: 0.070581, Val Loss: 0.149064\n",
      "Epoch 1125/2000, Train Loss: 0.070590, Val Loss: 0.148776\n",
      "Epoch 1126/2000, Train Loss: 0.070610, Val Loss: 0.149321\n",
      "Epoch 1127/2000, Train Loss: 0.070644, Val Loss: 0.149082\n",
      "Epoch 1128/2000, Train Loss: 0.070687, Val Loss: 0.148740\n",
      "Epoch 1129/2000, Train Loss: 0.070702, Val Loss: 0.149239\n",
      "Epoch 1130/2000, Train Loss: 0.070790, Val Loss: 0.149060\n",
      "Epoch 1131/2000, Train Loss: 0.070761, Val Loss: 0.149183\n",
      "Epoch 1132/2000, Train Loss: 0.070718, Val Loss: 0.149204\n",
      "Epoch 1133/2000, Train Loss: 0.070715, Val Loss: 0.149320\n",
      "Epoch 1134/2000, Train Loss: 0.070856, Val Loss: 0.149391\n",
      "Epoch 1135/2000, Train Loss: 0.070993, Val Loss: 0.149307\n",
      "Epoch 1136/2000, Train Loss: 0.070918, Val Loss: 0.148480\n",
      "Epoch 1137/2000, Train Loss: 0.071039, Val Loss: 0.149684\n",
      "Epoch 1138/2000, Train Loss: 0.070991, Val Loss: 0.148718\n",
      "Epoch 1139/2000, Train Loss: 0.071313, Val Loss: 0.150851\n",
      "Epoch 1140/2000, Train Loss: 0.070958, Val Loss: 0.148417\n",
      "Epoch 1141/2000, Train Loss: 0.070840, Val Loss: 0.149463\n",
      "Epoch 1142/2000, Train Loss: 0.070812, Val Loss: 0.148654\n",
      "Epoch 1143/2000, Train Loss: 0.070684, Val Loss: 0.149298\n",
      "Epoch 1144/2000, Train Loss: 0.070706, Val Loss: 0.148697\n",
      "Epoch 1145/2000, Train Loss: 0.070661, Val Loss: 0.149110\n",
      "Epoch 1146/2000, Train Loss: 0.070620, Val Loss: 0.149192\n",
      "Epoch 1147/2000, Train Loss: 0.070597, Val Loss: 0.148809\n",
      "Epoch 1148/2000, Train Loss: 0.070586, Val Loss: 0.149214\n",
      "Epoch 1149/2000, Train Loss: 0.070593, Val Loss: 0.148943\n",
      "Epoch 1150/2000, Train Loss: 0.070586, Val Loss: 0.149064\n",
      "Epoch 1151/2000, Train Loss: 0.070578, Val Loss: 0.148693\n",
      "Epoch 1152/2000, Train Loss: 0.070584, Val Loss: 0.149229\n",
      "Epoch 1153/2000, Train Loss: 0.070579, Val Loss: 0.148995\n",
      "Epoch 1154/2000, Train Loss: 0.070575, Val Loss: 0.148932\n",
      "Epoch 1155/2000, Train Loss: 0.070607, Val Loss: 0.149404\n",
      "Epoch 1156/2000, Train Loss: 0.070608, Val Loss: 0.148634\n",
      "Epoch 1157/2000, Train Loss: 0.070648, Val Loss: 0.149278\n",
      "Epoch 1158/2000, Train Loss: 0.070633, Val Loss: 0.149338\n",
      "Epoch 1159/2000, Train Loss: 0.070682, Val Loss: 0.149007\n",
      "Epoch 1160/2000, Train Loss: 0.070787, Val Loss: 0.148995\n",
      "Epoch 1161/2000, Train Loss: 0.070876, Val Loss: 0.150226\n",
      "Epoch 1162/2000, Train Loss: 0.070912, Val Loss: 0.148940\n",
      "Epoch 1163/2000, Train Loss: 0.071179, Val Loss: 0.149027\n",
      "Epoch 1164/2000, Train Loss: 0.071080, Val Loss: 0.149926\n",
      "Epoch 1165/2000, Train Loss: 0.071038, Val Loss: 0.148636\n",
      "Epoch 1166/2000, Train Loss: 0.071306, Val Loss: 0.149865\n",
      "Epoch 1167/2000, Train Loss: 0.071120, Val Loss: 0.149500\n",
      "Epoch 1168/2000, Train Loss: 0.071470, Val Loss: 0.150185\n",
      "Epoch 1169/2000, Train Loss: 0.071395, Val Loss: 0.148271\n",
      "Epoch 1170/2000, Train Loss: 0.071303, Val Loss: 0.150511\n",
      "Epoch 1171/2000, Train Loss: 0.071267, Val Loss: 0.149113\n",
      "Epoch 1172/2000, Train Loss: 0.070937, Val Loss: 0.149146\n",
      "Epoch 1173/2000, Train Loss: 0.070879, Val Loss: 0.149075\n",
      "Epoch 1174/2000, Train Loss: 0.070832, Val Loss: 0.148574\n",
      "Epoch 1175/2000, Train Loss: 0.070730, Val Loss: 0.149431\n",
      "Epoch 1176/2000, Train Loss: 0.070730, Val Loss: 0.148287\n",
      "Epoch 1177/2000, Train Loss: 0.070671, Val Loss: 0.148968\n",
      "Epoch 1178/2000, Train Loss: 0.070634, Val Loss: 0.149128\n",
      "Epoch 1179/2000, Train Loss: 0.070622, Val Loss: 0.148801\n",
      "Epoch 1180/2000, Train Loss: 0.070593, Val Loss: 0.148600\n",
      "Epoch 1181/2000, Train Loss: 0.070604, Val Loss: 0.149023\n",
      "Epoch 1182/2000, Train Loss: 0.070602, Val Loss: 0.148847\n",
      "Epoch 1183/2000, Train Loss: 0.070640, Val Loss: 0.148785\n",
      "Epoch 1184/2000, Train Loss: 0.070614, Val Loss: 0.149196\n",
      "Epoch 1185/2000, Train Loss: 0.070625, Val Loss: 0.148833\n",
      "Epoch 1186/2000, Train Loss: 0.070638, Val Loss: 0.148873\n",
      "Epoch 1187/2000, Train Loss: 0.070610, Val Loss: 0.148715\n",
      "Epoch 1188/2000, Train Loss: 0.070595, Val Loss: 0.149060\n",
      "Epoch 1189/2000, Train Loss: 0.070585, Val Loss: 0.149034\n",
      "Epoch 1190/2000, Train Loss: 0.070573, Val Loss: 0.149109\n",
      "Epoch 1191/2000, Train Loss: 0.070592, Val Loss: 0.148835\n",
      "Epoch 1192/2000, Train Loss: 0.070601, Val Loss: 0.149140\n",
      "Epoch 1193/2000, Train Loss: 0.070610, Val Loss: 0.148795\n",
      "Epoch 1194/2000, Train Loss: 0.070619, Val Loss: 0.149416\n",
      "Epoch 1195/2000, Train Loss: 0.070624, Val Loss: 0.148525\n",
      "Epoch 1196/2000, Train Loss: 0.070593, Val Loss: 0.148928\n",
      "Epoch 1197/2000, Train Loss: 0.070619, Val Loss: 0.149120\n",
      "Epoch 1198/2000, Train Loss: 0.070607, Val Loss: 0.149482\n",
      "Epoch 1199/2000, Train Loss: 0.070577, Val Loss: 0.148987\n",
      "Epoch 1200/2000, Train Loss: 0.070588, Val Loss: 0.149388\n",
      "Epoch 1201/2000, Train Loss: 0.070606, Val Loss: 0.148791\n",
      "Epoch 1202/2000, Train Loss: 0.070587, Val Loss: 0.149115\n",
      "Epoch 1203/2000, Train Loss: 0.070592, Val Loss: 0.148971\n",
      "Epoch 1204/2000, Train Loss: 0.070573, Val Loss: 0.149324\n",
      "Epoch 1205/2000, Train Loss: 0.070564, Val Loss: 0.148867\n",
      "Epoch 1206/2000, Train Loss: 0.070574, Val Loss: 0.149241\n",
      "Epoch 1207/2000, Train Loss: 0.070572, Val Loss: 0.149067\n",
      "Epoch 1208/2000, Train Loss: 0.070598, Val Loss: 0.148917\n",
      "Epoch 1209/2000, Train Loss: 0.070595, Val Loss: 0.149223\n",
      "Epoch 1210/2000, Train Loss: 0.070574, Val Loss: 0.149266\n",
      "Epoch 1211/2000, Train Loss: 0.070583, Val Loss: 0.148437\n",
      "Epoch 1212/2000, Train Loss: 0.070580, Val Loss: 0.148992\n",
      "Epoch 1213/2000, Train Loss: 0.070612, Val Loss: 0.149080\n",
      "Epoch 1214/2000, Train Loss: 0.070613, Val Loss: 0.149086\n",
      "Epoch 1215/2000, Train Loss: 0.070630, Val Loss: 0.149804\n",
      "Epoch 1216/2000, Train Loss: 0.070653, Val Loss: 0.148450\n",
      "Epoch 1217/2000, Train Loss: 0.070662, Val Loss: 0.149035\n",
      "Epoch 1218/2000, Train Loss: 0.070683, Val Loss: 0.149461\n",
      "Epoch 1219/2000, Train Loss: 0.070691, Val Loss: 0.148937\n",
      "Epoch 1220/2000, Train Loss: 0.070658, Val Loss: 0.148999\n",
      "Epoch 1221/2000, Train Loss: 0.070661, Val Loss: 0.149120\n",
      "Epoch 1222/2000, Train Loss: 0.070665, Val Loss: 0.149349\n",
      "Epoch 1223/2000, Train Loss: 0.070648, Val Loss: 0.148406\n",
      "Epoch 1224/2000, Train Loss: 0.070645, Val Loss: 0.149263\n",
      "Epoch 1225/2000, Train Loss: 0.070703, Val Loss: 0.149337\n",
      "Epoch 1226/2000, Train Loss: 0.070703, Val Loss: 0.148733\n",
      "Epoch 1227/2000, Train Loss: 0.070745, Val Loss: 0.149023\n",
      "Epoch 1228/2000, Train Loss: 0.070856, Val Loss: 0.149184\n",
      "Epoch 1229/2000, Train Loss: 0.071033, Val Loss: 0.149251\n",
      "Epoch 1230/2000, Train Loss: 0.071427, Val Loss: 0.148839\n",
      "Epoch 1231/2000, Train Loss: 0.071283, Val Loss: 0.149535\n",
      "Epoch 1232/2000, Train Loss: 0.071074, Val Loss: 0.148359\n",
      "Epoch 1233/2000, Train Loss: 0.071346, Val Loss: 0.149240\n",
      "Epoch 1234/2000, Train Loss: 0.071467, Val Loss: 0.149418\n",
      "Epoch 1235/2000, Train Loss: 0.072222, Val Loss: 0.150387\n",
      "Epoch 1236/2000, Train Loss: 0.074659, Val Loss: 0.149757\n",
      "Epoch 1237/2000, Train Loss: 0.073985, Val Loss: 0.149964\n",
      "Epoch 1238/2000, Train Loss: 0.073782, Val Loss: 0.147637\n",
      "Epoch 1239/2000, Train Loss: 0.073330, Val Loss: 0.148770\n",
      "Epoch 1240/2000, Train Loss: 0.073549, Val Loss: 0.148031\n",
      "Epoch 1241/2000, Train Loss: 0.072941, Val Loss: 0.151060\n",
      "Epoch 1242/2000, Train Loss: 0.073313, Val Loss: 0.148001\n",
      "Epoch 1243/2000, Train Loss: 0.073187, Val Loss: 0.148659\n",
      "Epoch 1244/2000, Train Loss: 0.072597, Val Loss: 0.150124\n",
      "Epoch 1245/2000, Train Loss: 0.072536, Val Loss: 0.149764\n",
      "Epoch 1246/2000, Train Loss: 0.073248, Val Loss: 0.148406\n",
      "Epoch 1247/2000, Train Loss: 0.072468, Val Loss: 0.147758\n",
      "Epoch 1248/2000, Train Loss: 0.072287, Val Loss: 0.148448\n",
      "Epoch 1249/2000, Train Loss: 0.073463, Val Loss: 0.149913\n",
      "Epoch 1250/2000, Train Loss: 0.073680, Val Loss: 0.150153\n",
      "Epoch 1251/2000, Train Loss: 0.073496, Val Loss: 0.148092\n",
      "Epoch 1252/2000, Train Loss: 0.072941, Val Loss: 0.147347\n",
      "Epoch 1253/2000, Train Loss: 0.072677, Val Loss: 0.149349\n",
      "Epoch 1254/2000, Train Loss: 0.072449, Val Loss: 0.148111\n",
      "Epoch 1255/2000, Train Loss: 0.072453, Val Loss: 0.148781\n",
      "Epoch 1256/2000, Train Loss: 0.072347, Val Loss: 0.149096\n",
      "Epoch 1257/2000, Train Loss: 0.072325, Val Loss: 0.147927\n",
      "Epoch 1258/2000, Train Loss: 0.072268, Val Loss: 0.148696\n",
      "Epoch 1259/2000, Train Loss: 0.072109, Val Loss: 0.148107\n",
      "Epoch 1260/2000, Train Loss: 0.072313, Val Loss: 0.148490\n",
      "Epoch 1261/2000, Train Loss: 0.072201, Val Loss: 0.148764\n",
      "Epoch 1262/2000, Train Loss: 0.072303, Val Loss: 0.148214\n",
      "Epoch 1263/2000, Train Loss: 0.072217, Val Loss: 0.148899\n",
      "Epoch 1264/2000, Train Loss: 0.072132, Val Loss: 0.148782\n",
      "Epoch 1265/2000, Train Loss: 0.072171, Val Loss: 0.148138\n",
      "Epoch 1266/2000, Train Loss: 0.072266, Val Loss: 0.149281\n",
      "Epoch 1267/2000, Train Loss: 0.072063, Val Loss: 0.148814\n",
      "Epoch 1268/2000, Train Loss: 0.071956, Val Loss: 0.148474\n",
      "Epoch 1269/2000, Train Loss: 0.071844, Val Loss: 0.148939\n",
      "Epoch 1270/2000, Train Loss: 0.072068, Val Loss: 0.148833\n",
      "Epoch 1271/2000, Train Loss: 0.071934, Val Loss: 0.149754\n",
      "Epoch 1272/2000, Train Loss: 0.071973, Val Loss: 0.148441\n",
      "Epoch 1273/2000, Train Loss: 0.072076, Val Loss: 0.149565\n",
      "Epoch 1274/2000, Train Loss: 0.071834, Val Loss: 0.148519\n",
      "Epoch 1275/2000, Train Loss: 0.071756, Val Loss: 0.148892\n",
      "Epoch 1276/2000, Train Loss: 0.071639, Val Loss: 0.149019\n",
      "Epoch 1277/2000, Train Loss: 0.071495, Val Loss: 0.149075\n",
      "Epoch 1278/2000, Train Loss: 0.071343, Val Loss: 0.149410\n",
      "Epoch 1279/2000, Train Loss: 0.071237, Val Loss: 0.149358\n",
      "Epoch 1280/2000, Train Loss: 0.071011, Val Loss: 0.149439\n",
      "Epoch 1281/2000, Train Loss: 0.070873, Val Loss: 0.149793\n",
      "Epoch 1282/2000, Train Loss: 0.070799, Val Loss: 0.148822\n",
      "Epoch 1283/2000, Train Loss: 0.070729, Val Loss: 0.149396\n",
      "Epoch 1284/2000, Train Loss: 0.070705, Val Loss: 0.149337\n",
      "Epoch 1285/2000, Train Loss: 0.070667, Val Loss: 0.149278\n",
      "Epoch 1286/2000, Train Loss: 0.070654, Val Loss: 0.149229\n",
      "Epoch 1287/2000, Train Loss: 0.070649, Val Loss: 0.149124\n",
      "Epoch 1288/2000, Train Loss: 0.070672, Val Loss: 0.150066\n",
      "Epoch 1289/2000, Train Loss: 0.070669, Val Loss: 0.149065\n",
      "Epoch 1290/2000, Train Loss: 0.070628, Val Loss: 0.149706\n",
      "Epoch 1291/2000, Train Loss: 0.070644, Val Loss: 0.149281\n",
      "Epoch 1292/2000, Train Loss: 0.070654, Val Loss: 0.149204\n",
      "Epoch 1293/2000, Train Loss: 0.070673, Val Loss: 0.149845\n",
      "Epoch 1294/2000, Train Loss: 0.070644, Val Loss: 0.148477\n",
      "Epoch 1295/2000, Train Loss: 0.070647, Val Loss: 0.149488\n",
      "Epoch 1296/2000, Train Loss: 0.070654, Val Loss: 0.149397\n",
      "Epoch 1297/2000, Train Loss: 0.070645, Val Loss: 0.149300\n",
      "Epoch 1298/2000, Train Loss: 0.070638, Val Loss: 0.149311\n",
      "Epoch 1299/2000, Train Loss: 0.070613, Val Loss: 0.149319\n",
      "Epoch 1300/2000, Train Loss: 0.070588, Val Loss: 0.149380\n",
      "Epoch 1301/2000, Train Loss: 0.070586, Val Loss: 0.149403\n",
      "Epoch 1302/2000, Train Loss: 0.070579, Val Loss: 0.149384\n",
      "Epoch 1303/2000, Train Loss: 0.070567, Val Loss: 0.149333\n",
      "Epoch 1304/2000, Train Loss: 0.070566, Val Loss: 0.149255\n",
      "Epoch 1305/2000, Train Loss: 0.070557, Val Loss: 0.149394\n",
      "Epoch 1306/2000, Train Loss: 0.070558, Val Loss: 0.149368\n",
      "Epoch 1307/2000, Train Loss: 0.070557, Val Loss: 0.149515\n",
      "Epoch 1308/2000, Train Loss: 0.070557, Val Loss: 0.149346\n",
      "Epoch 1309/2000, Train Loss: 0.070559, Val Loss: 0.149375\n",
      "Epoch 1310/2000, Train Loss: 0.070554, Val Loss: 0.149305\n",
      "Epoch 1311/2000, Train Loss: 0.070553, Val Loss: 0.149314\n",
      "Epoch 1312/2000, Train Loss: 0.070552, Val Loss: 0.149578\n",
      "Epoch 1313/2000, Train Loss: 0.070556, Val Loss: 0.149174\n",
      "Epoch 1314/2000, Train Loss: 0.070552, Val Loss: 0.149547\n",
      "Epoch 1315/2000, Train Loss: 0.070551, Val Loss: 0.149377\n",
      "Epoch 1316/2000, Train Loss: 0.070548, Val Loss: 0.149408\n",
      "Epoch 1317/2000, Train Loss: 0.070557, Val Loss: 0.149252\n",
      "Epoch 1318/2000, Train Loss: 0.070563, Val Loss: 0.149777\n",
      "Epoch 1319/2000, Train Loss: 0.070573, Val Loss: 0.149731\n",
      "Epoch 1320/2000, Train Loss: 0.070568, Val Loss: 0.148984\n",
      "Epoch 1321/2000, Train Loss: 0.070551, Val Loss: 0.149831\n",
      "Epoch 1322/2000, Train Loss: 0.070577, Val Loss: 0.149101\n",
      "Epoch 1323/2000, Train Loss: 0.070561, Val Loss: 0.149577\n",
      "Epoch 1324/2000, Train Loss: 0.070573, Val Loss: 0.149562\n",
      "Epoch 1325/2000, Train Loss: 0.070568, Val Loss: 0.149024\n",
      "Epoch 1326/2000, Train Loss: 0.070573, Val Loss: 0.149481\n",
      "Epoch 1327/2000, Train Loss: 0.070573, Val Loss: 0.149546\n",
      "Epoch 1328/2000, Train Loss: 0.070561, Val Loss: 0.149427\n",
      "Epoch 1329/2000, Train Loss: 0.070554, Val Loss: 0.149176\n",
      "Epoch 1330/2000, Train Loss: 0.070555, Val Loss: 0.149553\n",
      "Epoch 1331/2000, Train Loss: 0.070548, Val Loss: 0.149436\n",
      "Epoch 1332/2000, Train Loss: 0.070547, Val Loss: 0.149433\n",
      "Epoch 1333/2000, Train Loss: 0.070540, Val Loss: 0.149563\n",
      "Epoch 1334/2000, Train Loss: 0.070546, Val Loss: 0.149383\n",
      "Epoch 1335/2000, Train Loss: 0.070540, Val Loss: 0.149362\n",
      "Epoch 1336/2000, Train Loss: 0.070535, Val Loss: 0.149484\n",
      "Epoch 1337/2000, Train Loss: 0.070535, Val Loss: 0.149333\n",
      "Epoch 1338/2000, Train Loss: 0.070537, Val Loss: 0.149417\n",
      "Epoch 1339/2000, Train Loss: 0.070534, Val Loss: 0.149338\n",
      "Epoch 1340/2000, Train Loss: 0.070539, Val Loss: 0.149514\n",
      "Epoch 1341/2000, Train Loss: 0.070549, Val Loss: 0.149185\n",
      "Epoch 1342/2000, Train Loss: 0.070555, Val Loss: 0.149307\n",
      "Epoch 1343/2000, Train Loss: 0.070587, Val Loss: 0.149696\n",
      "Epoch 1344/2000, Train Loss: 0.070626, Val Loss: 0.149257\n",
      "Epoch 1345/2000, Train Loss: 0.070739, Val Loss: 0.149771\n",
      "Epoch 1346/2000, Train Loss: 0.070636, Val Loss: 0.149217\n",
      "Epoch 1347/2000, Train Loss: 0.070641, Val Loss: 0.149175\n",
      "Epoch 1348/2000, Train Loss: 0.070669, Val Loss: 0.149908\n",
      "Epoch 1349/2000, Train Loss: 0.070745, Val Loss: 0.149958\n",
      "Epoch 1350/2000, Train Loss: 0.070749, Val Loss: 0.149477\n",
      "Epoch 1351/2000, Train Loss: 0.070806, Val Loss: 0.149612\n",
      "Epoch 1352/2000, Train Loss: 0.070808, Val Loss: 0.148749\n",
      "Epoch 1353/2000, Train Loss: 0.070764, Val Loss: 0.150105\n",
      "Epoch 1354/2000, Train Loss: 0.070750, Val Loss: 0.150119\n",
      "Epoch 1355/2000, Train Loss: 0.070915, Val Loss: 0.149075\n",
      "Epoch 1356/2000, Train Loss: 0.070759, Val Loss: 0.149400\n",
      "Epoch 1357/2000, Train Loss: 0.070711, Val Loss: 0.149427\n",
      "Epoch 1358/2000, Train Loss: 0.070648, Val Loss: 0.149303\n",
      "Epoch 1359/2000, Train Loss: 0.070637, Val Loss: 0.149348\n",
      "Epoch 1360/2000, Train Loss: 0.070601, Val Loss: 0.149479\n",
      "Epoch 1361/2000, Train Loss: 0.070643, Val Loss: 0.149456\n",
      "Epoch 1362/2000, Train Loss: 0.070664, Val Loss: 0.149012\n",
      "Epoch 1363/2000, Train Loss: 0.070685, Val Loss: 0.148768\n",
      "Epoch 1364/2000, Train Loss: 0.070696, Val Loss: 0.149904\n",
      "Epoch 1365/2000, Train Loss: 0.070685, Val Loss: 0.148853\n",
      "Epoch 1366/2000, Train Loss: 0.070677, Val Loss: 0.149379\n",
      "Epoch 1367/2000, Train Loss: 0.070702, Val Loss: 0.149519\n",
      "Epoch 1368/2000, Train Loss: 0.070673, Val Loss: 0.150431\n",
      "Epoch 1369/2000, Train Loss: 0.070728, Val Loss: 0.149048\n",
      "Epoch 1370/2000, Train Loss: 0.070712, Val Loss: 0.148881\n",
      "Epoch 1371/2000, Train Loss: 0.070742, Val Loss: 0.149597\n",
      "Epoch 1372/2000, Train Loss: 0.070758, Val Loss: 0.148141\n",
      "Epoch 1373/2000, Train Loss: 0.070884, Val Loss: 0.150306\n",
      "Epoch 1374/2000, Train Loss: 0.071098, Val Loss: 0.150844\n",
      "Epoch 1375/2000, Train Loss: 0.071026, Val Loss: 0.149024\n",
      "Epoch 1376/2000, Train Loss: 0.071159, Val Loss: 0.149380\n",
      "Epoch 1377/2000, Train Loss: 0.071054, Val Loss: 0.149861\n",
      "Epoch 1378/2000, Train Loss: 0.071224, Val Loss: 0.149728\n",
      "Epoch 1379/2000, Train Loss: 0.071203, Val Loss: 0.148595\n",
      "Epoch 1380/2000, Train Loss: 0.071035, Val Loss: 0.149429\n",
      "Epoch 1381/2000, Train Loss: 0.070939, Val Loss: 0.150587\n",
      "Epoch 1382/2000, Train Loss: 0.070965, Val Loss: 0.149443\n",
      "Epoch 1383/2000, Train Loss: 0.070928, Val Loss: 0.149724\n",
      "Epoch 1384/2000, Train Loss: 0.070906, Val Loss: 0.149182\n",
      "Epoch 1385/2000, Train Loss: 0.070927, Val Loss: 0.149617\n",
      "Epoch 1386/2000, Train Loss: 0.070795, Val Loss: 0.148928\n",
      "Epoch 1387/2000, Train Loss: 0.070749, Val Loss: 0.149348\n",
      "Epoch 1388/2000, Train Loss: 0.070729, Val Loss: 0.148879\n",
      "Epoch 1389/2000, Train Loss: 0.070709, Val Loss: 0.149291\n",
      "Epoch 1390/2000, Train Loss: 0.070719, Val Loss: 0.148983\n",
      "Epoch 1391/2000, Train Loss: 0.070673, Val Loss: 0.149629\n",
      "Epoch 1392/2000, Train Loss: 0.070627, Val Loss: 0.149072\n",
      "Epoch 1393/2000, Train Loss: 0.070585, Val Loss: 0.149159\n",
      "Epoch 1394/2000, Train Loss: 0.070603, Val Loss: 0.149763\n",
      "Epoch 1395/2000, Train Loss: 0.070638, Val Loss: 0.148878\n",
      "Epoch 1396/2000, Train Loss: 0.070657, Val Loss: 0.149170\n",
      "Epoch 1397/2000, Train Loss: 0.070713, Val Loss: 0.150748\n",
      "Epoch 1398/2000, Train Loss: 0.070838, Val Loss: 0.148675\n",
      "Epoch 1399/2000, Train Loss: 0.070627, Val Loss: 0.149557\n",
      "Epoch 1400/2000, Train Loss: 0.070700, Val Loss: 0.149379\n",
      "Epoch 1401/2000, Train Loss: 0.070718, Val Loss: 0.148425\n",
      "Epoch 1402/2000, Train Loss: 0.070885, Val Loss: 0.150759\n",
      "Epoch 1403/2000, Train Loss: 0.070864, Val Loss: 0.150335\n",
      "Epoch 1404/2000, Train Loss: 0.070832, Val Loss: 0.149153\n",
      "Epoch 1405/2000, Train Loss: 0.070661, Val Loss: 0.149036\n",
      "Epoch 1406/2000, Train Loss: 0.070648, Val Loss: 0.150110\n",
      "Epoch 1407/2000, Train Loss: 0.070691, Val Loss: 0.148925\n",
      "Epoch 1408/2000, Train Loss: 0.070666, Val Loss: 0.149291\n",
      "Epoch 1409/2000, Train Loss: 0.070667, Val Loss: 0.149032\n",
      "Epoch 1410/2000, Train Loss: 0.070636, Val Loss: 0.149746\n",
      "Epoch 1411/2000, Train Loss: 0.070642, Val Loss: 0.149068\n",
      "Epoch 1412/2000, Train Loss: 0.070739, Val Loss: 0.149393\n",
      "Epoch 1413/2000, Train Loss: 0.070737, Val Loss: 0.148858\n",
      "Epoch 1414/2000, Train Loss: 0.070679, Val Loss: 0.149531\n",
      "Epoch 1415/2000, Train Loss: 0.070691, Val Loss: 0.148892\n",
      "Epoch 1416/2000, Train Loss: 0.070725, Val Loss: 0.149450\n",
      "Epoch 1417/2000, Train Loss: 0.070741, Val Loss: 0.149800\n",
      "Epoch 1418/2000, Train Loss: 0.070721, Val Loss: 0.148858\n",
      "Epoch 1419/2000, Train Loss: 0.070690, Val Loss: 0.149717\n",
      "Epoch 1420/2000, Train Loss: 0.070653, Val Loss: 0.149765\n",
      "Epoch 1421/2000, Train Loss: 0.070641, Val Loss: 0.148812\n",
      "Epoch 1422/2000, Train Loss: 0.070612, Val Loss: 0.148965\n",
      "Epoch 1423/2000, Train Loss: 0.070591, Val Loss: 0.149987\n",
      "Epoch 1424/2000, Train Loss: 0.070606, Val Loss: 0.149012\n",
      "Epoch 1425/2000, Train Loss: 0.070707, Val Loss: 0.149529\n",
      "Epoch 1426/2000, Train Loss: 0.070772, Val Loss: 0.149584\n",
      "Epoch 1427/2000, Train Loss: 0.070828, Val Loss: 0.150251\n",
      "Epoch 1428/2000, Train Loss: 0.070814, Val Loss: 0.147962\n",
      "Epoch 1429/2000, Train Loss: 0.070924, Val Loss: 0.149708\n",
      "Epoch 1430/2000, Train Loss: 0.071240, Val Loss: 0.151743\n",
      "Epoch 1431/2000, Train Loss: 0.071552, Val Loss: 0.149022\n",
      "Epoch 1432/2000, Train Loss: 0.071967, Val Loss: 0.151989\n",
      "Epoch 1433/2000, Train Loss: 0.073579, Val Loss: 0.148315\n",
      "Epoch 1434/2000, Train Loss: 0.073109, Val Loss: 0.150413\n",
      "Epoch 1435/2000, Train Loss: 0.072311, Val Loss: 0.148329\n",
      "Epoch 1436/2000, Train Loss: 0.071985, Val Loss: 0.151516\n",
      "Epoch 1437/2000, Train Loss: 0.073121, Val Loss: 0.149969\n",
      "Epoch 1438/2000, Train Loss: 0.072772, Val Loss: 0.149428\n",
      "Epoch 1439/2000, Train Loss: 0.072235, Val Loss: 0.148340\n",
      "Epoch 1440/2000, Train Loss: 0.071312, Val Loss: 0.148045\n",
      "Epoch 1441/2000, Train Loss: 0.071527, Val Loss: 0.149984\n",
      "Epoch 1442/2000, Train Loss: 0.071759, Val Loss: 0.147907\n",
      "Epoch 1443/2000, Train Loss: 0.071620, Val Loss: 0.152415\n",
      "Epoch 1444/2000, Train Loss: 0.071893, Val Loss: 0.148412\n",
      "Epoch 1445/2000, Train Loss: 0.071766, Val Loss: 0.148986\n",
      "Epoch 1446/2000, Train Loss: 0.071624, Val Loss: 0.151660\n",
      "Epoch 1447/2000, Train Loss: 0.071483, Val Loss: 0.149105\n",
      "Epoch 1448/2000, Train Loss: 0.071226, Val Loss: 0.149195\n",
      "Epoch 1449/2000, Train Loss: 0.071691, Val Loss: 0.148832\n",
      "Epoch 1450/2000, Train Loss: 0.071511, Val Loss: 0.150363\n",
      "Epoch 1451/2000, Train Loss: 0.071425, Val Loss: 0.148852\n",
      "Epoch 1452/2000, Train Loss: 0.071337, Val Loss: 0.148909\n",
      "Epoch 1453/2000, Train Loss: 0.071630, Val Loss: 0.150017\n",
      "Epoch 1454/2000, Train Loss: 0.071475, Val Loss: 0.148567\n",
      "Epoch 1455/2000, Train Loss: 0.071517, Val Loss: 0.150213\n",
      "Epoch 1456/2000, Train Loss: 0.071190, Val Loss: 0.147975\n",
      "Epoch 1457/2000, Train Loss: 0.071202, Val Loss: 0.150011\n",
      "Epoch 1458/2000, Train Loss: 0.070875, Val Loss: 0.147941\n",
      "Epoch 1459/2000, Train Loss: 0.070969, Val Loss: 0.149176\n",
      "Epoch 1460/2000, Train Loss: 0.070787, Val Loss: 0.149823\n",
      "Epoch 1461/2000, Train Loss: 0.070838, Val Loss: 0.149414\n",
      "Epoch 1462/2000, Train Loss: 0.070903, Val Loss: 0.149287\n",
      "Epoch 1463/2000, Train Loss: 0.070914, Val Loss: 0.149226\n",
      "Epoch 1464/2000, Train Loss: 0.070870, Val Loss: 0.149290\n",
      "Epoch 1465/2000, Train Loss: 0.070700, Val Loss: 0.149585\n",
      "Epoch 1466/2000, Train Loss: 0.070753, Val Loss: 0.149246\n",
      "Epoch 1467/2000, Train Loss: 0.070748, Val Loss: 0.149443\n",
      "Epoch 1468/2000, Train Loss: 0.070710, Val Loss: 0.148951\n",
      "Epoch 1469/2000, Train Loss: 0.070629, Val Loss: 0.149212\n",
      "Epoch 1470/2000, Train Loss: 0.070592, Val Loss: 0.148805\n",
      "Epoch 1471/2000, Train Loss: 0.070576, Val Loss: 0.149002\n",
      "Epoch 1472/2000, Train Loss: 0.070551, Val Loss: 0.148774\n",
      "Epoch 1473/2000, Train Loss: 0.070546, Val Loss: 0.149051\n",
      "Epoch 1474/2000, Train Loss: 0.070539, Val Loss: 0.148919\n",
      "Epoch 1475/2000, Train Loss: 0.070536, Val Loss: 0.149013\n",
      "Epoch 1476/2000, Train Loss: 0.070536, Val Loss: 0.149096\n",
      "Epoch 1477/2000, Train Loss: 0.070523, Val Loss: 0.148824\n",
      "Epoch 1478/2000, Train Loss: 0.070526, Val Loss: 0.149098\n",
      "Epoch 1479/2000, Train Loss: 0.070530, Val Loss: 0.148983\n",
      "Epoch 1480/2000, Train Loss: 0.070538, Val Loss: 0.149132\n",
      "Epoch 1481/2000, Train Loss: 0.070526, Val Loss: 0.148936\n",
      "Epoch 1482/2000, Train Loss: 0.070521, Val Loss: 0.148950\n",
      "Epoch 1483/2000, Train Loss: 0.070521, Val Loss: 0.148916\n",
      "Epoch 1484/2000, Train Loss: 0.070521, Val Loss: 0.149227\n",
      "Epoch 1485/2000, Train Loss: 0.070520, Val Loss: 0.148957\n",
      "Epoch 1486/2000, Train Loss: 0.070519, Val Loss: 0.149061\n",
      "Epoch 1487/2000, Train Loss: 0.070517, Val Loss: 0.149021\n",
      "Epoch 1488/2000, Train Loss: 0.070513, Val Loss: 0.149109\n",
      "Epoch 1489/2000, Train Loss: 0.070512, Val Loss: 0.149036\n",
      "Epoch 1490/2000, Train Loss: 0.070509, Val Loss: 0.149116\n",
      "Epoch 1491/2000, Train Loss: 0.070514, Val Loss: 0.148951\n",
      "Epoch 1492/2000, Train Loss: 0.070511, Val Loss: 0.149176\n",
      "Epoch 1493/2000, Train Loss: 0.070507, Val Loss: 0.148878\n",
      "Epoch 1494/2000, Train Loss: 0.070511, Val Loss: 0.149220\n",
      "Epoch 1495/2000, Train Loss: 0.070512, Val Loss: 0.149007\n",
      "Epoch 1496/2000, Train Loss: 0.070515, Val Loss: 0.149181\n",
      "Epoch 1497/2000, Train Loss: 0.070512, Val Loss: 0.148836\n",
      "Epoch 1498/2000, Train Loss: 0.070511, Val Loss: 0.149339\n",
      "Epoch 1499/2000, Train Loss: 0.070521, Val Loss: 0.148884\n",
      "Epoch 1500/2000, Train Loss: 0.070545, Val Loss: 0.149374\n",
      "Epoch 1501/2000, Train Loss: 0.070558, Val Loss: 0.148671\n",
      "Epoch 1502/2000, Train Loss: 0.070557, Val Loss: 0.149695\n",
      "Epoch 1503/2000, Train Loss: 0.070558, Val Loss: 0.148524\n",
      "Epoch 1504/2000, Train Loss: 0.070598, Val Loss: 0.149563\n",
      "Epoch 1505/2000, Train Loss: 0.070636, Val Loss: 0.149074\n",
      "Epoch 1506/2000, Train Loss: 0.070631, Val Loss: 0.149211\n",
      "Epoch 1507/2000, Train Loss: 0.070610, Val Loss: 0.148892\n",
      "Epoch 1508/2000, Train Loss: 0.070596, Val Loss: 0.149158\n",
      "Epoch 1509/2000, Train Loss: 0.070637, Val Loss: 0.149516\n",
      "Epoch 1510/2000, Train Loss: 0.070719, Val Loss: 0.149727\n",
      "Epoch 1511/2000, Train Loss: 0.070743, Val Loss: 0.148975\n",
      "Epoch 1512/2000, Train Loss: 0.070762, Val Loss: 0.149477\n",
      "Epoch 1513/2000, Train Loss: 0.070687, Val Loss: 0.149456\n",
      "Epoch 1514/2000, Train Loss: 0.070645, Val Loss: 0.149705\n",
      "Epoch 1515/2000, Train Loss: 0.070601, Val Loss: 0.149110\n",
      "Epoch 1516/2000, Train Loss: 0.070568, Val Loss: 0.148904\n",
      "Epoch 1517/2000, Train Loss: 0.070587, Val Loss: 0.149707\n",
      "Epoch 1518/2000, Train Loss: 0.070621, Val Loss: 0.149052\n",
      "Epoch 1519/2000, Train Loss: 0.070633, Val Loss: 0.148530\n",
      "Epoch 1520/2000, Train Loss: 0.070641, Val Loss: 0.149501\n",
      "Epoch 1521/2000, Train Loss: 0.070592, Val Loss: 0.148630\n",
      "Epoch 1522/2000, Train Loss: 0.070688, Val Loss: 0.149245\n",
      "Epoch 1523/2000, Train Loss: 0.070637, Val Loss: 0.149672\n",
      "Epoch 1524/2000, Train Loss: 0.070616, Val Loss: 0.149517\n",
      "Epoch 1525/2000, Train Loss: 0.070597, Val Loss: 0.149008\n",
      "Epoch 1526/2000, Train Loss: 0.070580, Val Loss: 0.149063\n",
      "Epoch 1527/2000, Train Loss: 0.070582, Val Loss: 0.149188\n",
      "Epoch 1528/2000, Train Loss: 0.070552, Val Loss: 0.149283\n",
      "Epoch 1529/2000, Train Loss: 0.070555, Val Loss: 0.149361\n",
      "Epoch 1530/2000, Train Loss: 0.070538, Val Loss: 0.148978\n",
      "Epoch 1531/2000, Train Loss: 0.070567, Val Loss: 0.149948\n",
      "Epoch 1532/2000, Train Loss: 0.070644, Val Loss: 0.148505\n",
      "Epoch 1533/2000, Train Loss: 0.070757, Val Loss: 0.149328\n",
      "Epoch 1534/2000, Train Loss: 0.070768, Val Loss: 0.149464\n",
      "Epoch 1535/2000, Train Loss: 0.070698, Val Loss: 0.148921\n",
      "Epoch 1536/2000, Train Loss: 0.070712, Val Loss: 0.148780\n",
      "Epoch 1537/2000, Train Loss: 0.070643, Val Loss: 0.149533\n",
      "Epoch 1538/2000, Train Loss: 0.070698, Val Loss: 0.149598\n",
      "Epoch 1539/2000, Train Loss: 0.070759, Val Loss: 0.148456\n",
      "Epoch 1540/2000, Train Loss: 0.070730, Val Loss: 0.150488\n",
      "Epoch 1541/2000, Train Loss: 0.070687, Val Loss: 0.149543\n",
      "Epoch 1542/2000, Train Loss: 0.070738, Val Loss: 0.148895\n",
      "Epoch 1543/2000, Train Loss: 0.070687, Val Loss: 0.150026\n",
      "Epoch 1544/2000, Train Loss: 0.070718, Val Loss: 0.149408\n",
      "Epoch 1545/2000, Train Loss: 0.070702, Val Loss: 0.148930\n",
      "Epoch 1546/2000, Train Loss: 0.070687, Val Loss: 0.149113\n",
      "Epoch 1547/2000, Train Loss: 0.070702, Val Loss: 0.149233\n",
      "Epoch 1548/2000, Train Loss: 0.070726, Val Loss: 0.149445\n",
      "Epoch 1549/2000, Train Loss: 0.070679, Val Loss: 0.149314\n",
      "Epoch 1550/2000, Train Loss: 0.070708, Val Loss: 0.149763\n",
      "Epoch 1551/2000, Train Loss: 0.070682, Val Loss: 0.148929\n",
      "Epoch 1552/2000, Train Loss: 0.070735, Val Loss: 0.149575\n",
      "Epoch 1553/2000, Train Loss: 0.070623, Val Loss: 0.148758\n",
      "Epoch 1554/2000, Train Loss: 0.070902, Val Loss: 0.149299\n",
      "Epoch 1555/2000, Train Loss: 0.070749, Val Loss: 0.149631\n",
      "Epoch 1556/2000, Train Loss: 0.070951, Val Loss: 0.150541\n",
      "Epoch 1557/2000, Train Loss: 0.071041, Val Loss: 0.149128\n",
      "Epoch 1558/2000, Train Loss: 0.070973, Val Loss: 0.149476\n",
      "Epoch 1559/2000, Train Loss: 0.070721, Val Loss: 0.148573\n",
      "Epoch 1560/2000, Train Loss: 0.070660, Val Loss: 0.149518\n",
      "Epoch 1561/2000, Train Loss: 0.070615, Val Loss: 0.149694\n",
      "Epoch 1562/2000, Train Loss: 0.070558, Val Loss: 0.148843\n",
      "Epoch 1563/2000, Train Loss: 0.070536, Val Loss: 0.149318\n",
      "Epoch 1564/2000, Train Loss: 0.070556, Val Loss: 0.149253\n",
      "Epoch 1565/2000, Train Loss: 0.070597, Val Loss: 0.148932\n",
      "Epoch 1566/2000, Train Loss: 0.070602, Val Loss: 0.149591\n",
      "Epoch 1567/2000, Train Loss: 0.070585, Val Loss: 0.149347\n",
      "Epoch 1568/2000, Train Loss: 0.070622, Val Loss: 0.148764\n",
      "Epoch 1569/2000, Train Loss: 0.070573, Val Loss: 0.149415\n",
      "Epoch 1570/2000, Train Loss: 0.070546, Val Loss: 0.149170\n",
      "Epoch 1571/2000, Train Loss: 0.070531, Val Loss: 0.149142\n",
      "Epoch 1572/2000, Train Loss: 0.070527, Val Loss: 0.149177\n",
      "Epoch 1573/2000, Train Loss: 0.070522, Val Loss: 0.148994\n",
      "Epoch 1574/2000, Train Loss: 0.070547, Val Loss: 0.149652\n",
      "Epoch 1575/2000, Train Loss: 0.070541, Val Loss: 0.149059\n",
      "Epoch 1576/2000, Train Loss: 0.070520, Val Loss: 0.149207\n",
      "Epoch 1577/2000, Train Loss: 0.070508, Val Loss: 0.149193\n",
      "Epoch 1578/2000, Train Loss: 0.070500, Val Loss: 0.149224\n",
      "Epoch 1579/2000, Train Loss: 0.070498, Val Loss: 0.149101\n",
      "Epoch 1580/2000, Train Loss: 0.070502, Val Loss: 0.149129\n",
      "Epoch 1581/2000, Train Loss: 0.070489, Val Loss: 0.149283\n",
      "Epoch 1582/2000, Train Loss: 0.070495, Val Loss: 0.149112\n",
      "Epoch 1583/2000, Train Loss: 0.070489, Val Loss: 0.149035\n",
      "Epoch 1584/2000, Train Loss: 0.070499, Val Loss: 0.149169\n",
      "Epoch 1585/2000, Train Loss: 0.070494, Val Loss: 0.148961\n",
      "Epoch 1586/2000, Train Loss: 0.070491, Val Loss: 0.149551\n",
      "Epoch 1587/2000, Train Loss: 0.070494, Val Loss: 0.149226\n",
      "Epoch 1588/2000, Train Loss: 0.070493, Val Loss: 0.149047\n",
      "Epoch 1589/2000, Train Loss: 0.070487, Val Loss: 0.149371\n",
      "Epoch 1590/2000, Train Loss: 0.070491, Val Loss: 0.149256\n",
      "Epoch 1591/2000, Train Loss: 0.070492, Val Loss: 0.149280\n",
      "Epoch 1592/2000, Train Loss: 0.070481, Val Loss: 0.149034\n",
      "Epoch 1593/2000, Train Loss: 0.070483, Val Loss: 0.149248\n",
      "Epoch 1594/2000, Train Loss: 0.070493, Val Loss: 0.149228\n",
      "Epoch 1595/2000, Train Loss: 0.070508, Val Loss: 0.149263\n",
      "Epoch 1596/2000, Train Loss: 0.070549, Val Loss: 0.148849\n",
      "Epoch 1597/2000, Train Loss: 0.070617, Val Loss: 0.150733\n",
      "Epoch 1598/2000, Train Loss: 0.070697, Val Loss: 0.148329\n",
      "Epoch 1599/2000, Train Loss: 0.070848, Val Loss: 0.150354\n",
      "Epoch 1600/2000, Train Loss: 0.070853, Val Loss: 0.149499\n",
      "Epoch 1601/2000, Train Loss: 0.070698, Val Loss: 0.149337\n",
      "Epoch 1602/2000, Train Loss: 0.070914, Val Loss: 0.149233\n",
      "Epoch 1603/2000, Train Loss: 0.071305, Val Loss: 0.152225\n",
      "Epoch 1604/2000, Train Loss: 0.071596, Val Loss: 0.147781\n",
      "Epoch 1605/2000, Train Loss: 0.071990, Val Loss: 0.151369\n",
      "Epoch 1606/2000, Train Loss: 0.072249, Val Loss: 0.148369\n",
      "Epoch 1607/2000, Train Loss: 0.071658, Val Loss: 0.150242\n",
      "Epoch 1608/2000, Train Loss: 0.071913, Val Loss: 0.149762\n",
      "Epoch 1609/2000, Train Loss: 0.073161, Val Loss: 0.149418\n",
      "Epoch 1610/2000, Train Loss: 0.072809, Val Loss: 0.148178\n",
      "Epoch 1611/2000, Train Loss: 0.072393, Val Loss: 0.148675\n",
      "Epoch 1612/2000, Train Loss: 0.071525, Val Loss: 0.149362\n",
      "Epoch 1613/2000, Train Loss: 0.071332, Val Loss: 0.149693\n",
      "Epoch 1614/2000, Train Loss: 0.071044, Val Loss: 0.149394\n",
      "Epoch 1615/2000, Train Loss: 0.071152, Val Loss: 0.148648\n",
      "Epoch 1616/2000, Train Loss: 0.071053, Val Loss: 0.148847\n",
      "Epoch 1617/2000, Train Loss: 0.070894, Val Loss: 0.149716\n",
      "Epoch 1618/2000, Train Loss: 0.070892, Val Loss: 0.148130\n",
      "Epoch 1619/2000, Train Loss: 0.070830, Val Loss: 0.149043\n",
      "Epoch 1620/2000, Train Loss: 0.070801, Val Loss: 0.148563\n",
      "Epoch 1621/2000, Train Loss: 0.070695, Val Loss: 0.149043\n",
      "Epoch 1622/2000, Train Loss: 0.070631, Val Loss: 0.149372\n",
      "Epoch 1623/2000, Train Loss: 0.070628, Val Loss: 0.148949\n",
      "Epoch 1624/2000, Train Loss: 0.070594, Val Loss: 0.148760\n",
      "Epoch 1625/2000, Train Loss: 0.070617, Val Loss: 0.149217\n",
      "Epoch 1626/2000, Train Loss: 0.070648, Val Loss: 0.148725\n",
      "Epoch 1627/2000, Train Loss: 0.070610, Val Loss: 0.149149\n",
      "Epoch 1628/2000, Train Loss: 0.070595, Val Loss: 0.148763\n",
      "Epoch 1629/2000, Train Loss: 0.070602, Val Loss: 0.149669\n",
      "Epoch 1630/2000, Train Loss: 0.070612, Val Loss: 0.148531\n",
      "Epoch 1631/2000, Train Loss: 0.070636, Val Loss: 0.149532\n",
      "Epoch 1632/2000, Train Loss: 0.070604, Val Loss: 0.148991\n",
      "Epoch 1633/2000, Train Loss: 0.070615, Val Loss: 0.149352\n",
      "Epoch 1634/2000, Train Loss: 0.070552, Val Loss: 0.148534\n",
      "Epoch 1635/2000, Train Loss: 0.070567, Val Loss: 0.149646\n",
      "Epoch 1636/2000, Train Loss: 0.070545, Val Loss: 0.148632\n",
      "Epoch 1637/2000, Train Loss: 0.070551, Val Loss: 0.149218\n",
      "Epoch 1638/2000, Train Loss: 0.070562, Val Loss: 0.148948\n",
      "Epoch 1639/2000, Train Loss: 0.070570, Val Loss: 0.149570\n",
      "Epoch 1640/2000, Train Loss: 0.070581, Val Loss: 0.148821\n",
      "Epoch 1641/2000, Train Loss: 0.070551, Val Loss: 0.149695\n",
      "Epoch 1642/2000, Train Loss: 0.070592, Val Loss: 0.148515\n",
      "Epoch 1643/2000, Train Loss: 0.070588, Val Loss: 0.149814\n",
      "Epoch 1644/2000, Train Loss: 0.070596, Val Loss: 0.148825\n",
      "Epoch 1645/2000, Train Loss: 0.070622, Val Loss: 0.148934\n",
      "Epoch 1646/2000, Train Loss: 0.070600, Val Loss: 0.149143\n",
      "Epoch 1647/2000, Train Loss: 0.070577, Val Loss: 0.149251\n",
      "Epoch 1648/2000, Train Loss: 0.070588, Val Loss: 0.149062\n",
      "Epoch 1649/2000, Train Loss: 0.070604, Val Loss: 0.148738\n",
      "Epoch 1650/2000, Train Loss: 0.070636, Val Loss: 0.149595\n",
      "Epoch 1651/2000, Train Loss: 0.070567, Val Loss: 0.148879\n",
      "Epoch 1652/2000, Train Loss: 0.070626, Val Loss: 0.149157\n",
      "Epoch 1653/2000, Train Loss: 0.070620, Val Loss: 0.149926\n",
      "Epoch 1654/2000, Train Loss: 0.070656, Val Loss: 0.148927\n",
      "Epoch 1655/2000, Train Loss: 0.070654, Val Loss: 0.148581\n",
      "Epoch 1656/2000, Train Loss: 0.070588, Val Loss: 0.149644\n",
      "Epoch 1657/2000, Train Loss: 0.070591, Val Loss: 0.148621\n",
      "Epoch 1658/2000, Train Loss: 0.070617, Val Loss: 0.150356\n",
      "Epoch 1659/2000, Train Loss: 0.070594, Val Loss: 0.148977\n",
      "Epoch 1660/2000, Train Loss: 0.070595, Val Loss: 0.148889\n",
      "Epoch 1661/2000, Train Loss: 0.070565, Val Loss: 0.148918\n",
      "Epoch 1662/2000, Train Loss: 0.070555, Val Loss: 0.149021\n",
      "Epoch 1663/2000, Train Loss: 0.070590, Val Loss: 0.149415\n",
      "Epoch 1664/2000, Train Loss: 0.070589, Val Loss: 0.148970\n",
      "Epoch 1665/2000, Train Loss: 0.070595, Val Loss: 0.149413\n",
      "Epoch 1666/2000, Train Loss: 0.070583, Val Loss: 0.148875\n",
      "Epoch 1667/2000, Train Loss: 0.070572, Val Loss: 0.149261\n",
      "Epoch 1668/2000, Train Loss: 0.070570, Val Loss: 0.148622\n",
      "Epoch 1669/2000, Train Loss: 0.070524, Val Loss: 0.149562\n",
      "Epoch 1670/2000, Train Loss: 0.070512, Val Loss: 0.148830\n",
      "Epoch 1671/2000, Train Loss: 0.070502, Val Loss: 0.149170\n",
      "Epoch 1672/2000, Train Loss: 0.070491, Val Loss: 0.149118\n",
      "Epoch 1673/2000, Train Loss: 0.070491, Val Loss: 0.148977\n",
      "Epoch 1674/2000, Train Loss: 0.070489, Val Loss: 0.148877\n",
      "Epoch 1675/2000, Train Loss: 0.070509, Val Loss: 0.149300\n",
      "Epoch 1676/2000, Train Loss: 0.070520, Val Loss: 0.148472\n",
      "Epoch 1677/2000, Train Loss: 0.070547, Val Loss: 0.149375\n",
      "Epoch 1678/2000, Train Loss: 0.070539, Val Loss: 0.148813\n",
      "Epoch 1679/2000, Train Loss: 0.070502, Val Loss: 0.149336\n",
      "Epoch 1680/2000, Train Loss: 0.070565, Val Loss: 0.148874\n",
      "Epoch 1681/2000, Train Loss: 0.070508, Val Loss: 0.149324\n",
      "Epoch 1682/2000, Train Loss: 0.070507, Val Loss: 0.148623\n",
      "Epoch 1683/2000, Train Loss: 0.070518, Val Loss: 0.149667\n",
      "Epoch 1684/2000, Train Loss: 0.070511, Val Loss: 0.149108\n",
      "Epoch 1685/2000, Train Loss: 0.070487, Val Loss: 0.148739\n",
      "Epoch 1686/2000, Train Loss: 0.070485, Val Loss: 0.149303\n",
      "Epoch 1687/2000, Train Loss: 0.070485, Val Loss: 0.148991\n",
      "Epoch 1688/2000, Train Loss: 0.070478, Val Loss: 0.149077\n",
      "Epoch 1689/2000, Train Loss: 0.070469, Val Loss: 0.148948\n",
      "Epoch 1690/2000, Train Loss: 0.070481, Val Loss: 0.149375\n",
      "Epoch 1691/2000, Train Loss: 0.070495, Val Loss: 0.149030\n",
      "Epoch 1692/2000, Train Loss: 0.070519, Val Loss: 0.149616\n",
      "Epoch 1693/2000, Train Loss: 0.070486, Val Loss: 0.148566\n",
      "Epoch 1694/2000, Train Loss: 0.070525, Val Loss: 0.149776\n",
      "Epoch 1695/2000, Train Loss: 0.070531, Val Loss: 0.148777\n",
      "Epoch 1696/2000, Train Loss: 0.070615, Val Loss: 0.149743\n",
      "Epoch 1697/2000, Train Loss: 0.070746, Val Loss: 0.148928\n",
      "Epoch 1698/2000, Train Loss: 0.070772, Val Loss: 0.149138\n",
      "Epoch 1699/2000, Train Loss: 0.070854, Val Loss: 0.148171\n",
      "Epoch 1700/2000, Train Loss: 0.070912, Val Loss: 0.149546\n",
      "Epoch 1701/2000, Train Loss: 0.070748, Val Loss: 0.149752\n",
      "Epoch 1702/2000, Train Loss: 0.070845, Val Loss: 0.148188\n",
      "Epoch 1703/2000, Train Loss: 0.070750, Val Loss: 0.149977\n",
      "Epoch 1704/2000, Train Loss: 0.070826, Val Loss: 0.149344\n",
      "Epoch 1705/2000, Train Loss: 0.070974, Val Loss: 0.149242\n",
      "Epoch 1706/2000, Train Loss: 0.071111, Val Loss: 0.149113\n",
      "Epoch 1707/2000, Train Loss: 0.071324, Val Loss: 0.153334\n",
      "Epoch 1708/2000, Train Loss: 0.071755, Val Loss: 0.147942\n",
      "Epoch 1709/2000, Train Loss: 0.072207, Val Loss: 0.149173\n",
      "Epoch 1710/2000, Train Loss: 0.074376, Val Loss: 0.151619\n",
      "Epoch 1711/2000, Train Loss: 0.076321, Val Loss: 0.152939\n",
      "Epoch 1712/2000, Train Loss: 0.075333, Val Loss: 0.149026\n",
      "Epoch 1713/2000, Train Loss: 0.074850, Val Loss: 0.150103\n",
      "Epoch 1714/2000, Train Loss: 0.074605, Val Loss: 0.150535\n",
      "Epoch 1715/2000, Train Loss: 0.074576, Val Loss: 0.148525\n",
      "Epoch 1716/2000, Train Loss: 0.074489, Val Loss: 0.150362\n",
      "Epoch 1717/2000, Train Loss: 0.074165, Val Loss: 0.150174\n",
      "Epoch 1718/2000, Train Loss: 0.073846, Val Loss: 0.149511\n",
      "Epoch 1719/2000, Train Loss: 0.072573, Val Loss: 0.149090\n",
      "Epoch 1720/2000, Train Loss: 0.073669, Val Loss: 0.148070\n",
      "Epoch 1721/2000, Train Loss: 0.073025, Val Loss: 0.151796\n",
      "Epoch 1722/2000, Train Loss: 0.072674, Val Loss: 0.148318\n",
      "Epoch 1723/2000, Train Loss: 0.071986, Val Loss: 0.148379\n",
      "Epoch 1724/2000, Train Loss: 0.071752, Val Loss: 0.149207\n",
      "Epoch 1725/2000, Train Loss: 0.071593, Val Loss: 0.148548\n",
      "Epoch 1726/2000, Train Loss: 0.071270, Val Loss: 0.148854\n",
      "Epoch 1727/2000, Train Loss: 0.071175, Val Loss: 0.148263\n",
      "Epoch 1728/2000, Train Loss: 0.071049, Val Loss: 0.148578\n",
      "Epoch 1729/2000, Train Loss: 0.071028, Val Loss: 0.148350\n",
      "Epoch 1730/2000, Train Loss: 0.071000, Val Loss: 0.149377\n",
      "Epoch 1731/2000, Train Loss: 0.070966, Val Loss: 0.147926\n",
      "Epoch 1732/2000, Train Loss: 0.070907, Val Loss: 0.148753\n",
      "Epoch 1733/2000, Train Loss: 0.071044, Val Loss: 0.148096\n",
      "Epoch 1734/2000, Train Loss: 0.070942, Val Loss: 0.148730\n",
      "Epoch 1735/2000, Train Loss: 0.070864, Val Loss: 0.148959\n",
      "Epoch 1736/2000, Train Loss: 0.070868, Val Loss: 0.148523\n",
      "Epoch 1737/2000, Train Loss: 0.070849, Val Loss: 0.149098\n",
      "Epoch 1738/2000, Train Loss: 0.070805, Val Loss: 0.148551\n",
      "Epoch 1739/2000, Train Loss: 0.070840, Val Loss: 0.148869\n",
      "Epoch 1740/2000, Train Loss: 0.070740, Val Loss: 0.149263\n",
      "Epoch 1741/2000, Train Loss: 0.070782, Val Loss: 0.148311\n",
      "Epoch 1742/2000, Train Loss: 0.070773, Val Loss: 0.149101\n",
      "Epoch 1743/2000, Train Loss: 0.070751, Val Loss: 0.148364\n",
      "Epoch 1744/2000, Train Loss: 0.070763, Val Loss: 0.149009\n",
      "Epoch 1745/2000, Train Loss: 0.070686, Val Loss: 0.148550\n",
      "Epoch 1746/2000, Train Loss: 0.070701, Val Loss: 0.148799\n",
      "Epoch 1747/2000, Train Loss: 0.070701, Val Loss: 0.149322\n",
      "Epoch 1748/2000, Train Loss: 0.070705, Val Loss: 0.148615\n",
      "Epoch 1749/2000, Train Loss: 0.070629, Val Loss: 0.148656\n",
      "Epoch 1750/2000, Train Loss: 0.070629, Val Loss: 0.148444\n",
      "Epoch 1751/2000, Train Loss: 0.070600, Val Loss: 0.148553\n",
      "Epoch 1752/2000, Train Loss: 0.070592, Val Loss: 0.149038\n",
      "Epoch 1753/2000, Train Loss: 0.070602, Val Loss: 0.148697\n",
      "Epoch 1754/2000, Train Loss: 0.070586, Val Loss: 0.148822\n",
      "Epoch 1755/2000, Train Loss: 0.070577, Val Loss: 0.148643\n",
      "Epoch 1756/2000, Train Loss: 0.070559, Val Loss: 0.148948\n",
      "Epoch 1757/2000, Train Loss: 0.070533, Val Loss: 0.148596\n",
      "Epoch 1758/2000, Train Loss: 0.070530, Val Loss: 0.148737\n",
      "Epoch 1759/2000, Train Loss: 0.070535, Val Loss: 0.148734\n",
      "Epoch 1760/2000, Train Loss: 0.070528, Val Loss: 0.148937\n",
      "Epoch 1761/2000, Train Loss: 0.070526, Val Loss: 0.148781\n",
      "Epoch 1762/2000, Train Loss: 0.070510, Val Loss: 0.148729\n",
      "Epoch 1763/2000, Train Loss: 0.070512, Val Loss: 0.148851\n",
      "Epoch 1764/2000, Train Loss: 0.070507, Val Loss: 0.148665\n",
      "Epoch 1765/2000, Train Loss: 0.070516, Val Loss: 0.148928\n",
      "Epoch 1766/2000, Train Loss: 0.070510, Val Loss: 0.148873\n",
      "Epoch 1767/2000, Train Loss: 0.070509, Val Loss: 0.148784\n",
      "Epoch 1768/2000, Train Loss: 0.070511, Val Loss: 0.148976\n",
      "Epoch 1769/2000, Train Loss: 0.070511, Val Loss: 0.148694\n",
      "Epoch 1770/2000, Train Loss: 0.070507, Val Loss: 0.149002\n",
      "Epoch 1771/2000, Train Loss: 0.070501, Val Loss: 0.148657\n",
      "Epoch 1772/2000, Train Loss: 0.070510, Val Loss: 0.149049\n",
      "Epoch 1773/2000, Train Loss: 0.070549, Val Loss: 0.149018\n",
      "Epoch 1774/2000, Train Loss: 0.070568, Val Loss: 0.148606\n",
      "Epoch 1775/2000, Train Loss: 0.070575, Val Loss: 0.149308\n",
      "Epoch 1776/2000, Train Loss: 0.070715, Val Loss: 0.148990\n",
      "Epoch 1777/2000, Train Loss: 0.070805, Val Loss: 0.148569\n",
      "Epoch 1778/2000, Train Loss: 0.070931, Val Loss: 0.150688\n",
      "Epoch 1779/2000, Train Loss: 0.070957, Val Loss: 0.148650\n",
      "Epoch 1780/2000, Train Loss: 0.070664, Val Loss: 0.148707\n",
      "Epoch 1781/2000, Train Loss: 0.070683, Val Loss: 0.148708\n",
      "Epoch 1782/2000, Train Loss: 0.070635, Val Loss: 0.149386\n",
      "Epoch 1783/2000, Train Loss: 0.070649, Val Loss: 0.149179\n",
      "Epoch 1784/2000, Train Loss: 0.070599, Val Loss: 0.148648\n",
      "Epoch 1785/2000, Train Loss: 0.070602, Val Loss: 0.149108\n",
      "Epoch 1786/2000, Train Loss: 0.070606, Val Loss: 0.148854\n",
      "Epoch 1787/2000, Train Loss: 0.070589, Val Loss: 0.148959\n",
      "Epoch 1788/2000, Train Loss: 0.070581, Val Loss: 0.148857\n",
      "Epoch 1789/2000, Train Loss: 0.070568, Val Loss: 0.148955\n",
      "Epoch 1790/2000, Train Loss: 0.070522, Val Loss: 0.149326\n",
      "Epoch 1791/2000, Train Loss: 0.070517, Val Loss: 0.148762\n",
      "Epoch 1792/2000, Train Loss: 0.070497, Val Loss: 0.148882\n",
      "Epoch 1793/2000, Train Loss: 0.070483, Val Loss: 0.148860\n",
      "Epoch 1794/2000, Train Loss: 0.070482, Val Loss: 0.148979\n",
      "Epoch 1795/2000, Train Loss: 0.070485, Val Loss: 0.149084\n",
      "Epoch 1796/2000, Train Loss: 0.070481, Val Loss: 0.148813\n",
      "Epoch 1797/2000, Train Loss: 0.070483, Val Loss: 0.148902\n",
      "Epoch 1798/2000, Train Loss: 0.070481, Val Loss: 0.149041\n",
      "Epoch 1799/2000, Train Loss: 0.070496, Val Loss: 0.148926\n",
      "Epoch 1800/2000, Train Loss: 0.070493, Val Loss: 0.148829\n",
      "Epoch 1801/2000, Train Loss: 0.070495, Val Loss: 0.149190\n",
      "Epoch 1802/2000, Train Loss: 0.070486, Val Loss: 0.149049\n",
      "Epoch 1803/2000, Train Loss: 0.070478, Val Loss: 0.149010\n",
      "Epoch 1804/2000, Train Loss: 0.070473, Val Loss: 0.148821\n",
      "Epoch 1805/2000, Train Loss: 0.070469, Val Loss: 0.149155\n",
      "Epoch 1806/2000, Train Loss: 0.070467, Val Loss: 0.148896\n",
      "Epoch 1807/2000, Train Loss: 0.070468, Val Loss: 0.149014\n",
      "Epoch 1808/2000, Train Loss: 0.070464, Val Loss: 0.148902\n",
      "Epoch 1809/2000, Train Loss: 0.070461, Val Loss: 0.148997\n",
      "Epoch 1810/2000, Train Loss: 0.070458, Val Loss: 0.149080\n",
      "Epoch 1811/2000, Train Loss: 0.070466, Val Loss: 0.149027\n",
      "Epoch 1812/2000, Train Loss: 0.070471, Val Loss: 0.149060\n",
      "Epoch 1813/2000, Train Loss: 0.070466, Val Loss: 0.148995\n",
      "Epoch 1814/2000, Train Loss: 0.070466, Val Loss: 0.148978\n",
      "Epoch 1815/2000, Train Loss: 0.070463, Val Loss: 0.149120\n",
      "Epoch 1816/2000, Train Loss: 0.070465, Val Loss: 0.148988\n",
      "Epoch 1817/2000, Train Loss: 0.070472, Val Loss: 0.149221\n",
      "Epoch 1818/2000, Train Loss: 0.070467, Val Loss: 0.149160\n",
      "Epoch 1819/2000, Train Loss: 0.070467, Val Loss: 0.148874\n",
      "Epoch 1820/2000, Train Loss: 0.070483, Val Loss: 0.149532\n",
      "Epoch 1821/2000, Train Loss: 0.070488, Val Loss: 0.148961\n",
      "Epoch 1822/2000, Train Loss: 0.070500, Val Loss: 0.148865\n",
      "Epoch 1823/2000, Train Loss: 0.070525, Val Loss: 0.148902\n",
      "Epoch 1824/2000, Train Loss: 0.070543, Val Loss: 0.149377\n",
      "Epoch 1825/2000, Train Loss: 0.070512, Val Loss: 0.149031\n",
      "Epoch 1826/2000, Train Loss: 0.070578, Val Loss: 0.149176\n",
      "Epoch 1827/2000, Train Loss: 0.070545, Val Loss: 0.149453\n",
      "Epoch 1828/2000, Train Loss: 0.070530, Val Loss: 0.148787\n",
      "Epoch 1829/2000, Train Loss: 0.070733, Val Loss: 0.149056\n",
      "Epoch 1830/2000, Train Loss: 0.070689, Val Loss: 0.149224\n",
      "Epoch 1831/2000, Train Loss: 0.070568, Val Loss: 0.148988\n",
      "Epoch 1832/2000, Train Loss: 0.070591, Val Loss: 0.149567\n",
      "Epoch 1833/2000, Train Loss: 0.070551, Val Loss: 0.149048\n",
      "Epoch 1834/2000, Train Loss: 0.070534, Val Loss: 0.148955\n",
      "Epoch 1835/2000, Train Loss: 0.070600, Val Loss: 0.149508\n",
      "Epoch 1836/2000, Train Loss: 0.070644, Val Loss: 0.148656\n",
      "Epoch 1837/2000, Train Loss: 0.070616, Val Loss: 0.150176\n",
      "Epoch 1838/2000, Train Loss: 0.070572, Val Loss: 0.148787\n",
      "Epoch 1839/2000, Train Loss: 0.070574, Val Loss: 0.149126\n",
      "Epoch 1840/2000, Train Loss: 0.070560, Val Loss: 0.148817\n",
      "Epoch 1841/2000, Train Loss: 0.070563, Val Loss: 0.149491\n",
      "Epoch 1842/2000, Train Loss: 0.070513, Val Loss: 0.149208\n",
      "Epoch 1843/2000, Train Loss: 0.070499, Val Loss: 0.148950\n",
      "Epoch 1844/2000, Train Loss: 0.070492, Val Loss: 0.148977\n",
      "Epoch 1845/2000, Train Loss: 0.070507, Val Loss: 0.149165\n",
      "Epoch 1846/2000, Train Loss: 0.070504, Val Loss: 0.148896\n",
      "Epoch 1847/2000, Train Loss: 0.070525, Val Loss: 0.149243\n",
      "Epoch 1848/2000, Train Loss: 0.070496, Val Loss: 0.148991\n",
      "Epoch 1849/2000, Train Loss: 0.070473, Val Loss: 0.149009\n",
      "Epoch 1850/2000, Train Loss: 0.070475, Val Loss: 0.148854\n",
      "Epoch 1851/2000, Train Loss: 0.070475, Val Loss: 0.149408\n",
      "Epoch 1852/2000, Train Loss: 0.070480, Val Loss: 0.148929\n",
      "Epoch 1853/2000, Train Loss: 0.070499, Val Loss: 0.149373\n",
      "Epoch 1854/2000, Train Loss: 0.070506, Val Loss: 0.149059\n",
      "Epoch 1855/2000, Train Loss: 0.070484, Val Loss: 0.149109\n",
      "Epoch 1856/2000, Train Loss: 0.070507, Val Loss: 0.148977\n",
      "Epoch 1857/2000, Train Loss: 0.070551, Val Loss: 0.149388\n",
      "Epoch 1858/2000, Train Loss: 0.070561, Val Loss: 0.148901\n",
      "Epoch 1859/2000, Train Loss: 0.070530, Val Loss: 0.149502\n",
      "Epoch 1860/2000, Train Loss: 0.070549, Val Loss: 0.149282\n",
      "Epoch 1861/2000, Train Loss: 0.070541, Val Loss: 0.149023\n",
      "Epoch 1862/2000, Train Loss: 0.070556, Val Loss: 0.149509\n",
      "Epoch 1863/2000, Train Loss: 0.070541, Val Loss: 0.149137\n",
      "Epoch 1864/2000, Train Loss: 0.070522, Val Loss: 0.148841\n",
      "Epoch 1865/2000, Train Loss: 0.070547, Val Loss: 0.148882\n",
      "Epoch 1866/2000, Train Loss: 0.070550, Val Loss: 0.149987\n",
      "Epoch 1867/2000, Train Loss: 0.070573, Val Loss: 0.148810\n",
      "Epoch 1868/2000, Train Loss: 0.070535, Val Loss: 0.148966\n",
      "Epoch 1869/2000, Train Loss: 0.070516, Val Loss: 0.149460\n",
      "Epoch 1870/2000, Train Loss: 0.070509, Val Loss: 0.149313\n",
      "Epoch 1871/2000, Train Loss: 0.070527, Val Loss: 0.148939\n",
      "Epoch 1872/2000, Train Loss: 0.070631, Val Loss: 0.150300\n",
      "Epoch 1873/2000, Train Loss: 0.070611, Val Loss: 0.149627\n",
      "Epoch 1874/2000, Train Loss: 0.070643, Val Loss: 0.149121\n",
      "Epoch 1875/2000, Train Loss: 0.070731, Val Loss: 0.150731\n",
      "Epoch 1876/2000, Train Loss: 0.070794, Val Loss: 0.148825\n",
      "Epoch 1877/2000, Train Loss: 0.070656, Val Loss: 0.149244\n",
      "Epoch 1878/2000, Train Loss: 0.070696, Val Loss: 0.150607\n",
      "Epoch 1879/2000, Train Loss: 0.070874, Val Loss: 0.148932\n",
      "Epoch 1880/2000, Train Loss: 0.070997, Val Loss: 0.148575\n",
      "Epoch 1881/2000, Train Loss: 0.070881, Val Loss: 0.150810\n",
      "Epoch 1882/2000, Train Loss: 0.070923, Val Loss: 0.148872\n",
      "Epoch 1883/2000, Train Loss: 0.070992, Val Loss: 0.149610\n",
      "Epoch 1884/2000, Train Loss: 0.070914, Val Loss: 0.149956\n",
      "Epoch 1885/2000, Train Loss: 0.071046, Val Loss: 0.149068\n",
      "Epoch 1886/2000, Train Loss: 0.071053, Val Loss: 0.149378\n",
      "Epoch 1887/2000, Train Loss: 0.071246, Val Loss: 0.150465\n",
      "Epoch 1888/2000, Train Loss: 0.071573, Val Loss: 0.150276\n",
      "Epoch 1889/2000, Train Loss: 0.071284, Val Loss: 0.150116\n",
      "Epoch 1890/2000, Train Loss: 0.071129, Val Loss: 0.151075\n",
      "Epoch 1891/2000, Train Loss: 0.071862, Val Loss: 0.149627\n",
      "Epoch 1892/2000, Train Loss: 0.071660, Val Loss: 0.148935\n",
      "Epoch 1893/2000, Train Loss: 0.071267, Val Loss: 0.150655\n",
      "Epoch 1894/2000, Train Loss: 0.072108, Val Loss: 0.149780\n",
      "Epoch 1895/2000, Train Loss: 0.072471, Val Loss: 0.155919\n",
      "Epoch 1896/2000, Train Loss: 0.073081, Val Loss: 0.149462\n",
      "Epoch 1897/2000, Train Loss: 0.072344, Val Loss: 0.150333\n",
      "Epoch 1898/2000, Train Loss: 0.075185, Val Loss: 0.150135\n",
      "Epoch 1899/2000, Train Loss: 0.076347, Val Loss: 0.149037\n",
      "Epoch 1900/2000, Train Loss: 0.074436, Val Loss: 0.148853\n",
      "Epoch 1901/2000, Train Loss: 0.075481, Val Loss: 0.150589\n",
      "Epoch 1902/2000, Train Loss: 0.073947, Val Loss: 0.148822\n",
      "Epoch 1903/2000, Train Loss: 0.074448, Val Loss: 0.150107\n",
      "Epoch 1904/2000, Train Loss: 0.073739, Val Loss: 0.149094\n",
      "Epoch 1905/2000, Train Loss: 0.073112, Val Loss: 0.150608\n",
      "Epoch 1906/2000, Train Loss: 0.073053, Val Loss: 0.149585\n",
      "Epoch 1907/2000, Train Loss: 0.072932, Val Loss: 0.148387\n",
      "Epoch 1908/2000, Train Loss: 0.072512, Val Loss: 0.148236\n",
      "Epoch 1909/2000, Train Loss: 0.072134, Val Loss: 0.148111\n",
      "Epoch 1910/2000, Train Loss: 0.072027, Val Loss: 0.148672\n",
      "Epoch 1911/2000, Train Loss: 0.071936, Val Loss: 0.148922\n",
      "Epoch 1912/2000, Train Loss: 0.071885, Val Loss: 0.148931\n",
      "Epoch 1913/2000, Train Loss: 0.071761, Val Loss: 0.148928\n",
      "Epoch 1914/2000, Train Loss: 0.071715, Val Loss: 0.148632\n",
      "Epoch 1915/2000, Train Loss: 0.071678, Val Loss: 0.148547\n",
      "Epoch 1916/2000, Train Loss: 0.071685, Val Loss: 0.148945\n",
      "Epoch 1917/2000, Train Loss: 0.071660, Val Loss: 0.149001\n",
      "Epoch 1918/2000, Train Loss: 0.071638, Val Loss: 0.149437\n",
      "Epoch 1919/2000, Train Loss: 0.071742, Val Loss: 0.148592\n",
      "Epoch 1920/2000, Train Loss: 0.071847, Val Loss: 0.149376\n",
      "Epoch 1921/2000, Train Loss: 0.071727, Val Loss: 0.148935\n",
      "Epoch 1922/2000, Train Loss: 0.071746, Val Loss: 0.149411\n",
      "Epoch 1923/2000, Train Loss: 0.071751, Val Loss: 0.149332\n",
      "Epoch 1924/2000, Train Loss: 0.071731, Val Loss: 0.149123\n",
      "Epoch 1925/2000, Train Loss: 0.071751, Val Loss: 0.150092\n",
      "Epoch 1926/2000, Train Loss: 0.071683, Val Loss: 0.149139\n",
      "Epoch 1927/2000, Train Loss: 0.071726, Val Loss: 0.149731\n",
      "Epoch 1928/2000, Train Loss: 0.071723, Val Loss: 0.149325\n",
      "Epoch 1929/2000, Train Loss: 0.071880, Val Loss: 0.148939\n",
      "Epoch 1930/2000, Train Loss: 0.071652, Val Loss: 0.149235\n",
      "Epoch 1931/2000, Train Loss: 0.071613, Val Loss: 0.149428\n",
      "Epoch 1932/2000, Train Loss: 0.071456, Val Loss: 0.148493\n",
      "Epoch 1933/2000, Train Loss: 0.071415, Val Loss: 0.149165\n",
      "Epoch 1934/2000, Train Loss: 0.071655, Val Loss: 0.149563\n",
      "Epoch 1935/2000, Train Loss: 0.071419, Val Loss: 0.149382\n",
      "Epoch 1936/2000, Train Loss: 0.071579, Val Loss: 0.149891\n",
      "Epoch 1937/2000, Train Loss: 0.071465, Val Loss: 0.148869\n",
      "Epoch 1938/2000, Train Loss: 0.071397, Val Loss: 0.148652\n",
      "Epoch 1939/2000, Train Loss: 0.071325, Val Loss: 0.149789\n",
      "Epoch 1940/2000, Train Loss: 0.071371, Val Loss: 0.149438\n",
      "Epoch 1941/2000, Train Loss: 0.071632, Val Loss: 0.148861\n",
      "Epoch 1942/2000, Train Loss: 0.071498, Val Loss: 0.148946\n",
      "Epoch 1943/2000, Train Loss: 0.071191, Val Loss: 0.149009\n",
      "Epoch 1944/2000, Train Loss: 0.071238, Val Loss: 0.149399\n",
      "Epoch 1945/2000, Train Loss: 0.071308, Val Loss: 0.150008\n",
      "Epoch 1946/2000, Train Loss: 0.071405, Val Loss: 0.148875\n",
      "Epoch 1947/2000, Train Loss: 0.071485, Val Loss: 0.149801\n",
      "Epoch 1948/2000, Train Loss: 0.071388, Val Loss: 0.150101\n",
      "Epoch 1949/2000, Train Loss: 0.071481, Val Loss: 0.149603\n",
      "Epoch 1950/2000, Train Loss: 0.071290, Val Loss: 0.149715\n",
      "Epoch 1951/2000, Train Loss: 0.071292, Val Loss: 0.150228\n",
      "Epoch 1952/2000, Train Loss: 0.071282, Val Loss: 0.149590\n",
      "Epoch 1953/2000, Train Loss: 0.071135, Val Loss: 0.150237\n",
      "Epoch 1954/2000, Train Loss: 0.071061, Val Loss: 0.150534\n",
      "Epoch 1955/2000, Train Loss: 0.071001, Val Loss: 0.149663\n",
      "Epoch 1956/2000, Train Loss: 0.070966, Val Loss: 0.149705\n",
      "Epoch 1957/2000, Train Loss: 0.070910, Val Loss: 0.150580\n",
      "Epoch 1958/2000, Train Loss: 0.070941, Val Loss: 0.149630\n",
      "Epoch 1959/2000, Train Loss: 0.071038, Val Loss: 0.150181\n",
      "Epoch 1960/2000, Train Loss: 0.070980, Val Loss: 0.150103\n",
      "Epoch 1961/2000, Train Loss: 0.071087, Val Loss: 0.150161\n",
      "Epoch 1962/2000, Train Loss: 0.070902, Val Loss: 0.150070\n",
      "Epoch 1963/2000, Train Loss: 0.070984, Val Loss: 0.150679\n",
      "Epoch 1964/2000, Train Loss: 0.070897, Val Loss: 0.150540\n",
      "Epoch 1965/2000, Train Loss: 0.070835, Val Loss: 0.150943\n",
      "Epoch 1966/2000, Train Loss: 0.071057, Val Loss: 0.150376\n",
      "Epoch 1967/2000, Train Loss: 0.071325, Val Loss: 0.151077\n",
      "Epoch 1968/2000, Train Loss: 0.070968, Val Loss: 0.150447\n",
      "Epoch 1969/2000, Train Loss: 0.070915, Val Loss: 0.149857\n",
      "Epoch 1970/2000, Train Loss: 0.070839, Val Loss: 0.149910\n",
      "Epoch 1971/2000, Train Loss: 0.070820, Val Loss: 0.150652\n",
      "Epoch 1972/2000, Train Loss: 0.070786, Val Loss: 0.149951\n",
      "Epoch 1973/2000, Train Loss: 0.070790, Val Loss: 0.151232\n",
      "Epoch 1974/2000, Train Loss: 0.070919, Val Loss: 0.149895\n",
      "Epoch 1975/2000, Train Loss: 0.071302, Val Loss: 0.151750\n",
      "Epoch 1976/2000, Train Loss: 0.071233, Val Loss: 0.150226\n",
      "Epoch 1977/2000, Train Loss: 0.071174, Val Loss: 0.150435\n",
      "Epoch 1978/2000, Train Loss: 0.071184, Val Loss: 0.150833\n",
      "Epoch 1979/2000, Train Loss: 0.071157, Val Loss: 0.150835\n",
      "Epoch 1980/2000, Train Loss: 0.071287, Val Loss: 0.149843\n",
      "Epoch 1981/2000, Train Loss: 0.071190, Val Loss: 0.150786\n",
      "Epoch 1982/2000, Train Loss: 0.071221, Val Loss: 0.151048\n",
      "Epoch 1983/2000, Train Loss: 0.071144, Val Loss: 0.150767\n",
      "Epoch 1984/2000, Train Loss: 0.071009, Val Loss: 0.151735\n",
      "Epoch 1985/2000, Train Loss: 0.070795, Val Loss: 0.150131\n",
      "Epoch 1986/2000, Train Loss: 0.070740, Val Loss: 0.151164\n",
      "Epoch 1987/2000, Train Loss: 0.070827, Val Loss: 0.151748\n",
      "Epoch 1988/2000, Train Loss: 0.070707, Val Loss: 0.151057\n",
      "Epoch 1989/2000, Train Loss: 0.070700, Val Loss: 0.150762\n",
      "Epoch 1990/2000, Train Loss: 0.070686, Val Loss: 0.150273\n",
      "Epoch 1991/2000, Train Loss: 0.070698, Val Loss: 0.152328\n",
      "Epoch 1992/2000, Train Loss: 0.070745, Val Loss: 0.151076\n",
      "Epoch 1993/2000, Train Loss: 0.070646, Val Loss: 0.150817\n",
      "Epoch 1994/2000, Train Loss: 0.070698, Val Loss: 0.150600\n",
      "Epoch 1995/2000, Train Loss: 0.070642, Val Loss: 0.151472\n",
      "Epoch 1996/2000, Train Loss: 0.070605, Val Loss: 0.151264\n",
      "Epoch 1997/2000, Train Loss: 0.070545, Val Loss: 0.151247\n",
      "Epoch 1998/2000, Train Loss: 0.070540, Val Loss: 0.151107\n",
      "Epoch 1999/2000, Train Loss: 0.070532, Val Loss: 0.151384\n",
      "Epoch 2000/2000, Train Loss: 0.070560, Val Loss: 0.150988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[271]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m results = evaluate_model_on_experiments(csv_path, trainer, output_dir=\u001b[33m'\u001b[39m\u001b[33mexperiment_results\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# R² 스코어 계산\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m r2_scores = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcalculate_r2_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[268]\u001b[39m\u001b[32m, line 425\u001b[39m, in \u001b[36mMembraneSystemTrainer.calculate_r2_scores\u001b[39m\u001b[34m(self, csv_path)\u001b[39m\n\u001b[32m    416\u001b[39m operation_params = {\n\u001b[32m    417\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m: first_row[\u001b[33m'\u001b[39m\u001b[33mT\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    418\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvoltage\u001b[39m\u001b[33m'\u001b[39m: first_row[\u001b[33m'\u001b[39m\u001b[33mV\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mki\u001b[39m\u001b[33m'\u001b[39m: first_row[\u001b[33m'\u001b[39m\u001b[33mKi\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    422\u001b[39m }\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# 실험 평가\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m eval_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m comparison = eval_result[\u001b[33m'\u001b[39m\u001b[33mcomparison\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    428\u001b[39m \u001b[38;5;66;03m# 실제 값과 예측 값 수집\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[268]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mMembraneSystemTrainer.evaluate_experiment\u001b[39m\u001b[34m(self, exp_data, initial_conditions, operation_params)\u001b[39m\n\u001b[32m    107\u001b[39m num_steps = \u001b[38;5;28mlen\u001b[39m(sim_time_points) - \u001b[32m1\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# 시뮬레이션 실행\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m sim_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# 실제 데이터에 가장 가까운 시뮬레이션 시간점 찾기\u001b[39;00m\n\u001b[32m    113\u001b[39m comparison = {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[268]\u001b[39m\u001b[32m, line 233\u001b[39m, in \u001b[36mMembraneSystemTrainer.simulate\u001b[39m\u001b[34m(self, initial_conditions, operation_params, num_steps)\u001b[39m\n\u001b[32m    230\u001b[39m input_tensor = torch.FloatTensor([sequence_buffer]).to(\u001b[38;5;28mself\u001b[39m.device)  \u001b[38;5;66;03m# [batch_size=1, seq_len, features]\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# 예측 실행\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m mol_change_pred, state_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# 마지막 시간 단계에 대한 예측만 사용\u001b[39;00m\n\u001b[32m    236\u001b[39m mol_change_np = mol_change_pred.squeeze()[-\u001b[32m1\u001b[39m].cpu().numpy() \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\repos_python\\Project\\BMED\\BMED_Model\\bmed-NN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\repos_python\\Project\\BMED\\BMED_Model\\bmed-NN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[266]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mMembraneSystemModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     57\u001b[39m new_states_list = []\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     new_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mphysics_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol_changes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     new_states_list.append(new_state.unsqueeze(\u001b[32m1\u001b[39m))\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 모든 시간 단계의 결과를 결합\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\repos_python\\Project\\BMED\\BMED_Model\\bmed-NN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\repos_python\\Project\\BMED\\BMED_Model\\bmed-NN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[265]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mPhysicsLayer.forward\u001b[39m\u001b[34m(self, mol_changes, features)\u001b[39m\n\u001b[32m     50\u001b[39m feed_k_mol = current_feed_k * current_feed_volume\n\u001b[32m     51\u001b[39m acid_la_mol = current_acid_la * current_acid_volume\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m acid_k_mol = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43macid_la_mol\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Acid에는 K+가 없음\u001b[39;00m\n\u001b[32m     53\u001b[39m base_la_mol = torch.zeros_like(current_base_k)  \u001b[38;5;66;03m# Base에는 LA가 없음\u001b[39;00m\n\u001b[32m     54\u001b[39m base_k_mol = current_base_k * current_base_volume\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # CSV 파일 경로\n",
    "    csv_path = 'BMED_data_v5.csv'\n",
    "    \n",
    "    # 모델 훈련\n",
    "    model, trainer, train_losses, val_losses = train_model_from_csv(\n",
    "        csv_path=csv_path,\n",
    "        lstm_units=64,\n",
    "        sequence_length=3,  # 이전 3개 시점 데이터 사용\n",
    "        time_step=0.01,\n",
    "        batch_size=16,\n",
    "        epochs=2000,\n",
    "        train_ratio=0.8,\n",
    "        mol_change_weight=1.0,\n",
    "        state_weight=1.0\n",
    "    )\n",
    "    \n",
    "    # 학습 곡선 시각화\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 모델 저장\n",
    "    torch.save(model.state_dict(), 'membrane_model.pth')\n",
    "    \n",
    "    # 모든 실험에 대한 평가\n",
    "    results = evaluate_model_on_experiments(csv_path, trainer, output_dir='experiment_results')\n",
    "    \n",
    "    # R² 스코어 계산\n",
    "    r2_scores = trainer.calculate_r2_scores(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmed-NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
