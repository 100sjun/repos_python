{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d17c793950>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix the random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward network for migration prediction\n",
    "class MigrationPredictor(nn.Module):\n",
    "    def __init__(self, hidden_nodes = 64, hidden_layers = 3, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_features = 7\n",
    "        n_outputs = 4\n",
    "\n",
    "        # Layer configuration\n",
    "        layers = []\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(n_features, hidden_nodes))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # hidden layers\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_nodes, hidden_nodes))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(hidden_nodes, n_outputs))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical layers for state update\n",
    "class PhysicalLayer:\n",
    "    def __init__(self, dt = 0.1):\n",
    "        self.dt = dt\n",
    "    \n",
    "    def update_state(self, cur_states, migrations):\n",
    "        # Current States\n",
    "        T = cur_states[0]\n",
    "        V = cur_states[1]\n",
    "        E = cur_states[2]\n",
    "        CF_LA = cur_states[3]\n",
    "        CA_LA = cur_states[4]\n",
    "        CF_K = cur_states[5]\n",
    "        CB_K = cur_states[6]\n",
    "        VF = cur_states[7]\n",
    "        VA = cur_states[8]\n",
    "        VB = cur_states[9]\n",
    "        \n",
    "        # Migration\n",
    "        dNLA = migrations[0] * self.dt\n",
    "        dNK = migrations[1] * self.dt\n",
    "        dVA = migrations[2] * self.dt\n",
    "        dVB = migrations[3] * self.dt\n",
    "\n",
    "        # Fixed variables\n",
    "        nT = T\n",
    "        nV = V\n",
    "        nE = E     \n",
    "        \n",
    "        # New Volumes\n",
    "        nVF = VF - dVA - dVB\n",
    "        nVA = VA + dVA\n",
    "        nVB = VB + dVB\n",
    "\n",
    "        # New Concentrations\n",
    "        nCF_LA = (CF_LA * VF - dNLA) / nVF\n",
    "        nCA_LA = (CA_LA * VA + dNLA) / nVA\n",
    "        nCF_K = (CF_K * VF - dNK) / nVF\n",
    "        nCB_K = (CB_K * VB + dNK) / nVB\n",
    "\n",
    "        # New States\n",
    "        new_states = cur_states.clone()\n",
    "        new_states[0] = nT\n",
    "        new_states[1] = nV\n",
    "        new_states[2] = nE\n",
    "        new_states[3] = nCF_LA\n",
    "        new_states[4] = nCA_LA\n",
    "        new_states[5] = nCF_K\n",
    "        new_states[6] = nCB_K\n",
    "        new_states[7] = nVF\n",
    "        new_states[8] = nVA\n",
    "        new_states[9] = nVB\n",
    "        \n",
    "        return new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "class BMEDDataset(Dataset):\n",
    "    def __init__(self, dict_spline):\n",
    "        self.states = ['T', 'V', 'E', 'CF_LA', 'CA_LA', 'CF_K', 'CB_K', 'VF', 'VA', 'VB']\n",
    "        self.experiments = []\n",
    "\n",
    "        for exp_id, exp_data in dict_spline.items():\n",
    "            # Save whole data of each experiment in one sample\n",
    "            exp_array = exp_data[self.states].values\n",
    "            times = exp_data['t'].values\n",
    "            self.experiments.append({\n",
    "                'init_state': torch.tensor(exp_array[0], dtype = torch.float32), # initial state\n",
    "                'measured_state': torch.tensor(exp_array, dtype = torch.float32), # whole measurements\n",
    "                'times': torch.tensor(times, dtype = torch.float32) # time points\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.experiments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.experiments[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics-informed neural network\n",
    "class BMEDModel(nn.Module):\n",
    "    def __init__(self, hidden_nodes = 192, hidden_layers = 3, dt = 0.1, scaler = None, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.migration_predictor = MigrationPredictor(hidden_nodes, hidden_layers, dropout = dropout)\n",
    "        self.physical_layer = PhysicalLayer(dt)\n",
    "        self.scaler = scaler\n",
    "        self.dt = dt\n",
    "\n",
    "    def forward(self, init_state, times):\n",
    "\n",
    "        cur_state = init_state # batch size 1\n",
    "        cur_time = 0.0\n",
    "        pred_states = []\n",
    "        measured_indices = []\n",
    "\n",
    "        times = times\n",
    "        times_np = times[0].numpy()\n",
    "        max_time = times_np[-1]\n",
    "        measured_indices.append(0)\n",
    "\n",
    "        # 초기 상태 저장\n",
    "        pred_states.append(cur_state)\n",
    "\n",
    "        while cur_time < max_time:\n",
    "            # input_feature에 해당하는 변수만 정규화\n",
    "            input_state = cur_state[:, :7]  # input feature 추출, 2차원 유지지\n",
    "            \n",
    "            norm_input = self.scaler.transform(input_state.detach().numpy())\n",
    "            norm_input = torch.tensor(norm_input)\n",
    "            \n",
    "            # 상태 예측\n",
    "            migration = self.migration_predictor(norm_input)  # (1, 6) -> (1, 3)\n",
    "            cur_state = self.physical_layer.update_state(cur_state[0], migration[0]).unsqueeze(0)  # (1,8)\n",
    "            pred_states.append(cur_state)  # (1, 8)\n",
    "            cur_time += self.dt\n",
    "\n",
    "            # 측정 시간과 매칭\n",
    "            for t in times_np:\n",
    "                if abs(cur_time - t) < self.dt/2:\n",
    "                    measured_indices.append(len(pred_states) - 1)\n",
    "\n",
    "        # 현재 배치의 예측 상태들을 스택\n",
    "        pred_states = torch.cat(pred_states, dim=0)  # (n_steps, 8)\n",
    "\n",
    "        return pred_states, measured_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare custom loss\n",
    "def custom_loss(pred_states, measured_indices, measured_states):\n",
    "\n",
    "    # default weight\n",
    "    default_wt = {\n",
    "        'T': 0,\n",
    "        'V': 0,\n",
    "        'E': 0,\n",
    "        'CF_LA': 1,\n",
    "        'CA_LA': 1,\n",
    "        'CF_K': 0.1,\n",
    "        'CB_K': 0.1,\n",
    "        'VF': 0.5,\n",
    "        'VA': 1,\n",
    "        'VB': 0.1\n",
    "    }\n",
    "\n",
    "    wt_tensor = torch.tensor([\n",
    "        default_wt['T'], default_wt['V'], default_wt['E'], \n",
    "        default_wt['CF_LA'], default_wt['CA_LA'], default_wt['CF_K'], default_wt['CB_K'],\n",
    "        default_wt['VF'], default_wt['VA'], default_wt['VB']\n",
    "    ])\n",
    "\n",
    "    total_loss = 0\n",
    "    for idx, measured_state in zip(measured_indices, measured_states[0]):\n",
    "        predicted_state = pred_states[idx]\n",
    "        sq_errors = (predicted_state - measured_state) ** 2\n",
    "        wt_errors = sq_errors * wt_tensor\n",
    "\n",
    "        total_loss += torch.mean(wt_errors)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_exp = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for exp in val_loader:\n",
    "            init_state = exp['init_state']\n",
    "            measured_states = exp['measured_state']\n",
    "            times = exp['times']\n",
    "\n",
    "            pred_states, measured_indices = model(init_state, times)\n",
    "            \n",
    "\n",
    "            loss = custom_loss(pred_states, measured_indices, measured_states)\n",
    "            total_loss += loss.item()\n",
    "            num_exp += 1\n",
    "\n",
    "    avg_loss = total_loss / num_exp\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, train_loader, val_loader,epochs = 100, learning_rate = 0.001, weight_decay = 1e-5, clip_norm = 0.1, schd_factor = 0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay= weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience = 5, factor = schd_factor, min_lr = 1e-6\n",
    "    )\n",
    "\n",
    "    patience = 50\n",
    "    min_delta = 0.0001\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    stopped_epoch = epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for exp in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            init_state = exp['init_state']\n",
    "            measured_state = exp['measured_state']\n",
    "            times = exp['times']\n",
    "\n",
    "            # Simulation\n",
    "            pred_state, measured_indices = model(init_state, times)\n",
    "\n",
    "            # Loss\n",
    "            loss = custom_loss(pred_state, measured_indices, measured_state)\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = train_loss / num_batches\n",
    "        val_loss = evaluate_model(model, val_loader)\n",
    "        \n",
    "        # 학습률 스케줄러 업데이트\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # 매 에포크마다 검증 손실이 개선될 때마다 모델 저장\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            # 모델 저장\n",
    "            torch.save(model.state_dict(), f'best_model_epoch_{epoch+1}.pt')\n",
    "            print(f'에포크 {epoch+1}/{epochs} - 훈련 손실: {avg_train_loss:.6f}, 검증 손실: {val_loss:.6f}, 학습률: {current_lr:.8f}, 인내심 카운터: {patience_counter}')\n",
    "            print(f'검증 손실 개선! 최고 검증 손실: {best_val_loss:.6f}, 모델 저장됨')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'에포크 {epoch+1}/{epochs} - 훈련 손실: {avg_train_loss:.6f}, 검증 손실: {val_loss:.6f}, 학습률: {current_lr:.8f}, 인내심 카운터: {patience_counter}')\n",
    "\n",
    "        # Early stopping 조건 추가\n",
    "        if patience_counter >= patience:\n",
    "            print(f'조기 종료! {patience}번의 에포크 동안 검증 손실이 개선되지 않았습니다.')\n",
    "            stopped_epoch = epoch + 1\n",
    "            # 최적의 모델 상태로 복원\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "    # 학습이 완료된 후 최적의 모델 상태로 복원\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f'훈련 완료! 최종 에포크: {stopped_epoch}, 최고 검증 손실: {best_val_loss:.6f}')\n",
    "    return stopped_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load raw data\n",
    "df = pd.read_csv('BMED_data_v8.csv')\n",
    "\n",
    "# Split the data by experiment number\n",
    "dict_spline = {}\n",
    "for exp in df['exp'].unique():\n",
    "    dict_spline[exp] = df[df['exp'] == exp].sort_values('t')\n",
    "\n",
    "# Declare scaler\n",
    "scaler = StandardScaler()\n",
    "col_to_scale = ['T', 'V', 'E', 'CF_LA', 'CA_LA', 'CF_K', 'CB_K']\n",
    "scaler.fit(df[col_to_scale].values)\n",
    "\n",
    "# Load best hyperparameter set from json\n",
    "with open('hpOpt_checkpoint_v1.json', 'r') as f:\n",
    "    opt_hp_para = json.load(f)\n",
    "\n",
    "hidden_nodes = opt_hp_para['best_params']['hidden_nodes']\n",
    "hidden_layers = opt_hp_para['best_params']['hidden_layers']\n",
    "lr = opt_hp_para['best_params']['lr']\n",
    "wd = opt_hp_para['best_params']['weight_decay']\n",
    "epochs = 10000\n",
    "dropout = opt_hp_para['best_params']['dropout']/100\n",
    "clip_norm = opt_hp_para['best_params']['clip_norm']/100\n",
    "schd_factor = opt_hp_para['best_params']['schd_factor']/100\n",
    "dt = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_state': tensor([25.0600, 20.0000,  0.2500,  0.5000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
       "          1.0000,  1.0000]),\n",
       " 'measured_state': tensor([[ 2.5060e+01,  2.0000e+01,  2.5000e-01,  5.0000e-01,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "         [ 2.5060e+01,  2.0000e+01,  2.5000e-01,  4.8200e-01,  7.0000e-03,\n",
       "           9.9300e-01, -1.6000e-02,  1.0230e+00,  9.9300e-01,  9.8300e-01],\n",
       "         [ 2.5060e+01,  2.0000e+01,  2.5000e-01,  4.5400e-01,  3.5000e-02,\n",
       "           8.1100e-01,  1.7300e-01,  1.0230e+00,  9.9300e-01,  9.8300e-01],\n",
       "         [ 2.5060e+01,  2.0000e+01,  2.5000e-01,  4.0400e-01,  1.0300e-01,\n",
       "           5.9200e-01,  4.1200e-01,  9.8300e-01,  1.0030e+00,  1.0130e+00],\n",
       "         [ 2.5060e+01,  2.0000e+01,  2.5000e-01,  2.9800e-01,  2.1400e-01,\n",
       "           3.7600e-01,  6.2500e-01,  9.4300e-01,  1.0240e+00,  1.0340e+00],\n",
       "         [ 2.5060e+01,  2.0000e+01,  2.5000e-01,  1.1200e-01,  4.0800e-01,\n",
       "           2.2100e-01,  7.1600e-01,  9.0700e-01,  9.7700e-01,  1.1160e+00]]),\n",
       " 'times': tensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000, 5.1000])}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dataset\n",
    "dataset = BMEDDataset(dict_spline)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 1/10000 - 훈련 손실: 0.687660, 검증 손실: 0.437830, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.437830\n",
      "에포크 2/10000 - 훈련 손실: 0.657840, 검증 손실: 0.416659, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.416659\n",
      "에포크 3/10000 - 훈련 손실: 0.610344, 검증 손실: 0.396633, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.396633\n",
      "에포크 4/10000 - 훈련 손실: 0.611424, 검증 손실: 0.379385, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.379385\n",
      "에포크 5/10000 - 훈련 손실: 0.572835, 검증 손실: 0.361971, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.361971\n",
      "에포크 6/10000 - 훈련 손실: 0.542085, 검증 손실: 0.346648, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.346648\n",
      "에포크 7/10000 - 훈련 손실: 0.514762, 검증 손실: 0.332971, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.332971\n",
      "에포크 8/10000 - 훈련 손실: 0.505074, 검증 손실: 0.320418, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.320418\n",
      "에포크 9/10000 - 훈련 손실: 0.471808, 검증 손실: 0.309108, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.309108\n",
      "에포크 10/10000 - 훈련 손실: 0.475755, 검증 손실: 0.298069, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.298069\n",
      "에포크 11/10000 - 훈련 손실: 0.467931, 검증 손실: 0.289221, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.289221\n",
      "에포크 12/10000 - 훈련 손실: 0.424588, 검증 손실: 0.279157, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.279157\n",
      "에포크 13/10000 - 훈련 손실: 0.409655, 검증 손실: 0.270854, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.270854\n",
      "에포크 14/10000 - 훈련 손실: 0.400368, 검증 손실: 0.262962, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.262962\n",
      "에포크 15/10000 - 훈련 손실: 0.397512, 검증 손실: 0.255915, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.255915\n",
      "에포크 16/10000 - 훈련 손실: 0.371951, 검증 손실: 0.249429, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.249429\n",
      "에포크 17/10000 - 훈련 손실: 0.362374, 검증 손실: 0.242310, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.242310\n",
      "에포크 18/10000 - 훈련 손실: 0.360274, 검증 손실: 0.236715, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.236715\n",
      "에포크 19/10000 - 훈련 손실: 0.343059, 검증 손실: 0.230955, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.230955\n",
      "에포크 20/10000 - 훈련 손실: 0.335126, 검증 손실: 0.225974, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.225974\n",
      "에포크 21/10000 - 훈련 손실: 0.329347, 검증 손실: 0.220762, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.220762\n",
      "에포크 22/10000 - 훈련 손실: 0.330569, 검증 손실: 0.216382, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.216382\n",
      "에포크 23/10000 - 훈련 손실: 0.307492, 검증 손실: 0.212569, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.212569\n",
      "에포크 24/10000 - 훈련 손실: 0.299723, 검증 손실: 0.208732, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.208732\n",
      "에포크 25/10000 - 훈련 손실: 0.291498, 검증 손실: 0.205183, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.205183\n",
      "에포크 26/10000 - 훈련 손실: 0.279266, 검증 손실: 0.202276, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.202276\n",
      "에포크 27/10000 - 훈련 손실: 0.274609, 검증 손실: 0.198888, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.198888\n",
      "에포크 28/10000 - 훈련 손실: 0.265188, 검증 손실: 0.196456, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.196456\n",
      "에포크 29/10000 - 훈련 손실: 0.266356, 검증 손실: 0.193703, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.193703\n",
      "에포크 30/10000 - 훈련 손실: 0.254859, 검증 손실: 0.191193, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.191193\n",
      "에포크 31/10000 - 훈련 손실: 0.247249, 검증 손실: 0.188962, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.188962\n",
      "에포크 32/10000 - 훈련 손실: 0.246169, 검증 손실: 0.186677, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.186677\n",
      "에포크 33/10000 - 훈련 손실: 0.239934, 검증 손실: 0.185570, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.185570\n",
      "에포크 34/10000 - 훈련 손실: 0.238738, 검증 손실: 0.183764, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.183764\n",
      "에포크 35/10000 - 훈련 손실: 0.226099, 검증 손실: 0.181948, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.181948\n",
      "에포크 36/10000 - 훈련 손실: 0.238176, 검증 손실: 0.180976, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.180976\n",
      "에포크 37/10000 - 훈련 손실: 0.218221, 검증 손실: 0.179340, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.179340\n",
      "에포크 38/10000 - 훈련 손실: 0.219204, 검증 손실: 0.177562, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.177562\n",
      "에포크 39/10000 - 훈련 손실: 0.207814, 검증 손실: 0.176739, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.176739\n",
      "에포크 40/10000 - 훈련 손실: 0.208862, 검증 손실: 0.175958, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.175958\n",
      "에포크 41/10000 - 훈련 손실: 0.208335, 검증 손실: 0.174954, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.174954\n",
      "에포크 42/10000 - 훈련 손실: 0.209217, 검증 손실: 0.173650, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.173650\n",
      "에포크 43/10000 - 훈련 손실: 0.196955, 검증 손실: 0.173368, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.173368\n",
      "에포크 44/10000 - 훈련 손실: 0.201365, 검증 손실: 0.173104, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.173104\n",
      "에포크 45/10000 - 훈련 손실: 0.200619, 검증 손실: 0.171736, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.171736\n",
      "에포크 46/10000 - 훈련 손실: 0.191673, 검증 손실: 0.171710, 학습률: 0.00000230, 인내심 카운터: 1\n",
      "에포크 47/10000 - 훈련 손실: 0.182852, 검증 손실: 0.170123, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.170123\n",
      "에포크 48/10000 - 훈련 손실: 0.194677, 검증 손실: 0.169059, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.169059\n",
      "에포크 49/10000 - 훈련 손실: 0.186581, 검증 손실: 0.168276, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.168276\n",
      "에포크 50/10000 - 훈련 손실: 0.180748, 검증 손실: 0.167446, 학습률: 0.00000230, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.167446\n",
      "에포크 51/10000 - 훈련 손실: 0.187227, 검증 손실: 0.168191, 학습률: 0.00000230, 인내심 카운터: 1\n",
      "에포크 52/10000 - 훈련 손실: 0.181510, 검증 손실: 0.168246, 학습률: 0.00000230, 인내심 카운터: 2\n",
      "에포크 53/10000 - 훈련 손실: 0.177860, 검증 손실: 0.167346, 학습률: 0.00000230, 인내심 카운터: 3\n",
      "에포크 54/10000 - 훈련 손실: 0.180174, 검증 손실: 0.168027, 학습률: 0.00000230, 인내심 카운터: 4\n",
      "에포크 55/10000 - 훈련 손실: 0.173835, 검증 손실: 0.167979, 학습률: 0.00000230, 인내심 카운터: 5\n",
      "에포크 56/10000 - 훈련 손실: 0.174078, 검증 손실: 0.168174, 학습률: 0.00000230, 인내심 카운터: 6\n",
      "에포크 57/10000 - 훈련 손실: 0.163787, 검증 손실: 0.168143, 학습률: 0.00000230, 인내심 카운터: 7\n",
      "에포크 58/10000 - 훈련 손실: 0.164711, 검증 손실: 0.167615, 학습률: 0.00000230, 인내심 카운터: 8\n",
      "에포크 59/10000 - 훈련 손실: 0.165084, 검증 손실: 0.167921, 학습률: 0.00000115, 인내심 카운터: 9\n",
      "에포크 60/10000 - 훈련 손실: 0.161958, 검증 손실: 0.167669, 학습률: 0.00000115, 인내심 카운터: 10\n",
      "에포크 61/10000 - 훈련 손실: 0.164965, 검증 손실: 0.167372, 학습률: 0.00000115, 인내심 카운터: 11\n",
      "에포크 62/10000 - 훈련 손실: 0.167837, 검증 손실: 0.167537, 학습률: 0.00000115, 인내심 카운터: 12\n",
      "에포크 63/10000 - 훈련 손실: 0.157508, 검증 손실: 0.167619, 학습률: 0.00000115, 인내심 카운터: 13\n",
      "에포크 64/10000 - 훈련 손실: 0.157779, 검증 손실: 0.167401, 학습률: 0.00000115, 인내심 카운터: 14\n",
      "에포크 65/10000 - 훈련 손실: 0.154900, 검증 손실: 0.167002, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.167002\n",
      "에포크 66/10000 - 훈련 손실: 0.164253, 검증 손실: 0.166480, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.166480\n",
      "에포크 67/10000 - 훈련 손실: 0.157501, 검증 손실: 0.166447, 학습률: 0.00000115, 인내심 카운터: 1\n",
      "에포크 68/10000 - 훈련 손실: 0.160519, 검증 손실: 0.166954, 학습률: 0.00000115, 인내심 카운터: 2\n",
      "에포크 69/10000 - 훈련 손실: 0.153748, 검증 손실: 0.166670, 학습률: 0.00000115, 인내심 카운터: 3\n",
      "에포크 70/10000 - 훈련 손실: 0.152775, 검증 손실: 0.166325, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.166325\n",
      "에포크 71/10000 - 훈련 손실: 0.158733, 검증 손실: 0.166168, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.166168\n",
      "에포크 72/10000 - 훈련 손실: 0.153952, 검증 손실: 0.165742, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.165742\n",
      "에포크 73/10000 - 훈련 손실: 0.153434, 검증 손실: 0.165601, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.165601\n",
      "에포크 74/10000 - 훈련 손실: 0.150968, 검증 손실: 0.165273, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.165273\n",
      "에포크 75/10000 - 훈련 손실: 0.150766, 검증 손실: 0.165377, 학습률: 0.00000115, 인내심 카운터: 1\n",
      "에포크 76/10000 - 훈련 손실: 0.152916, 검증 손실: 0.164927, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.164927\n",
      "에포크 77/10000 - 훈련 손실: 0.153599, 검증 손실: 0.164564, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.164564\n",
      "에포크 78/10000 - 훈련 손실: 0.146797, 검증 손실: 0.164797, 학습률: 0.00000115, 인내심 카운터: 1\n",
      "에포크 79/10000 - 훈련 손실: 0.150185, 검증 손실: 0.165252, 학습률: 0.00000115, 인내심 카운터: 2\n",
      "에포크 80/10000 - 훈련 손실: 0.147008, 검증 손실: 0.165321, 학습률: 0.00000115, 인내심 카운터: 3\n",
      "에포크 81/10000 - 훈련 손실: 0.150267, 검증 손실: 0.165045, 학습률: 0.00000115, 인내심 카운터: 4\n",
      "에포크 82/10000 - 훈련 손실: 0.150447, 검증 손실: 0.164631, 학습률: 0.00000115, 인내심 카운터: 5\n",
      "에포크 83/10000 - 훈련 손실: 0.142452, 검증 손실: 0.164493, 학습률: 0.00000115, 인내심 카운터: 6\n",
      "에포크 84/10000 - 훈련 손실: 0.140354, 검증 손실: 0.164926, 학습률: 0.00000115, 인내심 카운터: 7\n",
      "에포크 85/10000 - 훈련 손실: 0.149039, 검증 손실: 0.164812, 학습률: 0.00000115, 인내심 카운터: 8\n",
      "에포크 86/10000 - 훈련 손실: 0.145352, 검증 손실: 0.164448, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.164448\n",
      "에포크 87/10000 - 훈련 손실: 0.143632, 검증 손실: 0.164357, 학습률: 0.00000115, 인내심 카운터: 1\n",
      "에포크 88/10000 - 훈련 손실: 0.147859, 검증 손실: 0.164504, 학습률: 0.00000115, 인내심 카운터: 2\n",
      "에포크 89/10000 - 훈련 손실: 0.138591, 검증 손실: 0.164868, 학습률: 0.00000115, 인내심 카운터: 3\n",
      "에포크 90/10000 - 훈련 손실: 0.144953, 검증 손실: 0.164856, 학습률: 0.00000115, 인내심 카운터: 4\n",
      "에포크 91/10000 - 훈련 손실: 0.141520, 검증 손실: 0.164368, 학습률: 0.00000115, 인내심 카운터: 5\n",
      "에포크 92/10000 - 훈련 손실: 0.135848, 검증 손실: 0.164363, 학습률: 0.00000115, 인내심 카운터: 6\n",
      "에포크 93/10000 - 훈련 손실: 0.140560, 검증 손실: 0.164197, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.164197\n",
      "에포크 94/10000 - 훈련 손실: 0.139687, 검증 손실: 0.164115, 학습률: 0.00000115, 인내심 카운터: 1\n",
      "에포크 95/10000 - 훈련 손실: 0.137744, 검증 손실: 0.164067, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.164067\n",
      "에포크 96/10000 - 훈련 손실: 0.134203, 검증 손실: 0.163983, 학습률: 0.00000115, 인내심 카운터: 1\n",
      "에포크 97/10000 - 훈련 손실: 0.136971, 검증 손실: 0.164012, 학습률: 0.00000115, 인내심 카운터: 2\n",
      "에포크 98/10000 - 훈련 손실: 0.138793, 검증 손실: 0.163845, 학습률: 0.00000115, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.163845\n",
      "에포크 99/10000 - 훈련 손실: 0.141008, 검증 손실: 0.164396, 학습률: 0.00000115, 인내심 카운터: 1\n",
      "에포크 100/10000 - 훈련 손실: 0.137121, 검증 손실: 0.164394, 학습률: 0.00000115, 인내심 카운터: 2\n",
      "에포크 101/10000 - 훈련 손실: 0.140339, 검증 손실: 0.164154, 학습률: 0.00000115, 인내심 카운터: 3\n",
      "에포크 102/10000 - 훈련 손실: 0.134085, 검증 손실: 0.164215, 학습률: 0.00000115, 인내심 카운터: 4\n",
      "에포크 103/10000 - 훈련 손실: 0.138980, 검증 손실: 0.164058, 학습률: 0.00000115, 인내심 카운터: 5\n",
      "에포크 104/10000 - 훈련 손실: 0.134985, 검증 손실: 0.163966, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 105/10000 - 훈련 손실: 0.134168, 검증 손실: 0.163790, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 106/10000 - 훈련 손실: 0.136535, 검증 손실: 0.163457, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.163457\n",
      "에포크 107/10000 - 훈련 손실: 0.138320, 검증 손실: 0.163293, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.163293\n",
      "에포크 108/10000 - 훈련 손실: 0.130256, 검증 손실: 0.162976, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.162976\n",
      "에포크 109/10000 - 훈련 손실: 0.129513, 검증 손실: 0.162350, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.162350\n",
      "에포크 110/10000 - 훈련 손실: 0.133866, 검증 손실: 0.162159, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.162159\n",
      "에포크 111/10000 - 훈련 손실: 0.130831, 검증 손실: 0.162303, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 112/10000 - 훈련 손실: 0.126897, 검증 손실: 0.162669, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 113/10000 - 훈련 손실: 0.130854, 검증 손실: 0.162645, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 114/10000 - 훈련 손실: 0.128698, 검증 손실: 0.161911, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.161911\n",
      "에포크 115/10000 - 훈련 손실: 0.129879, 검증 손실: 0.161309, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.161309\n",
      "에포크 116/10000 - 훈련 손실: 0.128490, 검증 손실: 0.161552, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 117/10000 - 훈련 손실: 0.130147, 검증 손실: 0.161151, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.161151\n",
      "에포크 118/10000 - 훈련 손실: 0.127835, 검증 손실: 0.161420, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 119/10000 - 훈련 손실: 0.125058, 검증 손실: 0.161234, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 120/10000 - 훈련 손실: 0.127759, 검증 손실: 0.160893, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.160893\n",
      "에포크 121/10000 - 훈련 손실: 0.127660, 검증 손실: 0.161520, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 122/10000 - 훈련 손실: 0.123351, 검증 손실: 0.161907, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 123/10000 - 훈련 손실: 0.127626, 검증 손실: 0.162029, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 124/10000 - 훈련 손실: 0.128657, 검증 손실: 0.162114, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 125/10000 - 훈련 손실: 0.126658, 검증 손실: 0.161624, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 126/10000 - 훈련 손실: 0.125081, 검증 손실: 0.160978, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 127/10000 - 훈련 손실: 0.125718, 검증 손실: 0.161028, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 128/10000 - 훈련 손실: 0.129317, 검증 손실: 0.160688, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.160688\n",
      "에포크 129/10000 - 훈련 손실: 0.123866, 검증 손실: 0.160750, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 130/10000 - 훈련 손실: 0.119951, 검증 손실: 0.160383, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.160383\n",
      "에포크 131/10000 - 훈련 손실: 0.124576, 검증 손실: 0.160376, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 132/10000 - 훈련 손실: 0.125724, 검증 손실: 0.160222, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.160222\n",
      "에포크 133/10000 - 훈련 손실: 0.125227, 검증 손실: 0.159972, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.159972\n",
      "에포크 134/10000 - 훈련 손실: 0.125010, 검증 손실: 0.160408, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 135/10000 - 훈련 손실: 0.122233, 검증 손실: 0.160135, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 136/10000 - 훈련 손실: 0.124680, 검증 손실: 0.160128, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 137/10000 - 훈련 손실: 0.120719, 검증 손실: 0.159927, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 138/10000 - 훈련 손실: 0.124732, 검증 손실: 0.159840, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.159840\n",
      "에포크 139/10000 - 훈련 손실: 0.117128, 검증 손실: 0.159201, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.159201\n",
      "에포크 140/10000 - 훈련 손실: 0.119402, 검증 손실: 0.158995, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.158995\n",
      "에포크 141/10000 - 훈련 손실: 0.120665, 검증 손실: 0.159303, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 142/10000 - 훈련 손실: 0.118696, 검증 손실: 0.159346, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 143/10000 - 훈련 손실: 0.121751, 검증 손실: 0.158832, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.158832\n",
      "에포크 144/10000 - 훈련 손실: 0.120018, 검증 손실: 0.158763, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 145/10000 - 훈련 손실: 0.120135, 검증 손실: 0.159204, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 146/10000 - 훈련 손실: 0.116648, 검증 손실: 0.158865, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 147/10000 - 훈련 손실: 0.115777, 검증 손실: 0.158443, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.158443\n",
      "에포크 148/10000 - 훈련 손실: 0.119648, 검증 손실: 0.158313, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.158313\n",
      "에포크 149/10000 - 훈련 손실: 0.120093, 검증 손실: 0.157945, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.157945\n",
      "에포크 150/10000 - 훈련 손실: 0.120895, 검증 손실: 0.157814, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.157814\n",
      "에포크 151/10000 - 훈련 손실: 0.115644, 검증 손실: 0.157097, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.157097\n",
      "에포크 152/10000 - 훈련 손실: 0.119634, 검증 손실: 0.157043, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 153/10000 - 훈련 손실: 0.115862, 검증 손실: 0.156732, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.156732\n",
      "에포크 154/10000 - 훈련 손실: 0.118457, 검증 손실: 0.156658, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 155/10000 - 훈련 손실: 0.117918, 검증 손실: 0.156630, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.156630\n",
      "에포크 156/10000 - 훈련 손실: 0.115460, 검증 손실: 0.156506, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.156506\n",
      "에포크 157/10000 - 훈련 손실: 0.119618, 검증 손실: 0.156005, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.156005\n",
      "에포크 158/10000 - 훈련 손실: 0.117403, 검증 손실: 0.155834, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.155834\n",
      "에포크 159/10000 - 훈련 손실: 0.116518, 검증 손실: 0.155274, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.155274\n",
      "에포크 160/10000 - 훈련 손실: 0.114442, 검증 손실: 0.155446, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 161/10000 - 훈련 손실: 0.114181, 검증 손실: 0.155332, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 162/10000 - 훈련 손실: 0.117641, 검증 손실: 0.155067, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.155067\n",
      "에포크 163/10000 - 훈련 손실: 0.111439, 검증 손실: 0.154450, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.154450\n",
      "에포크 164/10000 - 훈련 손실: 0.111669, 검증 손실: 0.154161, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.154161\n",
      "에포크 165/10000 - 훈련 손실: 0.114456, 검증 손실: 0.153912, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.153912\n",
      "에포크 166/10000 - 훈련 손실: 0.114668, 검증 손실: 0.153474, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.153474\n",
      "에포크 167/10000 - 훈련 손실: 0.112752, 검증 손실: 0.152569, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.152569\n",
      "에포크 168/10000 - 훈련 손실: 0.112325, 검증 손실: 0.152491, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 169/10000 - 훈련 손실: 0.112543, 검증 손실: 0.152538, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 170/10000 - 훈련 손실: 0.112296, 검증 손실: 0.152401, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.152401\n",
      "에포크 171/10000 - 훈련 손실: 0.115023, 검증 손실: 0.153105, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 172/10000 - 훈련 손실: 0.112696, 검증 손실: 0.152730, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 173/10000 - 훈련 손실: 0.112540, 검증 손실: 0.152247, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.152247\n",
      "에포크 174/10000 - 훈련 손실: 0.115108, 검증 손실: 0.151768, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.151768\n",
      "에포크 175/10000 - 훈련 손실: 0.110016, 검증 손실: 0.151241, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.151241\n",
      "에포크 176/10000 - 훈련 손실: 0.109202, 검증 손실: 0.151124, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.151124\n",
      "에포크 177/10000 - 훈련 손실: 0.108604, 검증 손실: 0.150865, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.150865\n",
      "에포크 178/10000 - 훈련 손실: 0.110130, 검증 손실: 0.150926, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 179/10000 - 훈련 손실: 0.107994, 검증 손실: 0.150975, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 180/10000 - 훈련 손실: 0.107871, 검증 손실: 0.150661, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.150661\n",
      "에포크 181/10000 - 훈련 손실: 0.107562, 검증 손실: 0.149638, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.149638\n",
      "에포크 182/10000 - 훈련 손실: 0.108792, 검증 손실: 0.149141, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.149141\n",
      "에포크 183/10000 - 훈련 손실: 0.107951, 검증 손실: 0.148569, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.148569\n",
      "에포크 184/10000 - 훈련 손실: 0.109033, 검증 손실: 0.148295, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.148295\n",
      "에포크 185/10000 - 훈련 손실: 0.110160, 검증 손실: 0.147937, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.147937\n",
      "에포크 186/10000 - 훈련 손실: 0.109251, 검증 손실: 0.147559, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.147559\n",
      "에포크 187/10000 - 훈련 손실: 0.105692, 검증 손실: 0.146950, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.146950\n",
      "에포크 188/10000 - 훈련 손실: 0.104555, 검증 손실: 0.146800, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.146800\n",
      "에포크 189/10000 - 훈련 손실: 0.107139, 검증 손실: 0.146688, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.146688\n",
      "에포크 190/10000 - 훈련 손실: 0.106514, 검증 손실: 0.146616, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 191/10000 - 훈련 손실: 0.104429, 검증 손실: 0.146744, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 192/10000 - 훈련 손실: 0.105906, 검증 손실: 0.146524, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.146524\n",
      "에포크 193/10000 - 훈련 손실: 0.104005, 검증 손실: 0.146386, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.146386\n",
      "에포크 194/10000 - 훈련 손실: 0.103846, 검증 손실: 0.146497, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 195/10000 - 훈련 손실: 0.102350, 검증 손실: 0.146263, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.146263\n",
      "에포크 196/10000 - 훈련 손실: 0.105462, 검증 손실: 0.146048, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.146048\n",
      "에포크 197/10000 - 훈련 손실: 0.110250, 검증 손실: 0.145557, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.145557\n",
      "에포크 198/10000 - 훈련 손실: 0.109380, 검증 손실: 0.145749, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 199/10000 - 훈련 손실: 0.104247, 검증 손실: 0.145070, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.145070\n",
      "에포크 200/10000 - 훈련 손실: 0.105722, 검증 손실: 0.144779, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.144779\n",
      "에포크 201/10000 - 훈련 손실: 0.101107, 검증 손실: 0.144631, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.144631\n",
      "에포크 202/10000 - 훈련 손실: 0.100620, 검증 손실: 0.144859, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 203/10000 - 훈련 손실: 0.101320, 검증 손실: 0.144652, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 204/10000 - 훈련 손실: 0.106930, 검증 손실: 0.144076, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.144076\n",
      "에포크 205/10000 - 훈련 손실: 0.105509, 검증 손실: 0.143890, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.143890\n",
      "에포크 206/10000 - 훈련 손실: 0.099655, 검증 손실: 0.143468, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.143468\n",
      "에포크 207/10000 - 훈련 손실: 0.103548, 검증 손실: 0.143219, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.143219\n",
      "에포크 208/10000 - 훈련 손실: 0.103593, 검증 손실: 0.142580, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.142580\n",
      "에포크 209/10000 - 훈련 손실: 0.101349, 검증 손실: 0.141865, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.141865\n",
      "에포크 210/10000 - 훈련 손실: 0.100372, 검증 손실: 0.141580, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.141580\n",
      "에포크 211/10000 - 훈련 손실: 0.102070, 검증 손실: 0.141664, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 212/10000 - 훈련 손실: 0.100200, 검증 손실: 0.141811, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 213/10000 - 훈련 손실: 0.102407, 검증 손실: 0.141800, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 214/10000 - 훈련 손실: 0.103440, 검증 손실: 0.141683, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 215/10000 - 훈련 손실: 0.101567, 검증 손실: 0.141628, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 216/10000 - 훈련 손실: 0.099909, 검증 손실: 0.140973, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.140973\n",
      "에포크 217/10000 - 훈련 손실: 0.098010, 검증 손실: 0.140842, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.140842\n",
      "에포크 218/10000 - 훈련 손실: 0.099873, 검증 손실: 0.140375, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.140375\n",
      "에포크 219/10000 - 훈련 손실: 0.101070, 검증 손실: 0.139989, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.139989\n",
      "에포크 220/10000 - 훈련 손실: 0.098340, 검증 손실: 0.139945, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 221/10000 - 훈련 손실: 0.097664, 검증 손실: 0.139744, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.139744\n",
      "에포크 222/10000 - 훈련 손실: 0.099811, 검증 손실: 0.140044, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 223/10000 - 훈련 손실: 0.096584, 검증 손실: 0.139635, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.139635\n",
      "에포크 224/10000 - 훈련 손실: 0.095755, 검증 손실: 0.139877, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 225/10000 - 훈련 손실: 0.098994, 검증 손실: 0.139242, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.139242\n",
      "에포크 226/10000 - 훈련 손실: 0.095911, 검증 손실: 0.138590, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.138590\n",
      "에포크 227/10000 - 훈련 손실: 0.100800, 검증 손실: 0.138415, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.138415\n",
      "에포크 228/10000 - 훈련 손실: 0.099788, 검증 손실: 0.138253, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.138253\n",
      "에포크 229/10000 - 훈련 손실: 0.097785, 검증 손실: 0.137962, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.137962\n",
      "에포크 230/10000 - 훈련 손실: 0.098363, 검증 손실: 0.138011, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 231/10000 - 훈련 손실: 0.097361, 검증 손실: 0.137582, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.137582\n",
      "에포크 232/10000 - 훈련 손실: 0.095693, 검증 손실: 0.136514, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.136514\n",
      "에포크 233/10000 - 훈련 손실: 0.100256, 검증 손실: 0.135634, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.135634\n",
      "에포크 234/10000 - 훈련 손실: 0.094128, 검증 손실: 0.135004, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.135004\n",
      "에포크 235/10000 - 훈련 손실: 0.098080, 검증 손실: 0.134676, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.134676\n",
      "에포크 236/10000 - 훈련 손실: 0.096679, 검증 손실: 0.135046, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 237/10000 - 훈련 손실: 0.098430, 검증 손실: 0.134581, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 238/10000 - 훈련 손실: 0.094066, 검증 손실: 0.134570, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.134570\n",
      "에포크 239/10000 - 훈련 손실: 0.095120, 검증 손실: 0.134363, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.134363\n",
      "에포크 240/10000 - 훈련 손실: 0.095283, 검증 손실: 0.133965, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.133965\n",
      "에포크 241/10000 - 훈련 손실: 0.096334, 검증 손실: 0.133949, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 242/10000 - 훈련 손실: 0.095427, 검증 손실: 0.133414, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.133414\n",
      "에포크 243/10000 - 훈련 손실: 0.095898, 검증 손실: 0.133404, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 244/10000 - 훈련 손실: 0.095376, 검증 손실: 0.133472, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 245/10000 - 훈련 손실: 0.093844, 검증 손실: 0.133420, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 246/10000 - 훈련 손실: 0.095477, 검증 손실: 0.133135, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.133135\n",
      "에포크 247/10000 - 훈련 손실: 0.094145, 검증 손실: 0.132818, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.132818\n",
      "에포크 248/10000 - 훈련 손실: 0.095592, 검증 손실: 0.132987, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 249/10000 - 훈련 손실: 0.094997, 검증 손실: 0.132425, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.132425\n",
      "에포크 250/10000 - 훈련 손실: 0.091310, 검증 손실: 0.132127, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.132127\n",
      "에포크 251/10000 - 훈련 손실: 0.091269, 검증 손실: 0.131696, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.131696\n",
      "에포크 252/10000 - 훈련 손실: 0.095519, 검증 손실: 0.131617, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 253/10000 - 훈련 손실: 0.092998, 검증 손실: 0.131202, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.131202\n",
      "에포크 254/10000 - 훈련 손실: 0.091751, 검증 손실: 0.130636, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.130636\n",
      "에포크 255/10000 - 훈련 손실: 0.092659, 검증 손실: 0.130106, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.130106\n",
      "에포크 256/10000 - 훈련 손실: 0.093602, 검증 손실: 0.130181, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 257/10000 - 훈련 손실: 0.092363, 검증 손실: 0.130077, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 258/10000 - 훈련 손실: 0.088447, 검증 손실: 0.129253, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.129253\n",
      "에포크 259/10000 - 훈련 손실: 0.090823, 검증 손실: 0.128885, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.128885\n",
      "에포크 260/10000 - 훈련 손실: 0.090481, 검증 손실: 0.128951, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 261/10000 - 훈련 손실: 0.091525, 검증 손실: 0.129113, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 262/10000 - 훈련 손실: 0.089316, 검증 손실: 0.128702, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.128702\n",
      "에포크 263/10000 - 훈련 손실: 0.090716, 검증 손실: 0.128585, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.128585\n",
      "에포크 264/10000 - 훈련 손실: 0.091134, 검증 손실: 0.128736, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 265/10000 - 훈련 손실: 0.089879, 검증 손실: 0.129104, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 266/10000 - 훈련 손실: 0.090123, 검증 손실: 0.128477, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.128477\n",
      "에포크 267/10000 - 훈련 손실: 0.090404, 검증 손실: 0.128300, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.128300\n",
      "에포크 268/10000 - 훈련 손실: 0.089167, 검증 손실: 0.127538, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.127538\n",
      "에포크 269/10000 - 훈련 손실: 0.088786, 검증 손실: 0.127338, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.127338\n",
      "에포크 270/10000 - 훈련 손실: 0.091639, 검증 손실: 0.127484, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 271/10000 - 훈련 손실: 0.090147, 검증 손실: 0.127506, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 272/10000 - 훈련 손실: 0.087961, 검증 손실: 0.127081, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.127081\n",
      "에포크 273/10000 - 훈련 손실: 0.088305, 검증 손실: 0.127070, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 274/10000 - 훈련 손실: 0.088390, 검증 손실: 0.126888, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.126888\n",
      "에포크 275/10000 - 훈련 손실: 0.084679, 검증 손실: 0.127018, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 276/10000 - 훈련 손실: 0.090225, 검증 손실: 0.126811, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 277/10000 - 훈련 손실: 0.088995, 검증 손실: 0.126370, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.126370\n",
      "에포크 278/10000 - 훈련 손실: 0.087115, 검증 손실: 0.126597, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 279/10000 - 훈련 손실: 0.087971, 검증 손실: 0.125690, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.125690\n",
      "에포크 280/10000 - 훈련 손실: 0.083070, 검증 손실: 0.125029, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.125029\n",
      "에포크 281/10000 - 훈련 손실: 0.086164, 검증 손실: 0.125104, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 282/10000 - 훈련 손실: 0.086944, 검증 손실: 0.125060, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 283/10000 - 훈련 손실: 0.087862, 검증 손실: 0.125389, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 284/10000 - 훈련 손실: 0.086363, 검증 손실: 0.125339, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 285/10000 - 훈련 손실: 0.087450, 검증 손실: 0.125059, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 286/10000 - 훈련 손실: 0.085782, 검증 손실: 0.124554, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.124554\n",
      "에포크 287/10000 - 훈련 손실: 0.086308, 검증 손실: 0.123937, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.123937\n",
      "에포크 288/10000 - 훈련 손실: 0.088734, 검증 손실: 0.123480, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.123480\n",
      "에포크 289/10000 - 훈련 손실: 0.086526, 검증 손실: 0.123352, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.123352\n",
      "에포크 290/10000 - 훈련 손실: 0.089428, 검증 손실: 0.123406, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 291/10000 - 훈련 손실: 0.086635, 검증 손실: 0.122814, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.122814\n",
      "에포크 292/10000 - 훈련 손실: 0.085437, 검증 손실: 0.122810, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 293/10000 - 훈련 손실: 0.085045, 검증 손실: 0.122538, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.122538\n",
      "에포크 294/10000 - 훈련 손실: 0.086269, 검증 손실: 0.122558, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 295/10000 - 훈련 손실: 0.081821, 검증 손실: 0.122729, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 296/10000 - 훈련 손실: 0.085516, 검증 손실: 0.122155, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.122155\n",
      "에포크 297/10000 - 훈련 손실: 0.084493, 검증 손실: 0.121967, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.121967\n",
      "에포크 298/10000 - 훈련 손실: 0.084948, 검증 손실: 0.121648, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.121648\n",
      "에포크 299/10000 - 훈련 손실: 0.085655, 검증 손실: 0.121849, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 300/10000 - 훈련 손실: 0.085109, 검증 손실: 0.121565, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 301/10000 - 훈련 손실: 0.087577, 검증 손실: 0.121880, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 302/10000 - 훈련 손실: 0.082774, 검증 손실: 0.122514, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 303/10000 - 훈련 손실: 0.083294, 검증 손실: 0.122127, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 304/10000 - 훈련 손실: 0.085157, 검증 손실: 0.121845, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 305/10000 - 훈련 손실: 0.080603, 검증 손실: 0.121544, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.121544\n",
      "에포크 306/10000 - 훈련 손실: 0.084143, 검증 손실: 0.121202, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.121202\n",
      "에포크 307/10000 - 훈련 손실: 0.083457, 검증 손실: 0.121106, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 308/10000 - 훈련 손실: 0.079915, 검증 손실: 0.120289, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.120289\n",
      "에포크 309/10000 - 훈련 손실: 0.087439, 검증 손실: 0.119748, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.119748\n",
      "에포크 310/10000 - 훈련 손실: 0.085127, 검증 손실: 0.119160, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.119160\n",
      "에포크 311/10000 - 훈련 손실: 0.083425, 검증 손실: 0.118470, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.118470\n",
      "에포크 312/10000 - 훈련 손실: 0.079352, 검증 손실: 0.118246, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.118246\n",
      "에포크 313/10000 - 훈련 손실: 0.081300, 검증 손실: 0.118472, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 314/10000 - 훈련 손실: 0.080609, 검증 손실: 0.117948, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.117948\n",
      "에포크 315/10000 - 훈련 손실: 0.082527, 검증 손실: 0.116970, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.116970\n",
      "에포크 316/10000 - 훈련 손실: 0.080844, 검증 손실: 0.116638, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.116638\n",
      "에포크 317/10000 - 훈련 손실: 0.080639, 검증 손실: 0.116653, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 318/10000 - 훈련 손실: 0.083714, 검증 손실: 0.115895, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.115895\n",
      "에포크 319/10000 - 훈련 손실: 0.080600, 검증 손실: 0.115675, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.115675\n",
      "에포크 320/10000 - 훈련 손실: 0.081528, 검증 손실: 0.115483, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.115483\n",
      "에포크 321/10000 - 훈련 손실: 0.081143, 검증 손실: 0.115329, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.115329\n",
      "에포크 322/10000 - 훈련 손실: 0.080535, 검증 손실: 0.115860, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 323/10000 - 훈련 손실: 0.080422, 검증 손실: 0.115497, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 324/10000 - 훈련 손실: 0.079318, 검증 손실: 0.114808, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.114808\n",
      "에포크 325/10000 - 훈련 손실: 0.077581, 검증 손실: 0.114332, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.114332\n",
      "에포크 326/10000 - 훈련 손실: 0.082686, 검증 손실: 0.114237, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 327/10000 - 훈련 손실: 0.078244, 검증 손실: 0.113643, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.113643\n",
      "에포크 328/10000 - 훈련 손실: 0.079622, 검증 손실: 0.113428, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.113428\n",
      "에포크 329/10000 - 훈련 손실: 0.077295, 검증 손실: 0.113865, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 330/10000 - 훈련 손실: 0.074080, 검증 손실: 0.114060, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 331/10000 - 훈련 손실: 0.078591, 검증 손실: 0.113527, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 332/10000 - 훈련 손실: 0.078755, 검증 손실: 0.113376, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 333/10000 - 훈련 손실: 0.078397, 검증 손실: 0.112662, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.112662\n",
      "에포크 334/10000 - 훈련 손실: 0.078497, 검증 손실: 0.112251, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.112251\n",
      "에포크 335/10000 - 훈련 손실: 0.077330, 검증 손실: 0.112175, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 336/10000 - 훈련 손실: 0.081455, 검증 손실: 0.111833, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.111833\n",
      "에포크 337/10000 - 훈련 손실: 0.077824, 검증 손실: 0.111415, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.111415\n",
      "에포크 338/10000 - 훈련 손실: 0.081611, 검증 손실: 0.111209, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.111209\n",
      "에포크 339/10000 - 훈련 손실: 0.077633, 검증 손실: 0.111487, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 340/10000 - 훈련 손실: 0.074548, 검증 손실: 0.111583, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 341/10000 - 훈련 손실: 0.076414, 검증 손실: 0.111120, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 342/10000 - 훈련 손실: 0.078091, 검증 손실: 0.111248, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 343/10000 - 훈련 손실: 0.074871, 검증 손실: 0.111885, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 344/10000 - 훈련 손실: 0.077642, 검증 손실: 0.111576, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 345/10000 - 훈련 손실: 0.073948, 검증 손실: 0.111072, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.111072\n",
      "에포크 346/10000 - 훈련 손실: 0.074194, 검증 손실: 0.111243, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 347/10000 - 훈련 손실: 0.076159, 검증 손실: 0.110999, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 348/10000 - 훈련 손실: 0.075372, 검증 손실: 0.110799, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.110799\n",
      "에포크 349/10000 - 훈련 손실: 0.073561, 검증 손실: 0.110343, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.110343\n",
      "에포크 350/10000 - 훈련 손실: 0.073419, 검증 손실: 0.109688, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.109688\n",
      "에포크 351/10000 - 훈련 손실: 0.072856, 검증 손실: 0.109615, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 352/10000 - 훈련 손실: 0.077346, 검증 손실: 0.109779, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 353/10000 - 훈련 손실: 0.073651, 검증 손실: 0.109566, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.109566\n",
      "에포크 354/10000 - 훈련 손실: 0.075020, 검증 손실: 0.108834, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.108834\n",
      "에포크 355/10000 - 훈련 손실: 0.076468, 검증 손실: 0.108887, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 356/10000 - 훈련 손실: 0.074729, 검증 손실: 0.108386, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.108386\n",
      "에포크 357/10000 - 훈련 손실: 0.076538, 검증 손실: 0.108293, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 358/10000 - 훈련 손실: 0.077439, 검증 손실: 0.107998, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.107998\n",
      "에포크 359/10000 - 훈련 손실: 0.071651, 검증 손실: 0.107564, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.107564\n",
      "에포크 360/10000 - 훈련 손실: 0.073214, 검증 손실: 0.107285, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.107285\n",
      "에포크 361/10000 - 훈련 손실: 0.075374, 검증 손실: 0.106772, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.106772\n",
      "에포크 362/10000 - 훈련 손실: 0.073210, 검증 손실: 0.106744, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 363/10000 - 훈련 손실: 0.072665, 검증 손실: 0.106156, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.106156\n",
      "에포크 364/10000 - 훈련 손실: 0.071525, 검증 손실: 0.106067, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 365/10000 - 훈련 손실: 0.073187, 검증 손실: 0.106320, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 366/10000 - 훈련 손실: 0.075722, 검증 손실: 0.106296, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 367/10000 - 훈련 손실: 0.073512, 검증 손실: 0.106094, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 368/10000 - 훈련 손실: 0.076303, 검증 손실: 0.105203, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.105203\n",
      "에포크 369/10000 - 훈련 손실: 0.074362, 검증 손실: 0.104823, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.104823\n",
      "에포크 370/10000 - 훈련 손실: 0.071681, 검증 손실: 0.104252, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.104252\n",
      "에포크 371/10000 - 훈련 손실: 0.073988, 검증 손실: 0.104394, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 372/10000 - 훈련 손실: 0.070668, 검증 손실: 0.104694, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 373/10000 - 훈련 손실: 0.073460, 검증 손실: 0.104782, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 374/10000 - 훈련 손실: 0.071698, 검증 손실: 0.104854, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 375/10000 - 훈련 손실: 0.071658, 검증 손실: 0.104899, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 376/10000 - 훈련 손실: 0.069802, 검증 손실: 0.104548, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 377/10000 - 훈련 손실: 0.072894, 검증 손실: 0.104741, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 378/10000 - 훈련 손실: 0.071478, 검증 손실: 0.104679, 학습률: 0.00000100, 인내심 카운터: 8\n",
      "에포크 379/10000 - 훈련 손실: 0.070690, 검증 손실: 0.104015, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.104015\n",
      "에포크 380/10000 - 훈련 손실: 0.074449, 검증 손실: 0.103812, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.103812\n",
      "에포크 381/10000 - 훈련 손실: 0.068910, 검증 손실: 0.103441, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.103441\n",
      "에포크 382/10000 - 훈련 손실: 0.071077, 검증 손실: 0.102911, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.102911\n",
      "에포크 383/10000 - 훈련 손실: 0.070379, 검증 손실: 0.102487, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.102487\n",
      "에포크 384/10000 - 훈련 손실: 0.066710, 검증 손실: 0.102560, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 385/10000 - 훈련 손실: 0.070092, 검증 손실: 0.102446, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 386/10000 - 훈련 손실: 0.071956, 검증 손실: 0.102233, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.102233\n",
      "에포크 387/10000 - 훈련 손실: 0.071105, 검증 손실: 0.101698, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.101698\n",
      "에포크 388/10000 - 훈련 손실: 0.068544, 검증 손실: 0.101878, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 389/10000 - 훈련 손실: 0.068980, 검증 손실: 0.101782, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 390/10000 - 훈련 손실: 0.066813, 검증 손실: 0.101325, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.101325\n",
      "에포크 391/10000 - 훈련 손실: 0.069940, 검증 손실: 0.101295, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 392/10000 - 훈련 손실: 0.068037, 검증 손실: 0.101169, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.101169\n",
      "에포크 393/10000 - 훈련 손실: 0.067452, 검증 손실: 0.100817, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.100817\n",
      "에포크 394/10000 - 훈련 손실: 0.067592, 검증 손실: 0.100395, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.100395\n",
      "에포크 395/10000 - 훈련 손실: 0.068916, 검증 손실: 0.100091, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.100091\n",
      "에포크 396/10000 - 훈련 손실: 0.072859, 검증 손실: 0.099893, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.099893\n",
      "에포크 397/10000 - 훈련 손실: 0.065909, 검증 손실: 0.099506, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.099506\n",
      "에포크 398/10000 - 훈련 손실: 0.064081, 검증 손실: 0.099732, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 399/10000 - 훈련 손실: 0.067845, 검증 손실: 0.100169, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 400/10000 - 훈련 손실: 0.065745, 검증 손실: 0.100332, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 401/10000 - 훈련 손실: 0.065514, 검증 손실: 0.100258, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 402/10000 - 훈련 손실: 0.068301, 검증 손실: 0.099817, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 403/10000 - 훈련 손실: 0.068393, 검증 손실: 0.098942, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.098942\n",
      "에포크 404/10000 - 훈련 손실: 0.067195, 검증 손실: 0.098558, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.098558\n",
      "에포크 405/10000 - 훈련 손실: 0.069855, 검증 손실: 0.098754, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 406/10000 - 훈련 손실: 0.067826, 검증 손실: 0.098724, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 407/10000 - 훈련 손실: 0.064869, 검증 손실: 0.098959, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 408/10000 - 훈련 손실: 0.068276, 검증 손실: 0.098768, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 409/10000 - 훈련 손실: 0.065505, 검증 손실: 0.098435, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.098435\n",
      "에포크 410/10000 - 훈련 손실: 0.066218, 검증 손실: 0.098669, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 411/10000 - 훈련 손실: 0.065176, 검증 손실: 0.098479, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 412/10000 - 훈련 손실: 0.063717, 검증 손실: 0.097907, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.097907\n",
      "에포크 413/10000 - 훈련 손실: 0.063342, 검증 손실: 0.097392, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.097392\n",
      "에포크 414/10000 - 훈련 손실: 0.064625, 검증 손실: 0.097603, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 415/10000 - 훈련 손실: 0.066445, 검증 손실: 0.097970, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 416/10000 - 훈련 손실: 0.067072, 검증 손실: 0.097436, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 417/10000 - 훈련 손실: 0.065978, 검증 손실: 0.097383, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 418/10000 - 훈련 손실: 0.068415, 검증 손실: 0.097073, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.097073\n",
      "에포크 419/10000 - 훈련 손실: 0.066690, 검증 손실: 0.096599, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.096599\n",
      "에포크 420/10000 - 훈련 손실: 0.067099, 검증 손실: 0.096197, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.096197\n",
      "에포크 421/10000 - 훈련 손실: 0.065927, 검증 손실: 0.096042, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.096042\n",
      "에포크 422/10000 - 훈련 손실: 0.063325, 검증 손실: 0.095936, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.095936\n",
      "에포크 423/10000 - 훈련 손실: 0.064224, 검증 손실: 0.095489, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.095489\n",
      "에포크 424/10000 - 훈련 손실: 0.066353, 검증 손실: 0.095206, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.095206\n",
      "에포크 425/10000 - 훈련 손실: 0.065388, 검증 손실: 0.094749, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.094749\n",
      "에포크 426/10000 - 훈련 손실: 0.065730, 검증 손실: 0.094502, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.094502\n",
      "에포크 427/10000 - 훈련 손실: 0.062121, 검증 손실: 0.094475, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 428/10000 - 훈련 손실: 0.063551, 검증 손실: 0.094624, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 429/10000 - 훈련 손실: 0.062910, 검증 손실: 0.094323, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.094323\n",
      "에포크 430/10000 - 훈련 손실: 0.063435, 검증 손실: 0.094068, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.094068\n",
      "에포크 431/10000 - 훈련 손실: 0.063602, 검증 손실: 0.092737, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.092737\n",
      "에포크 432/10000 - 훈련 손실: 0.067141, 검증 손실: 0.092378, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.092378\n",
      "에포크 433/10000 - 훈련 손실: 0.066186, 검증 손실: 0.091940, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.091940\n",
      "에포크 434/10000 - 훈련 손실: 0.064161, 검증 손실: 0.091733, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.091733\n",
      "에포크 435/10000 - 훈련 손실: 0.064550, 검증 손실: 0.091930, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 436/10000 - 훈련 손실: 0.061756, 검증 손실: 0.092059, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 437/10000 - 훈련 손실: 0.062697, 검증 손실: 0.092064, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 438/10000 - 훈련 손실: 0.063155, 검증 손실: 0.091868, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 439/10000 - 훈련 손실: 0.063421, 검증 손실: 0.091196, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.091196\n",
      "에포크 440/10000 - 훈련 손실: 0.067893, 검증 손실: 0.091147, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 441/10000 - 훈련 손실: 0.060876, 검증 손실: 0.090787, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.090787\n",
      "에포크 442/10000 - 훈련 손실: 0.063564, 검증 손실: 0.090049, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.090049\n",
      "에포크 443/10000 - 훈련 손실: 0.062356, 검증 손실: 0.089768, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.089768\n",
      "에포크 444/10000 - 훈련 손실: 0.061569, 검증 손실: 0.090483, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 445/10000 - 훈련 손실: 0.062962, 검증 손실: 0.090808, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 446/10000 - 훈련 손실: 0.061810, 검증 손실: 0.090136, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 447/10000 - 훈련 손실: 0.062630, 검증 손실: 0.090534, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 448/10000 - 훈련 손실: 0.060531, 검증 손실: 0.090829, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 449/10000 - 훈련 손실: 0.065536, 검증 손실: 0.090667, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 450/10000 - 훈련 손실: 0.062708, 검증 손실: 0.090686, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 451/10000 - 훈련 손실: 0.061539, 검증 손실: 0.090273, 학습률: 0.00000100, 인내심 카운터: 8\n",
      "에포크 452/10000 - 훈련 손실: 0.057641, 검증 손실: 0.090300, 학습률: 0.00000100, 인내심 카운터: 9\n",
      "에포크 453/10000 - 훈련 손실: 0.064006, 검증 손실: 0.090212, 학습률: 0.00000100, 인내심 카운터: 10\n",
      "에포크 454/10000 - 훈련 손실: 0.059680, 검증 손실: 0.089816, 학습률: 0.00000100, 인내심 카운터: 11\n",
      "에포크 455/10000 - 훈련 손실: 0.063353, 검증 손실: 0.089637, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.089637\n",
      "에포크 456/10000 - 훈련 손실: 0.060212, 검증 손실: 0.089371, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.089371\n",
      "에포크 457/10000 - 훈련 손실: 0.062281, 검증 손실: 0.089517, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 458/10000 - 훈련 손실: 0.059086, 검증 손실: 0.089667, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 459/10000 - 훈련 손실: 0.062432, 검증 손실: 0.089723, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 460/10000 - 훈련 손실: 0.060740, 검증 손실: 0.089350, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 461/10000 - 훈련 손실: 0.060316, 검증 손실: 0.089134, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.089134\n",
      "에포크 462/10000 - 훈련 손실: 0.057942, 검증 손실: 0.089083, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 463/10000 - 훈련 손실: 0.059223, 검증 손실: 0.089032, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.089032\n",
      "에포크 464/10000 - 훈련 손실: 0.058884, 검증 손실: 0.088919, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.088919\n",
      "에포크 465/10000 - 훈련 손실: 0.059891, 검증 손실: 0.088708, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.088708\n",
      "에포크 466/10000 - 훈련 손실: 0.057853, 검증 손실: 0.087939, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.087939\n",
      "에포크 467/10000 - 훈련 손실: 0.056562, 검증 손실: 0.087722, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.087722\n",
      "에포크 468/10000 - 훈련 손실: 0.058006, 검증 손실: 0.087643, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 469/10000 - 훈련 손실: 0.058614, 검증 손실: 0.087667, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 470/10000 - 훈련 손실: 0.058310, 검증 손실: 0.087421, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.087421\n",
      "에포크 471/10000 - 훈련 손실: 0.061735, 검증 손실: 0.087084, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.087084\n",
      "에포크 472/10000 - 훈련 손실: 0.059732, 검증 손실: 0.086320, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.086320\n",
      "에포크 473/10000 - 훈련 손실: 0.059513, 검증 손실: 0.085856, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.085856\n",
      "에포크 474/10000 - 훈련 손실: 0.056143, 검증 손실: 0.085723, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.085723\n",
      "에포크 475/10000 - 훈련 손실: 0.057736, 검증 손실: 0.085934, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 476/10000 - 훈련 손실: 0.058676, 검증 손실: 0.085697, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 477/10000 - 훈련 손실: 0.059880, 검증 손실: 0.085760, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 478/10000 - 훈련 손실: 0.057245, 검증 손실: 0.085679, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 479/10000 - 훈련 손실: 0.058802, 검증 손실: 0.085316, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.085316\n",
      "에포크 480/10000 - 훈련 손실: 0.062681, 검증 손실: 0.084767, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.084767\n",
      "에포크 481/10000 - 훈련 손실: 0.057235, 검증 손실: 0.084509, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.084509\n",
      "에포크 482/10000 - 훈련 손실: 0.058721, 검증 손실: 0.084278, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.084278\n",
      "에포크 483/10000 - 훈련 손실: 0.058844, 검증 손실: 0.083997, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.083997\n",
      "에포크 484/10000 - 훈련 손실: 0.054079, 검증 손실: 0.083402, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.083402\n",
      "에포크 485/10000 - 훈련 손실: 0.057566, 검증 손실: 0.083467, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 486/10000 - 훈련 손실: 0.058882, 검증 손실: 0.083648, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 487/10000 - 훈련 손실: 0.059282, 검증 손실: 0.083086, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.083086\n",
      "에포크 488/10000 - 훈련 손실: 0.057399, 검증 손실: 0.083109, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 489/10000 - 훈련 손실: 0.055271, 검증 손실: 0.083235, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 490/10000 - 훈련 손실: 0.055267, 검증 손실: 0.083538, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 491/10000 - 훈련 손실: 0.054723, 검증 손실: 0.083992, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 492/10000 - 훈련 손실: 0.056672, 검증 손실: 0.084537, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 493/10000 - 훈련 손실: 0.056233, 검증 손실: 0.084789, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 494/10000 - 훈련 손실: 0.057005, 검증 손실: 0.084489, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 495/10000 - 훈련 손실: 0.054666, 검증 손실: 0.084009, 학습률: 0.00000100, 인내심 카운터: 8\n",
      "에포크 496/10000 - 훈련 손실: 0.053523, 검증 손실: 0.083951, 학습률: 0.00000100, 인내심 카운터: 9\n",
      "에포크 497/10000 - 훈련 손실: 0.053939, 검증 손실: 0.083514, 학습률: 0.00000100, 인내심 카운터: 10\n",
      "에포크 498/10000 - 훈련 손실: 0.057414, 검증 손실: 0.083690, 학습률: 0.00000100, 인내심 카운터: 11\n",
      "에포크 499/10000 - 훈련 손실: 0.052906, 검증 손실: 0.084204, 학습률: 0.00000100, 인내심 카운터: 12\n",
      "에포크 500/10000 - 훈련 손실: 0.056478, 검증 손실: 0.084299, 학습률: 0.00000100, 인내심 카운터: 13\n",
      "에포크 501/10000 - 훈련 손실: 0.054021, 검증 손실: 0.083426, 학습률: 0.00000100, 인내심 카운터: 14\n",
      "에포크 502/10000 - 훈련 손실: 0.053648, 검증 손실: 0.083321, 학습률: 0.00000100, 인내심 카운터: 15\n",
      "에포크 503/10000 - 훈련 손실: 0.056400, 검증 손실: 0.083172, 학습률: 0.00000100, 인내심 카운터: 16\n",
      "에포크 504/10000 - 훈련 손실: 0.057712, 검증 손실: 0.082493, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.082493\n",
      "에포크 505/10000 - 훈련 손실: 0.052333, 검증 손실: 0.082576, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 506/10000 - 훈련 손실: 0.055938, 검증 손실: 0.083260, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 507/10000 - 훈련 손실: 0.054894, 검증 손실: 0.083649, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 508/10000 - 훈련 손실: 0.051243, 검증 손실: 0.082978, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 509/10000 - 훈련 손실: 0.054773, 검증 손실: 0.082275, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.082275\n",
      "에포크 510/10000 - 훈련 손실: 0.053535, 검증 손실: 0.081998, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.081998\n",
      "에포크 511/10000 - 훈련 손실: 0.051770, 검증 손실: 0.081885, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.081885\n",
      "에포크 512/10000 - 훈련 손실: 0.055123, 검증 손실: 0.082137, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 513/10000 - 훈련 손실: 0.050674, 검증 손실: 0.082306, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 514/10000 - 훈련 손실: 0.054409, 검증 손실: 0.082251, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 515/10000 - 훈련 손실: 0.051478, 검증 손실: 0.081984, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 516/10000 - 훈련 손실: 0.055229, 검증 손실: 0.081733, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.081733\n",
      "에포크 517/10000 - 훈련 손실: 0.054011, 검증 손실: 0.081087, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.081087\n",
      "에포크 518/10000 - 훈련 손실: 0.054595, 검증 손실: 0.080456, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.080456\n",
      "에포크 519/10000 - 훈련 손실: 0.052210, 검증 손실: 0.080509, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 520/10000 - 훈련 손실: 0.048464, 검증 손실: 0.080147, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.080147\n",
      "에포크 521/10000 - 훈련 손실: 0.054222, 검증 손실: 0.079787, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.079787\n",
      "에포크 522/10000 - 훈련 손실: 0.057964, 검증 손실: 0.079807, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 523/10000 - 훈련 손실: 0.055128, 검증 손실: 0.079989, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 524/10000 - 훈련 손실: 0.051250, 검증 손실: 0.079877, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 525/10000 - 훈련 손실: 0.054033, 검증 손실: 0.079587, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.079587\n",
      "에포크 526/10000 - 훈련 손실: 0.052060, 검증 손실: 0.079721, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 527/10000 - 훈련 손실: 0.051414, 검증 손실: 0.079833, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 528/10000 - 훈련 손실: 0.055309, 검증 손실: 0.079567, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 529/10000 - 훈련 손실: 0.053408, 검증 손실: 0.079252, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.079252\n",
      "에포크 530/10000 - 훈련 손실: 0.051531, 검증 손실: 0.078638, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.078638\n",
      "에포크 531/10000 - 훈련 손실: 0.051930, 검증 손실: 0.078148, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.078148\n",
      "에포크 532/10000 - 훈련 손실: 0.050825, 검증 손실: 0.077521, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.077521\n",
      "에포크 533/10000 - 훈련 손실: 0.053344, 검증 손실: 0.077631, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 534/10000 - 훈련 손실: 0.049816, 검증 손실: 0.077909, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 535/10000 - 훈련 손실: 0.052841, 검증 손실: 0.077476, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 536/10000 - 훈련 손실: 0.052813, 검증 손실: 0.077383, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.077383\n",
      "에포크 537/10000 - 훈련 손실: 0.053489, 검증 손실: 0.076725, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.076725\n",
      "에포크 538/10000 - 훈련 손실: 0.051460, 검증 손실: 0.076603, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.076603\n",
      "에포크 539/10000 - 훈련 손실: 0.054502, 검증 손실: 0.076573, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 540/10000 - 훈련 손실: 0.052589, 검증 손실: 0.076603, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 541/10000 - 훈련 손실: 0.050754, 검증 손실: 0.076650, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 542/10000 - 훈련 손실: 0.051460, 검증 손실: 0.076734, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 543/10000 - 훈련 손실: 0.052022, 검증 손실: 0.076867, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 544/10000 - 훈련 손실: 0.054562, 검증 손실: 0.076386, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.076386\n",
      "에포크 545/10000 - 훈련 손실: 0.050872, 검증 손실: 0.076235, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.076235\n",
      "에포크 546/10000 - 훈련 손실: 0.050864, 검증 손실: 0.075931, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.075931\n",
      "에포크 547/10000 - 훈련 손실: 0.049095, 검증 손실: 0.075906, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 548/10000 - 훈련 손실: 0.048732, 검증 손실: 0.076639, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 549/10000 - 훈련 손실: 0.048968, 검증 손실: 0.076518, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 550/10000 - 훈련 손실: 0.054405, 검증 손실: 0.076375, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 551/10000 - 훈련 손실: 0.053344, 검증 손실: 0.076295, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 552/10000 - 훈련 손실: 0.047125, 검증 손실: 0.075679, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.075679\n",
      "에포크 553/10000 - 훈련 손실: 0.050995, 검증 손실: 0.075467, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.075467\n",
      "에포크 554/10000 - 훈련 손실: 0.050319, 검증 손실: 0.075282, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.075282\n",
      "에포크 555/10000 - 훈련 손실: 0.048168, 검증 손실: 0.075566, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 556/10000 - 훈련 손실: 0.048648, 검증 손실: 0.075773, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 557/10000 - 훈련 손실: 0.049243, 검증 손실: 0.076332, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 558/10000 - 훈련 손실: 0.048603, 검증 손실: 0.076049, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 559/10000 - 훈련 손실: 0.048070, 검증 손실: 0.076485, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 560/10000 - 훈련 손실: 0.048080, 검증 손실: 0.076478, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 561/10000 - 훈련 손실: 0.049082, 검증 손실: 0.076677, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 562/10000 - 훈련 손실: 0.045316, 검증 손실: 0.076532, 학습률: 0.00000100, 인내심 카운터: 8\n",
      "에포크 563/10000 - 훈련 손실: 0.046884, 검증 손실: 0.076307, 학습률: 0.00000100, 인내심 카운터: 9\n",
      "에포크 564/10000 - 훈련 손실: 0.050680, 검증 손실: 0.076795, 학습률: 0.00000100, 인내심 카운터: 10\n",
      "에포크 565/10000 - 훈련 손실: 0.049036, 검증 손실: 0.076771, 학습률: 0.00000100, 인내심 카운터: 11\n",
      "에포크 566/10000 - 훈련 손실: 0.050638, 검증 손실: 0.076039, 학습률: 0.00000100, 인내심 카운터: 12\n",
      "에포크 567/10000 - 훈련 손실: 0.049934, 검증 손실: 0.075211, 학습률: 0.00000100, 인내심 카운터: 13\n",
      "에포크 568/10000 - 훈련 손실: 0.045760, 검증 손실: 0.075066, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.075066\n",
      "에포크 569/10000 - 훈련 손실: 0.046005, 검증 손실: 0.074992, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 570/10000 - 훈련 손실: 0.048837, 검증 손실: 0.074638, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.074638\n",
      "에포크 571/10000 - 훈련 손실: 0.048620, 검증 손실: 0.073902, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.073902\n",
      "에포크 572/10000 - 훈련 손실: 0.047408, 검증 손실: 0.073646, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.073646\n",
      "에포크 573/10000 - 훈련 손실: 0.047849, 검증 손실: 0.073073, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.073073\n",
      "에포크 574/10000 - 훈련 손실: 0.044613, 검증 손실: 0.073208, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 575/10000 - 훈련 손실: 0.047063, 검증 손실: 0.073119, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 576/10000 - 훈련 손실: 0.046728, 검증 손실: 0.072765, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.072765\n",
      "에포크 577/10000 - 훈련 손실: 0.046805, 검증 손실: 0.072408, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.072408\n",
      "에포크 578/10000 - 훈련 손실: 0.048005, 검증 손실: 0.072343, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 579/10000 - 훈련 손실: 0.051208, 검증 손실: 0.072422, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 580/10000 - 훈련 손실: 0.044186, 검증 손실: 0.073028, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 581/10000 - 훈련 손실: 0.044579, 검증 손실: 0.072865, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 582/10000 - 훈련 손실: 0.044111, 검증 손실: 0.073182, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 583/10000 - 훈련 손실: 0.047272, 검증 손실: 0.073936, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 584/10000 - 훈련 손실: 0.045871, 검증 손실: 0.074903, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 585/10000 - 훈련 손실: 0.046204, 검증 손실: 0.075483, 학습률: 0.00000100, 인내심 카운터: 8\n",
      "에포크 586/10000 - 훈련 손실: 0.046354, 검증 손실: 0.075269, 학습률: 0.00000100, 인내심 카운터: 9\n",
      "에포크 587/10000 - 훈련 손실: 0.047062, 검증 손실: 0.074275, 학습률: 0.00000100, 인내심 카운터: 10\n",
      "에포크 588/10000 - 훈련 손실: 0.045535, 검증 손실: 0.074793, 학습률: 0.00000100, 인내심 카운터: 11\n",
      "에포크 589/10000 - 훈련 손실: 0.048261, 검증 손실: 0.074914, 학습률: 0.00000100, 인내심 카운터: 12\n",
      "에포크 590/10000 - 훈련 손실: 0.045750, 검증 손실: 0.075218, 학습률: 0.00000100, 인내심 카운터: 13\n",
      "에포크 591/10000 - 훈련 손실: 0.044156, 검증 손실: 0.074701, 학습률: 0.00000100, 인내심 카운터: 14\n",
      "에포크 592/10000 - 훈련 손실: 0.045355, 검증 손실: 0.073921, 학습률: 0.00000100, 인내심 카운터: 15\n",
      "에포크 593/10000 - 훈련 손실: 0.044548, 검증 손실: 0.073343, 학습률: 0.00000100, 인내심 카운터: 16\n",
      "에포크 594/10000 - 훈련 손실: 0.043767, 검증 손실: 0.073476, 학습률: 0.00000100, 인내심 카운터: 17\n",
      "에포크 595/10000 - 훈련 손실: 0.045805, 검증 손실: 0.072548, 학습률: 0.00000100, 인내심 카운터: 18\n",
      "에포크 596/10000 - 훈련 손실: 0.045218, 검증 손실: 0.071917, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.071917\n",
      "에포크 597/10000 - 훈련 손실: 0.050530, 검증 손실: 0.072146, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 598/10000 - 훈련 손실: 0.046732, 검증 손실: 0.071938, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 599/10000 - 훈련 손실: 0.043273, 검증 손실: 0.071590, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.071590\n",
      "에포크 600/10000 - 훈련 손실: 0.045371, 검증 손실: 0.071265, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.071265\n",
      "에포크 601/10000 - 훈련 손실: 0.044622, 검증 손실: 0.071398, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 602/10000 - 훈련 손실: 0.045209, 검증 손실: 0.071837, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 603/10000 - 훈련 손실: 0.041815, 검증 손실: 0.072235, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 604/10000 - 훈련 손실: 0.043617, 검증 손실: 0.071885, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 605/10000 - 훈련 손실: 0.043768, 검증 손실: 0.072089, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 606/10000 - 훈련 손실: 0.044094, 검증 손실: 0.071701, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 607/10000 - 훈련 손실: 0.041046, 검증 손실: 0.070976, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.070976\n",
      "에포크 608/10000 - 훈련 손실: 0.042850, 검증 손실: 0.070683, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.070683\n",
      "에포크 609/10000 - 훈련 손실: 0.043778, 검증 손실: 0.070796, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 610/10000 - 훈련 손실: 0.044369, 검증 손실: 0.070659, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 611/10000 - 훈련 손실: 0.044001, 검증 손실: 0.070750, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 612/10000 - 훈련 손실: 0.044109, 검증 손실: 0.070797, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 613/10000 - 훈련 손실: 0.043679, 검증 손실: 0.071284, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 614/10000 - 훈련 손실: 0.043976, 검증 손실: 0.071820, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 615/10000 - 훈련 손실: 0.043155, 검증 손실: 0.070945, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 616/10000 - 훈련 손실: 0.042501, 검증 손실: 0.070298, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.070298\n",
      "에포크 617/10000 - 훈련 손실: 0.045257, 검증 손실: 0.069983, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.069983\n",
      "에포크 618/10000 - 훈련 손실: 0.043957, 검증 손실: 0.069688, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.069688\n",
      "에포크 619/10000 - 훈련 손실: 0.044277, 검증 손실: 0.069927, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 620/10000 - 훈련 손실: 0.043315, 검증 손실: 0.069725, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 621/10000 - 훈련 손실: 0.043180, 검증 손실: 0.069659, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 622/10000 - 훈련 손실: 0.042749, 검증 손실: 0.069652, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 623/10000 - 훈련 손실: 0.042228, 검증 손실: 0.069667, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 624/10000 - 훈련 손실: 0.042350, 검증 손실: 0.069502, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.069502\n",
      "에포크 625/10000 - 훈련 손실: 0.043446, 검증 손실: 0.069848, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 626/10000 - 훈련 손실: 0.046943, 검증 손실: 0.070147, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 627/10000 - 훈련 손실: 0.044057, 검증 손실: 0.069696, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 628/10000 - 훈련 손실: 0.041625, 검증 손실: 0.070127, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 629/10000 - 훈련 손실: 0.042084, 검증 손실: 0.069756, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 630/10000 - 훈련 손실: 0.043972, 검증 손실: 0.069295, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.069295\n",
      "에포크 631/10000 - 훈련 손실: 0.046377, 검증 손실: 0.069985, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 632/10000 - 훈련 손실: 0.042313, 검증 손실: 0.070531, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 633/10000 - 훈련 손실: 0.042294, 검증 손실: 0.070634, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 634/10000 - 훈련 손실: 0.041973, 검증 손실: 0.070357, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 635/10000 - 훈련 손실: 0.041399, 검증 손실: 0.070531, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 636/10000 - 훈련 손실: 0.042600, 검증 손실: 0.070218, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 637/10000 - 훈련 손실: 0.039814, 검증 손실: 0.069592, 학습률: 0.00000100, 인내심 카운터: 7\n",
      "에포크 638/10000 - 훈련 손실: 0.042171, 검증 손실: 0.069987, 학습률: 0.00000100, 인내심 카운터: 8\n",
      "에포크 639/10000 - 훈련 손실: 0.041154, 검증 손실: 0.069042, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.069042\n",
      "에포크 640/10000 - 훈련 손실: 0.045396, 검증 손실: 0.069030, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 641/10000 - 훈련 손실: 0.043219, 검증 손실: 0.068661, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.068661\n",
      "에포크 642/10000 - 훈련 손실: 0.040534, 검증 손실: 0.068471, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.068471\n",
      "에포크 643/10000 - 훈련 손실: 0.041140, 검증 손실: 0.068597, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 644/10000 - 훈련 손실: 0.041640, 검증 손실: 0.068881, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 645/10000 - 훈련 손실: 0.042475, 검증 손실: 0.068550, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 646/10000 - 훈련 손실: 0.041121, 검증 손실: 0.068043, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.068043\n",
      "에포크 647/10000 - 훈련 손실: 0.043863, 검증 손실: 0.067899, 학습률: 0.00000100, 인내심 카운터: 0\n",
      "검증 손실 개선! 최고 검증 손실: 0.067899\n",
      "에포크 648/10000 - 훈련 손실: 0.041600, 검증 손실: 0.067997, 학습률: 0.00000100, 인내심 카운터: 1\n",
      "에포크 649/10000 - 훈련 손실: 0.042850, 검증 손실: 0.068219, 학습률: 0.00000100, 인내심 카운터: 2\n",
      "에포크 650/10000 - 훈련 손실: 0.040154, 검증 손실: 0.069280, 학습률: 0.00000100, 인내심 카운터: 3\n",
      "에포크 651/10000 - 훈련 손실: 0.041893, 검증 손실: 0.069780, 학습률: 0.00000100, 인내심 카운터: 4\n",
      "에포크 652/10000 - 훈련 손실: 0.039144, 검증 손실: 0.069570, 학습률: 0.00000100, 인내심 카운터: 5\n",
      "에포크 653/10000 - 훈련 손실: 0.041936, 검증 손실: 0.069614, 학습률: 0.00000100, 인내심 카운터: 6\n",
      "에포크 654/10000 - 훈련 손실: 0.040048, 검증 손실: 0.068955, 학습률: 0.00000100, 인내심 카운터: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Generate model\u001b[39;00m\n\u001b[32m     26\u001b[39m model = BMEDModel(hidden_nodes = hidden_nodes, hidden_layers = hidden_layers, dt = dt, scaler = scaler, dropout = dropout)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_norm\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschd_factor\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mschd_factor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, learning_rate, weight_decay, clip_norm, schd_factor)\u001b[39m\n\u001b[32m     33\u001b[39m train_loss += loss.item()\n\u001b[32m     34\u001b[39m num_batches += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# gradient clipping\u001b[39;00m\n\u001b[32m     39\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = clip_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\repos_python\\Project\\BMED\\BMED_Model\\bmed-NN\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\repos_python\\Project\\BMED\\BMED_Model\\bmed-NN\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\repos_python\\Project\\BMED\\BMED_Model\\bmed-NN\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Split dataset into train and test\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "idx = list(range(dataset_size))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_idx = idx[:train_size]\n",
    "val_idx = idx[train_size:train_size + val_size]\n",
    "test_idx = idx[train_size + val_size:]\n",
    "\n",
    "# Generate subsets\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "# Generate dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size = 1, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = False)\n",
    "\n",
    "# Generate model\n",
    "model = BMEDModel(hidden_nodes = hidden_nodes, hidden_layers = hidden_layers, dt = dt, scaler = scaler, dropout = dropout)\n",
    "\n",
    "# 모델 훈련\n",
    "train_model(model, train_loader, val_loader, epochs = epochs, learning_rate = lr, weight_decay = wd, clip_norm = clip_norm, schd_factor = schd_factor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>t</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>E</th>\n",
       "      <th>CF_LA</th>\n",
       "      <th>CA_LA</th>\n",
       "      <th>CF_K</th>\n",
       "      <th>CB_K</th>\n",
       "      <th>VF</th>\n",
       "      <th>VA</th>\n",
       "      <th>VB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.06</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.06</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>1.023</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.06</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.173</td>\n",
       "      <td>1.023</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.06</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.06</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.24</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.680</td>\n",
       "      <td>0.105</td>\n",
       "      <td>1.067</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.24</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.063</td>\n",
       "      <td>1.211</td>\n",
       "      <td>0.325</td>\n",
       "      <td>1.133</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.24</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.472</td>\n",
       "      <td>1.133</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>21</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.24</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.635</td>\n",
       "      <td>1.111</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>21</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.24</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.769</td>\n",
       "      <td>1.058</td>\n",
       "      <td>0.977</td>\n",
       "      <td>1.965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     exp    t      T   V     E  CF_LA  CA_LA   CF_K   CB_K     VF     VA     VB\n",
       "0      1  0.0  25.06  20  0.25  0.500  0.000  1.000  0.000  1.000  1.000  1.000\n",
       "1      1  1.0  25.06  20  0.25  0.482  0.007  0.993 -0.016  1.023  0.993  0.983\n",
       "2      1  2.0  25.06  20  0.25  0.454  0.035  0.811  0.173  1.023  0.993  0.983\n",
       "3      1  3.0  25.06  20  0.25  0.404  0.103  0.592  0.412  0.983  1.003  1.013\n",
       "4      1  4.0  25.06  20  0.25  0.298  0.214  0.376  0.625  0.943  1.024  1.034\n",
       "..   ...  ...    ...  ..   ...    ...    ...    ...    ...    ...    ...    ...\n",
       "126   21  2.0  20.24  20  0.25  0.909  0.030  1.680  0.105  1.067  0.968  1.965\n",
       "127   21  4.0  20.24  20  0.25  0.831  0.063  1.211  0.325  1.133  0.932  1.935\n",
       "128   21  5.0  20.24  20  0.25  0.800  0.101  0.959  0.472  1.133  0.932  1.935\n",
       "129   21  6.0  20.24  20  0.25  0.739  0.190  0.685  0.635  1.111  0.939  1.949\n",
       "130   21  7.0  20.24  20  0.25  0.650  0.319  0.462  0.769  1.058  0.977  1.965\n",
       "\n",
       "[131 rows x 12 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmed-NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
