{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\repos_python\\Project\\BMED\\BMED_Model\\bmed-NN\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import optuna\n",
    "from optuna.samplers import GPSampler\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b55bfb8490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward network for migration prediction\n",
    "class MigrationPredictor(nn.Module):\n",
    "    def __init__(self, hidden_nodes = 64, hidden_layers = 3, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_features = 7\n",
    "        n_outputs = 4\n",
    "\n",
    "        # Layer configuration\n",
    "        layers = []\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(n_features, hidden_nodes))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # hidden layers\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_nodes, hidden_nodes))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(hidden_nodes, n_outputs))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical Layers for State update\n",
    "class PhysicalLayer:\n",
    "    def __init__(self, dt = 0.1):\n",
    "        self.dt = dt\n",
    "    \n",
    "    def update_state(self, cur_states, migrations):\n",
    "        # Current States\n",
    "        T = cur_states[0]\n",
    "        V = cur_states[1]\n",
    "        E = cur_states[2]\n",
    "        CF_LA = cur_states[3]\n",
    "        CA_LA = cur_states[4]\n",
    "        CF_K = cur_states[5]\n",
    "        CB_K = cur_states[6]\n",
    "        VF = cur_states[7]\n",
    "        VA = cur_states[8]\n",
    "        VB = cur_states[9]\n",
    "        \n",
    "        # Migration\n",
    "        dNLA = migrations[0] * self.dt\n",
    "        dNK = migrations[1] * self.dt\n",
    "        dVA = migrations[2] * self.dt\n",
    "        dVB = migrations[3] * self.dt\n",
    "\n",
    "        # Fixed variables\n",
    "        nT = T\n",
    "        nV = V\n",
    "        nE = E     \n",
    "        \n",
    "        # New Volumes\n",
    "        nVF = VF - dVA - dVB\n",
    "        nVA = VA + dVA\n",
    "        nVB = VB + dVB\n",
    "\n",
    "        # New Concentrations\n",
    "        nCF_LA = (CF_LA * VF - dNLA) / nVF\n",
    "        nCA_LA = (CA_LA * VA + dNLA) / nVA\n",
    "        nCF_K = (CF_K * VF - dNK) / nVF\n",
    "        nCB_K = (CB_K * VB + dNK) / nVB\n",
    "\n",
    "        # New States\n",
    "        new_states = cur_states.clone()\n",
    "        new_states[0] = nT\n",
    "        new_states[1] = nV\n",
    "        new_states[2] = nE\n",
    "        new_states[3] = nCF_LA\n",
    "        new_states[4] = nCA_LA\n",
    "        new_states[5] = nCF_K\n",
    "        new_states[6] = nCB_K\n",
    "        new_states[7] = nVF\n",
    "        new_states[8] = nVA\n",
    "        new_states[9] = nVB\n",
    "        \n",
    "        return new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDDataset(Dataset):\n",
    "    def __init__(self, dict_spline):\n",
    "        self.states = ['T', 'V', 'E', 'CF_LA', 'CA_LA', 'CF_K', 'CB_K', 'VF', 'VA', 'VB']\n",
    "        self.experiments = []\n",
    "\n",
    "        for exp_id, exp_data in dict_spline.items():\n",
    "            # Save whole data of each experiment in one sample\n",
    "            exp_array = exp_data[self.states].values\n",
    "            times = exp_data['t'].values\n",
    "            self.experiments.append({\n",
    "                'init_state': torch.tensor(exp_array[0], dtype = torch.float32), # initial state\n",
    "                'measured_state': torch.tensor(exp_array, dtype = torch.float32), # whole measurements\n",
    "                'times': torch.tensor(times, dtype = torch.float32) # time points\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.experiments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.experiments[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDModel(nn.Module):\n",
    "    def __init__(self, hidden_nodes = 32, hidden_layers = 3, dt = 0.1, scaler = None, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.migration_predictor = MigrationPredictor(hidden_nodes, hidden_layers, dropout = dropout)\n",
    "        self.physical_layer = PhysicalLayer(dt)\n",
    "        self.scaler = scaler\n",
    "        self.dt = dt\n",
    "\n",
    "    def forward(self, init_state, times):\n",
    "\n",
    "        cur_state = init_state # batch size 1\n",
    "        cur_time = 0.0\n",
    "        pred_states = []\n",
    "        measured_indices = []\n",
    "\n",
    "        times = times\n",
    "        times_np = times[0].numpy()\n",
    "        max_time = times_np[-1]\n",
    "        measured_indices.append(0)\n",
    "\n",
    "        # 초기 상태 저장\n",
    "        pred_states.append(cur_state)\n",
    "\n",
    "        while cur_time < max_time:\n",
    "            # input_feature에 해당하는 변수만 정규화\n",
    "            input_state = cur_state[:, :7]  # input feature 추출, 2차원 유지지\n",
    "            \n",
    "            norm_input = self.scaler.transform(input_state.detach().numpy())\n",
    "            norm_input = torch.tensor(norm_input)\n",
    "            \n",
    "            # 상태 예측\n",
    "            migration = self.migration_predictor(norm_input)  # (1, 6) -> (1, 3)\n",
    "            cur_state = self.physical_layer.update_state(cur_state[0], migration[0]).unsqueeze(0)  # (1,8)\n",
    "            pred_states.append(cur_state)  # (1, 8)\n",
    "            cur_time += self.dt\n",
    "\n",
    "            # 측정 시간과 매칭\n",
    "            for t in times_np:\n",
    "                if abs(cur_time - t) < self.dt/2:\n",
    "                    measured_indices.append(len(pred_states) - 1)\n",
    "\n",
    "        # 현재 배치의 예측 상태들을 스택\n",
    "        pred_states = torch.cat(pred_states, dim=0)  # (n_steps, 8)\n",
    "\n",
    "        return pred_states, measured_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pred_states, measured_indices, measured_states):\n",
    "\n",
    "    # default weight\n",
    "    default_wt = {\n",
    "        'T': 0,\n",
    "        'V': 0,\n",
    "        'E': 0,\n",
    "        'CF_LA': 1,\n",
    "        'CA_LA': 1,\n",
    "        'CF_K': 0.1,\n",
    "        'CB_K': 0.1,\n",
    "        'VF': 0.5,\n",
    "        'VA': 1,\n",
    "        'VB': 0.1\n",
    "    }\n",
    "\n",
    "    wt_tensor = torch.tensor([\n",
    "        default_wt['T'], default_wt['V'], default_wt['E'], \n",
    "        default_wt['CF_LA'], default_wt['CA_LA'], default_wt['CF_K'], default_wt['CB_K'],\n",
    "        default_wt['VF'], default_wt['VA'], default_wt['VB']\n",
    "    ])\n",
    "\n",
    "    total_loss = 0\n",
    "    for idx, measured_state in zip(measured_indices, measured_states[0]):\n",
    "        predicted_state = pred_states[idx]\n",
    "        sq_errors = (predicted_state - measured_state) ** 2\n",
    "        wt_errors = sq_errors * wt_tensor\n",
    "\n",
    "        total_loss += torch.mean(wt_errors)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_exp = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for exp in val_loader:\n",
    "            init_state = exp['init_state']\n",
    "            measured_states = exp['measured_state']\n",
    "            times = exp['times']\n",
    "\n",
    "            pred_states, measured_indices = model(init_state, times)\n",
    "            \n",
    "\n",
    "            loss = custom_loss(pred_states, measured_indices, measured_states)\n",
    "            total_loss += loss.item()\n",
    "            num_exp += 1\n",
    "\n",
    "    avg_loss = total_loss / num_exp\n",
    "\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader,epochs = 100, learning_rate = 0.001, weight_decay = 1e-5, clip_norm = 0.1, schd_factor = 0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay= weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience = 5, factor = schd_factor, min_lr = 1e-6\n",
    "    )\n",
    "\n",
    "    patience = 5\n",
    "    min_delta = 0.0001\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    stopped_epoch = epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for exp in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            init_state = exp['init_state']\n",
    "            measured_state = exp['measured_state']\n",
    "            times = exp['times']\n",
    "\n",
    "            # Simulation\n",
    "            pred_state, measured_indices = model(init_state, times)\n",
    "\n",
    "            # Loss\n",
    "            loss = custom_loss(pred_state, measured_indices, measured_state)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = evaluate_model(model, val_loader)\n",
    "\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping 조건 추가\n",
    "        if patience_counter >= patience:\n",
    "            stopped_epoch = epoch + 1\n",
    "            # 최적의 모델 상태로 복원\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "                # 학습이 완료된 후 최적의 모델 상태로 복원\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return stopped_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_calculator(pred_states, measured_indices, measured_states):\n",
    "\n",
    "    pred_value = pred_states[measured_indices][:,3:].numpy()\n",
    "    target_value = measured_states[0][:,3:].numpy()\n",
    "    r2_scores = []\n",
    "    for col in range(pred_value.shape[1]):\n",
    "        col_r2 = r2_score(pred_value[:,col], target_value[:,col])\n",
    "        r2_scores.append(col_r2)\n",
    "\n",
    "    return np.mean(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_path, exp_idx = None):\n",
    "    # Load raw data\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    if exp_idx is not None:\n",
    "        df = df[df['exp'].isin(exp_idx)]\n",
    "\n",
    "    # split the data by experiment number\n",
    "    dict_spline = {}\n",
    "    for exp in df['exp'].unique():\n",
    "        dict_spline[exp] = df[df['exp'] == exp].sort_values('t')\n",
    "\n",
    "    # scaler\n",
    "    scaler = StandardScaler()\n",
    "    col_to_scale = ['T', 'V', 'E', 'CF_LA', 'CA_LA', 'CF_K', 'CB_K']\n",
    "    scaler.fit(df[col_to_scale].values)\n",
    "    return dict_spline, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model, dataset, exp_idx=0):\n",
    "    # 데이터 준비\n",
    "    exp = dataset.experiments[exp_idx]\n",
    "    init_state = torch.tensor(exp['init_state'], dtype=torch.float32).unsqueeze(0)\n",
    "    times = torch.tensor(exp['times'], dtype=torch.float32).unsqueeze(0)\n",
    "    measured_state = torch.tensor(exp['measured_state'], dtype=torch.float32)\n",
    "    \n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        pred_states, measured_indices = model(init_state, times)\n",
    "    \n",
    "    # 예측을 위한 시간 포인트 생성 (dt = 0.1 간격)\n",
    "    t_pred = torch.arange(0, times[0][-1].item() + 0.1, 0.1)\n",
    "    \n",
    "    # 변수 이름과 단위\n",
    "    var_names = {\n",
    "        'T': 'Temperature (°C)',\n",
    "        'V': 'Voltage (V)',\n",
    "        'E': 'Electric Field (V/cm)',\n",
    "        'CF_LA': 'Feed LA Conc. (M)',\n",
    "        'CA_LA': 'Acid LA Conc. (M)',\n",
    "        'CF_K': 'Feed K Conc. (M)',\n",
    "        'CB_K': 'Base K Conc. (M)',\n",
    "        'VF': 'Feed Volume (L)',\n",
    "        'VA': 'Acid Volume (L)',\n",
    "        'VB': 'Base Volume (L)'\n",
    "    }\n",
    "    \n",
    "    # 그래프 그리기\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(15, 20))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (var, label) in enumerate(var_names.items()):\n",
    "        ax = axes[i]\n",
    "        # 실제 측정값 (점으로 표시)\n",
    "        ax.plot(times[0].numpy(), measured_state[:, i].numpy(), \n",
    "                'bo', label='Measured', markersize=6)\n",
    "        # 예측값 (연속 선으로 표시)\n",
    "        ax.plot(t_pred.numpy(), pred_states[:, i].numpy(), \n",
    "                'r-', label='Predicted', linewidth=2)\n",
    "        \n",
    "        ax.set_title(label, fontsize=12, pad=10)\n",
    "        ax.set_xlabel('Time (hr)', fontsize=10)\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_callback(study, trial):\n",
    "    with open('hpOpt_study.pkl', 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "\n",
    "    # 2. best value만 json으로 저장\n",
    "    best_trial_info = {\n",
    "        'best_params': study.best_trial.params,\n",
    "        'best_value': study.best_trial.value,\n",
    "        'best_avg_epoch': study.best_trial.user_attrs['avg_epoch']\n",
    "    }\n",
    "\n",
    "    with open('hpOpt_checkpoint.json', 'w') as f:\n",
    "        json.dump(best_trial_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # load data\n",
    "    df_path = '../data/BMED_data_v8.csv'\n",
    "    dict_spline, scaler = prepare_data(df_path)\n",
    "\n",
    "    # Hyperparameter 설정\n",
    "    hidden_nodes = trial.suggest_int('hidden_nodes', 16, 32, step = 16)\n",
    "    hidden_layers = trial.suggest_int('hidden_layers', 1, 2, step = 1)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log = True)\n",
    "    wd = trial.suggest_float('weight_decay', 1e-6, 1e-3, log = True)\n",
    "    epochs = trial.suggest_int('epochs', 10, 20, step = 1)\n",
    "    dropout = trial.suggest_int('dropout', 20, 50, step = 10) / 100\n",
    "    clip_norm = trial.suggest_int('clip_norm', 10, 100, step = 10) / 100\n",
    "    schd_factor = trial.suggest_int('schd_factor', 10, 50, step = 10) / 100\n",
    "    dt = 0.1\n",
    "\n",
    "    # 5-fold 설정\n",
    "    kfolds = 5\n",
    "    kf = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n",
    "\n",
    "    # dataset 생성\n",
    "    dataset = BMEDDataset(dict_spline)\n",
    "\n",
    "    # k-fold 교차 검증 스코어 저장\n",
    "    fold_scores = []\n",
    "    fold_epochs = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        # kfold로 전체 dataset에 대해서 train과 validation subset을 생성\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Dataloader 생성\n",
    "        train_loader = DataLoader(train_subset, batch_size = 1, shuffle = True)\n",
    "        val_loader = DataLoader(val_subset, batch_size = 1, shuffle = False)\n",
    "\n",
    "        # 모델 생성\n",
    "        model = BMEDModel(hidden_nodes=hidden_nodes, hidden_layers=hidden_layers, dt = dt, scaler=scaler, dropout=dropout)\n",
    "\n",
    "        # 모델 학습\n",
    "        stopped_epoch = train_model(\n",
    "            model = model, train_loader = train_loader, val_loader=val_loader, \n",
    "            epochs = epochs, learning_rate = lr, weight_decay = wd, clip_norm = clip_norm, schd_factor = schd_factor)   \n",
    "        fold_epochs.append(stopped_epoch)\n",
    "\n",
    "        # 모델 평가\n",
    "        avg_loss = evaluate_model(model, val_loader)  \n",
    "        fold_scores.append(avg_loss)\n",
    "    \n",
    "    fold_epoch_str = ', '.join([f'fold {i+1}: {epoch}' for i, epoch in enumerate(fold_epochs)])\n",
    "    avg_epoch = sum(fold_epochs) / len(fold_epochs)\n",
    "    print(f'[{fold_epoch_str}, avg_epoch: {avg_epoch:.1f}]')\n",
    "\n",
    "    trial.set_user_attr('avg_epoch', avg_epoch)\n",
    "    \n",
    "    return np.mean(fold_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_29872\\176618578.py:2: ExperimentalWarning: GPSampler is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  sampler = GPSampler(n_startup_trials=10, seed=42)\n",
      "[I 2025-04-23 14:34:31,911] A new study created in memory with name: hpOpt\n",
      "[I 2025-04-23 14:38:41,564] Trial 1 finished with value: 508.89494350090627 and parameters: {'hidden_nodes': 32, 'hidden_layers': 1, 'lr': 3.726935131340302e-05, 'weight_decay': 0.0001284287969942974, 'epochs': 17, 'dropout': 40, 'clip_norm': 80, 'schd_factor': 40}. Best is trial 1 with value: 508.89494350090627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 9, fold 2: 7, fold 3: 17, fold 4: 8, fold 5: 10, avg_epoch: 10.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:40:02,512] Trial 0 finished with value: 0.09126608686521649 and parameters: {'hidden_nodes': 32, 'hidden_layers': 2, 'lr': 0.0011319905725288555, 'weight_decay': 3.342500955866597e-05, 'epochs': 13, 'dropout': 50, 'clip_norm': 50, 'schd_factor': 10}. Best is trial 0 with value: 0.09126608686521649.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 13, fold 2: 13, fold 3: 13, fold 4: 11, fold 5: 13, avg_epoch: 12.6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:40:10,939] Trial 3 finished with value: 0.08593458508141338 and parameters: {'hidden_nodes': 32, 'hidden_layers': 2, 'lr': 0.0029726427562539585, 'weight_decay': 0.0001984332529376147, 'epochs': 14, 'dropout': 40, 'clip_norm': 70, 'schd_factor': 10}. Best is trial 3 with value: 0.08593458508141338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 14, fold 2: 12, fold 3: 10, fold 4: 14, fold 5: 14, avg_epoch: 12.8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:40:25,807] Trial 4 finished with value: 3406.3997380609812 and parameters: {'hidden_nodes': 16, 'hidden_layers': 2, 'lr': 2.3697494032769806e-05, 'weight_decay': 1.571763644443812e-05, 'epochs': 17, 'dropout': 50, 'clip_norm': 30, 'schd_factor': 20}. Best is trial 3 with value: 0.08593458508141338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 10, fold 2: 13, fold 3: 17, fold 4: 7, fold 5: 17, avg_epoch: 12.8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:40:30,798] Trial 2 finished with value: 22.117637841627 and parameters: {'hidden_nodes': 32, 'hidden_layers': 1, 'lr': 0.0001766610712988348, 'weight_decay': 1.9079907798000062e-05, 'epochs': 20, 'dropout': 50, 'clip_norm': 10, 'schd_factor': 30}. Best is trial 3 with value: 0.08593458508141338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 7, fold 2: 8, fold 3: 20, fold 4: 18, fold 5: 20, avg_epoch: 14.6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:43:19,525] Trial 5 finished with value: 160.32787550866604 and parameters: {'hidden_nodes': 16, 'hidden_layers': 1, 'lr': 2.4063814199383383e-05, 'weight_decay': 2.5751477643455706e-06, 'epochs': 17, 'dropout': 30, 'clip_norm': 10, 'schd_factor': 10}. Best is trial 3 with value: 0.08593458508141338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 10, fold 2: 8, fold 3: 12, fold 4: 12, fold 5: 17, avg_epoch: 11.8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:45:52,799] Trial 9 finished with value: 206.0491349565983 and parameters: {'hidden_nodes': 16, 'hidden_layers': 1, 'lr': 0.00010202789741458674, 'weight_decay': 8.272035808633498e-05, 'epochs': 20, 'dropout': 40, 'clip_norm': 40, 'schd_factor': 40}. Best is trial 3 with value: 0.08593458508141338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 7, fold 2: 20, fold 3: 12, fold 4: 8, fold 5: 20, avg_epoch: 13.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:46:08,005] Trial 6 finished with value: 1.7087290444690733 and parameters: {'hidden_nodes': 32, 'hidden_layers': 2, 'lr': 0.0006351278935515012, 'weight_decay': 0.00014201106555709168, 'epochs': 16, 'dropout': 20, 'clip_norm': 60, 'schd_factor': 10}. Best is trial 3 with value: 0.08593458508141338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 7, fold 2: 16, fold 3: 16, fold 4: 16, fold 5: 16, avg_epoch: 14.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:46:38,957] Trial 7 finished with value: 0.1605440355464816 and parameters: {'hidden_nodes': 32, 'hidden_layers': 2, 'lr': 0.0003618270106975604, 'weight_decay': 1.3947375329083607e-05, 'epochs': 15, 'dropout': 40, 'clip_norm': 100, 'schd_factor': 40}. Best is trial 3 with value: 0.08593458508141338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 15, fold 2: 15, fold 3: 15, fold 4: 15, fold 5: 15, avg_epoch: 15.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:47:03,161] Trial 8 finished with value: 3.378379808906466 and parameters: {'hidden_nodes': 32, 'hidden_layers': 2, 'lr': 0.00012861779891723245, 'weight_decay': 0.0006079467593961284, 'epochs': 17, 'dropout': 40, 'clip_norm': 40, 'schd_factor': 20}. Best is trial 3 with value: 0.08593458508141338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1: 17, fold 2: 17, fold 3: 6, fold 4: 17, fold 5: 17, avg_epoch: 14.8]\n"
     ]
    }
   ],
   "source": [
    "# generate study with gaussian process sampler\n",
    "sampler = GPSampler(n_startup_trials=10, seed=42)\n",
    "n_trials = 100\n",
    "study = optuna.create_study(\n",
    "    study_name = 'hpOpt',\n",
    "    direction='minimize',\n",
    "    sampler=sampler,\n",
    "    load_if_exists = True)\n",
    "\n",
    "# optimize the hyperparameters\n",
    "study.optimize(objective, n_trials=n_trials, n_jobs=5, callbacks=[save_callback])\n",
    "\n",
    "# return the best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "best_value = study.best_value\n",
    "\n",
    "# optimize the model with the best hyperparameters\n",
    "results = {\n",
    "    'best_params': best_params,\n",
    "    'best_value': best_value,\n",
    "    'study': study\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = {\n",
    "            'best_params': results['best_params'],\n",
    "            'best_r2_score': float(results['best_value']),\n",
    "        }\n",
    "\n",
    "with open('hpOpt.json', 'w') as f:\n",
    "    json.dump(save_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmed-NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
