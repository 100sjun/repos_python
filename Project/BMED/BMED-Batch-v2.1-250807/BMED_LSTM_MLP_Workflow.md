# BMED LSTM-MLP í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì›Œí¬í”Œë¡œìš°

## ğŸ“Š í”„ë¡œì íŠ¸ ê°œìš”
**ëª©í‘œ**: Bipolar Membrane Electrodialysis (BMED) ë°°ì¹˜ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì´ìš©í•œ LSTM-MLP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ê°œë°œ  
**ë°ì´í„°**: 0.25ì‹œê°„ ê°„ê²© ì¸¡ì •, Feed/Acid/Baseì˜ LA/K ë†ë„, ë¶€í”¼, ì „ì••, ì „ë¥˜ ë³€í™”  
**ì •ê·œí™”**: 20% í™•ì¥ MinMaxScaler (ì •ë³´ ë³´ì¡´ + ì™¸ì‚½ ì•ˆì •ì„±)  
**ìµœì í™”**: Optuna ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, 5-fold Cross Validation

## ë°ì´í„° êµ¬ì¡° ë¶„ì„
**ì…ë ¥ ë³€ìˆ˜**: exp, V, E, t, VF, VA, VB, CFLA, CALA, CBLA, CFK, CAK, CBK, I  
**ì‹œê³„ì—´ íŠ¹ì„±**: 0.25ì‹œê°„ ê°„ê²©, ë°°ì¹˜ë³„ ì‹œí€€ìŠ¤ ë°ì´í„°  
**íƒ€ê²Ÿ ë³€ìˆ˜**: dCALA, dCBLA, dCAK, dCBK, dVA, dVB (ë³€í™”ëŸ‰)

---

# Phase 1: ë°ì´í„° ì „ì²˜ë¦¬ & êµ¬ì¡°í™” (2-3ì¼)

## 1.1 ë°ì´í„° ë¡œë”© & íƒìƒ‰ì  ë¶„ì„
```python
# ìˆœì°¨ì  ì½”ë“œ ì‘ì„± (í•¨ìˆ˜í™” ë‚˜ì¤‘ì—)
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import KFold
import optuna
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë”©
df = pd.read_csv('BMED_DATA_AG.csv')
experiments = df['exp'].unique()
time_steps = df['t'].unique()

print(f"ì‹¤í—˜ ìˆ˜: {len(experiments)}")
print(f"ì‹œê°„ ë‹¨ê³„: {len(time_steps)}")
print(f"ë°ì´í„° í˜•íƒœ: {df.shape}")
```

## 1.2 í™•ì¥ MinMax Scaling êµ¬í˜„ (20% ì™¸ì‚½ í—ˆìš©)
```python
# 20% í™•ì¥ MinMax Scaling - ì •ë³´ ë³´ì¡´ + ì™¸ì‚½ ì•ˆì •ì„±
class ExtendedMinMaxScaler:
    def __init__(self, extend_ratio=0.2):
        self.extend_ratio = extend_ratio
        self.original_min_ = None
        self.original_max_ = None
        self.extended_min_ = None
        self.extended_max_ = None
    
    def fit(self, X):
        self.original_min_ = X.min(axis=0)
        self.original_max_ = X.max(axis=0)
        
        # 20% í™•ì¥ ë²”ìœ„ ê³„ì‚° (ì™¸ì‚½ í—ˆìš©)
        range_size = self.original_max_ - self.original_min_
        extension = range_size * self.extend_ratio
        
        self.extended_min_ = self.original_min_ - extension
        self.extended_max_ = self.original_max_ + extension
        
        # ë†ë„ ë³€ìˆ˜ëŠ” 0 ì´í•˜ë¡œ ê°ˆ ìˆ˜ ì—†ìŒ (ë¬¼ë¦¬ì  ì œì•½)
        concentration_cols = [7, 8, 9, 10, 11]  # CFLA, CALA, CBLA, CFK, CAK, CBK ì¸ë±ìŠ¤
        for i in concentration_cols:
            if i < len(self.extended_min_):
                self.extended_min_[i] = max(0, self.extended_min_[i])
        
        return self
    
    def transform(self, X):
        return (X - self.extended_min_) / (self.extended_max_ - self.extended_min_)
    
    def inverse_transform(self, X_scaled):
        return X_scaled * (self.extended_max_ - self.extended_min_) + self.extended_min_

# ë³€ìˆ˜ë³„ ë§ì¶¤í˜• í™•ì¥ ë¹„ìœ¨ ì„¤ì •
extension_ratios = {
    'V': 0.15,      # ì „ì••: 15% (ì¸¡ì • ì•ˆì •ì )
    'E': 0.20,      # ì „ê¸°ì¥: 20% (ë³€ë™ ìˆìŒ)
    'VF': 0.10,     # Feed ë¶€í”¼: 10% (ë¬¼ë¦¬ì  ì œì•½)
    'VA': 0.10,     # Acid ë¶€í”¼: 10% (ë¬¼ë¦¬ì  ì œì•½)  
    'VB': 0.10,     # Base ë¶€í”¼: 10% (ë¬¼ë¦¬ì  ì œì•½)
    'CFLA': 0.25,   # Feed LAë†ë„: 25% (í° ë³€ë™)
    'CALA': 0.25,   # Acid LAë†ë„: 25% (í° ë³€ë™)
    'CBLA': 0.25,   # Base LAë†ë„: 25% (í° ë³€ë™)
    'CFK': 0.25,    # Feed Kë†ë„: 25% (í° ë³€ë™)
    'CAK': 0.25,    # Acid Kë†ë„: 25% (í° ë³€ë™)
    'CBK': 0.25,    # Base Kë†ë„: 25% (í° ë³€ë™)
    'I': 0.30       # ì „ë¥˜: 30% (ê°€ì¥ ë³€ë™ ì‹¬í•¨)
}

feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']

# ë³€ìˆ˜ë³„ ê°œë³„ ìŠ¤ì¼€ì¼ë§ ì ìš©
scalers = {}
df_scaled = df.copy()

for i, col in enumerate(feature_cols):
    ratio = extension_ratios.get(col, 0.2)  # ê¸°ë³¸ 20%
    scaler = ExtendedMinMaxScaler(extend_ratio=ratio)
    df_scaled[col] = scaler.fit_transform(df[[col]]).flatten()
    scalers[col] = scaler

print("ë³€ìˆ˜ë³„ ìŠ¤ì¼€ì¼ë§ ë²”ìœ„:")
for col in feature_cols:
    orig_min, orig_max = df[col].min(), df[col].max()
    ext_min, ext_max = scalers[col].extended_min_[0], scalers[col].extended_max_[0]
    print(f"{col}: [{orig_min:.3f}, {orig_max:.3f}] â†’ [{ext_min:.3f}, {ext_max:.3f}]")
```

## 1.3 ì‹œí€€ìŠ¤ ë°ì´í„° êµ¬ì¡°í™”
```python
# ì‹¤í—˜ë³„ ì‹œí€€ìŠ¤ ë¶„ë¦¬ (teacher forcingìš©) - ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì‚¬ìš©
sequences = []
for exp in experiments:
    exp_data = df_scaled[df_scaled['exp'] == exp].sort_values('t')
    sequences.append(exp_data[feature_cols].values)

print(f"ì‹œí€€ìŠ¤ ê°œìˆ˜: {len(sequences)}")
print(f"í‰ê·  ì‹œí€€ìŠ¤ ê¸¸ì´: {np.mean([len(seq) for seq in sequences])}")
print(f"ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ë²”ìœ„ í™•ì¸:")
for i, col in enumerate(feature_cols):
    seq_values = [seq[:, i] for seq in sequences]
    all_values = np.concatenate(seq_values)
    print(f"{col}: [{all_values.min():.3f}, {all_values.max():.3f}]")
```

## 1.4 ë³€í™”ëŸ‰ ê³„ì‚° (Delta íƒ€ê²Ÿ)
```python
# dCALA, dCBLA, dCAK, dCBK, dVA, dVB ê³„ì‚° - ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ ê³„ì‚°
delta_targets = ['dCALA', 'dCBLA', 'dCAK', 'dCBK', 'dVA', 'dVB']
target_sequences = []
target_cols = ['CALA', 'CBLA', 'CAK', 'CBK', 'VA', 'VB']

for exp in experiments:
    exp_data = df_scaled[df_scaled['exp'] == exp].sort_values('t')
    
    # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ ë³€í™”ëŸ‰ ê³„ì‚°
    deltas = np.diff(exp_data[target_cols].values, axis=0)
    target_sequences.append(deltas)

print(f"íƒ€ê²Ÿ ì‹œí€€ìŠ¤ í˜•íƒœ: {[seq.shape for seq in target_sequences[:3]]}")

# ë³€í™”ëŸ‰ ë²”ìœ„ í™•ì¸
all_deltas = np.concatenate(target_sequences, axis=0)
print("ë³€í™”ëŸ‰ í†µê³„:")
for i, col in enumerate(target_cols):
    delta_values = all_deltas[:, i]
    print(f"d{col}: mean={delta_values.mean():.4f}, std={delta_values.std():.4f}, "
          f"range=[{delta_values.min():.4f}, {delta_values.max():.4f}]")
```

---

# Phase 2: ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ (3-4ì¼)

## 2.1 LSTM ì¸ì½”ë” ì„¤ê³„
```python
# LSTM í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì •ì˜
lstm_hidden_sizes = [32, 64, 128, 256]
lstm_num_layers = [1, 2, 3]
lstm_dropout = [0.1, 0.2, 0.3]

class LSTMEncoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):
        super(LSTMEncoder, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        
    def forward(self, x):
        output, (hidden, cell) = self.lstm(x)
        return output, hidden, cell
```

## 2.2 MLP ë””ì½”ë” ì„¤ê³„
```python
# MLP í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„
mlp_hidden_sizes = [64, 128, 256, 512]
mlp_num_layers = [2, 3, 4]
mlp_dropout = [0.1, 0.2, 0.3]

class MLPDecoder(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.1):
        super(MLPDecoder, self).__init__()
        layers = []
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´
        layers.append(nn.Linear(input_size, hidden_size))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout))
        
        # ì¤‘ê°„ ë ˆì´ì–´ë“¤
        for _ in range(num_layers - 2):
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
        
        # ì¶œë ¥ ë ˆì´ì–´
        layers.append(nn.Linear(hidden_size, output_size))
        
        self.mlp = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.mlp(x)
```

## 2.3 Mass Balance Layer êµ¬í˜„
```python
# ë¬¼ë¦¬ ë²•ì¹™ ê¸°ë°˜ ê³„ì‚° ë ˆì´ì–´
class MassBalanceLayer(nn.Module):
    def __init__(self):
        super(MassBalanceLayer, self).__init__()
    
    def forward(self, deltas, current_state):
        """
        deltas: [dCALA, dCBLA, dCAK, dCBK, dVA, dVB]
        current_state: [CALA, CBLA, CAK, CBK, VA, VB]
        """
        # ë†ë„ ë³€í™” ì ìš©
        next_cala = current_state[:, 0] + deltas[:, 0]
        next_cbla = current_state[:, 1] + deltas[:, 1]
        next_cak = current_state[:, 2] + deltas[:, 2]
        next_cbk = current_state[:, 3] + deltas[:, 3]
        
        # ë¶€í”¼ ë³€í™” ì ìš©
        next_va = current_state[:, 4] + deltas[:, 4]
        next_vb = current_state[:, 5] + deltas[:, 5]
        
        # ë¬¼ë¦¬ì  ì œì•½ ì¡°ê±´ ì ìš© (ë†ë„ >= 0, ë¶€í”¼ >= 0)
        next_cala = torch.clamp(next_cala, min=0)
        next_cbla = torch.clamp(next_cbla, min=0)
        next_cak = torch.clamp(next_cak, min=0)
        next_cbk = torch.clamp(next_cbk, min=0)
        next_va = torch.clamp(next_va, min=0)
        next_vb = torch.clamp(next_vb, min=0)
        
        return torch.stack([next_cala, next_cbla, next_cak, next_cbk, next_va, next_vb], dim=1)
```

## 2.4 í†µí•© ëª¨ë¸ ì„¤ê³„
```python
class BMEDModel(nn.Module):
    def __init__(self, lstm_params, mlp_params):
        super(BMEDModel, self).__init__()
        self.lstm_encoder = LSTMEncoder(**lstm_params)
        self.mlp_decoder = MLPDecoder(**mlp_params)
        self.mass_balance = MassBalanceLayer()
        
    def forward(self, x, current_state, teacher_forcing_ratio=0.8):
        # LSTMìœ¼ë¡œ ì‹œí€€ìŠ¤ ì¸ì½”ë”©
        lstm_output, _, _ = self.lstm_encoder(x)
        
        # MLPë¡œ ë³€í™”ëŸ‰ ì˜ˆì¸¡
        deltas = self.mlp_decoder(lstm_output[:, -1, :])  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì‚¬ìš©
        
        # Mass Balanceë¡œ ë‹¤ìŒ ìƒíƒœ ê³„ì‚°
        next_state = self.mass_balance(deltas, current_state)
        
        return deltas, next_state
```

## 2.5 Teacher Forcing ë©”ì»¤ë‹ˆì¦˜
```python
# í›ˆë ¨ ì‹œ ì‹¤ì œ ê°’ ì‚¬ìš©, ì¶”ë¡  ì‹œ ì˜ˆì¸¡ ê°’ ì‚¬ìš©
def teacher_forcing_step(model, input_seq, target_seq, current_states, teacher_forcing_ratio=0.8):
    batch_size, seq_len, input_size = input_seq.shape
    outputs = []
    
    for t in range(seq_len - 1):
        # í˜„ì¬ê¹Œì§€ì˜ ì‹œí€€ìŠ¤ë¡œ ë‹¤ìŒ ìƒíƒœ ì˜ˆì¸¡
        current_input = input_seq[:, :t+1, :]
        current_state = current_states[:, t, :]
        
        deltas, next_state = model(current_input, current_state)
        outputs.append(deltas)
        
        # Teacher forcing ì ìš©
        if np.random.random() < teacher_forcing_ratio:
            # ì‹¤ì œ ê°’ ì‚¬ìš©
            current_states[:, t+1, :] = target_seq[:, t, :]
        else:
            # ì˜ˆì¸¡ ê°’ ì‚¬ìš©
            current_states[:, t+1, :] = next_state.detach()
    
    return torch.stack(outputs, dim=1)
```

---

# Phase 3: Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (4-5ì¼)

## 3.1 Objective Function ì„¤ê³„
```python
def objective(trial):
    # LSTM í•˜ì´í¼íŒŒë¼ë¯¸í„°
    lstm_hidden = trial.suggest_categorical('lstm_hidden', [32, 64, 128, 256])
    lstm_layers = trial.suggest_int('lstm_layers', 1, 3)
    lstm_dropout = trial.suggest_float('lstm_dropout', 0.1, 0.3)
    
    # MLP í•˜ì´í¼íŒŒë¼ë¯¸í„°
    mlp_hidden = trial.suggest_categorical('mlp_hidden', [64, 128, 256, 512])
    mlp_layers = trial.suggest_int('mlp_layers', 2, 4)
    mlp_dropout = trial.suggest_float('mlp_dropout', 0.1, 0.3)
    
    # í•™ìŠµë¥  & ë°°ì¹˜ ì‚¬ì´ì¦ˆ
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
    lstm_params = {
        'input_size': len(feature_cols),
        'hidden_size': lstm_hidden,
        'num_layers': lstm_layers,
        'dropout': lstm_dropout
    }
    
    mlp_params = {
        'input_size': lstm_hidden,
        'hidden_size': mlp_hidden,
        'output_size': 6,  # dCALA, dCBLA, dCAK, dCBK, dVA, dVB
        'num_layers': mlp_layers,
        'dropout': mlp_dropout
    }
    
    # 5-Fold Cross Validation
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(sequences)):
        # í´ë“œë³„ ë°ì´í„° ë¶„í• 
        train_sequences = [sequences[i] for i in train_idx]
        val_sequences = [sequences[i] for i in val_idx]
        train_targets = [target_sequences[i] for i in train_idx]
        val_targets = [target_sequences[i] for i in val_idx]
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        model = BMEDModel(lstm_params, mlp_params)
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        criterion = nn.MSELoss()
        
        # í›ˆë ¨
        model.train()
        for epoch in range(50):  # ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ 50 ì—í¬í¬ë¡œ ì œí•œ
            train_loss = 0
            for i, (seq, target) in enumerate(zip(train_sequences, train_targets)):
                if len(seq) < 2:  # ìµœì†Œ 2ê°œ íƒ€ì„ìŠ¤í… í•„ìš”
                    continue
                    
                optimizer.zero_grad()
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                input_tensor = torch.FloatTensor(seq[:-1]).unsqueeze(0)
                target_tensor = torch.FloatTensor(target).unsqueeze(0)
                current_states = torch.FloatTensor(seq[1:, [7, 8, 10, 11, 5, 6]]).unsqueeze(0)  # CALA, CBLA, CAK, CBK, VA, VB
                
                # ì˜ˆì¸¡
                predicted_deltas = teacher_forcing_step(model, input_tensor, target_tensor, current_states)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = criterion(predicted_deltas, target_tensor)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        val_loss = 0
        val_count = 0
        
        with torch.no_grad():
            for i, (seq, target) in enumerate(zip(val_sequences, val_targets)):
                if len(seq) < 2:
                    continue
                    
                input_tensor = torch.FloatTensor(seq[:-1]).unsqueeze(0)
                target_tensor = torch.FloatTensor(target).unsqueeze(0)
                current_states = torch.FloatTensor(seq[1:, [7, 8, 10, 11, 5, 6]]).unsqueeze(0)
                
                predicted_deltas = teacher_forcing_step(model, input_tensor, target_tensor, current_states, teacher_forcing_ratio=0.0)
                loss = criterion(predicted_deltas, target_tensor)
                
                val_loss += loss.item()
                val_count += 1
        
        if val_count > 0:
            cv_scores.append(val_loss / val_count)
    
    return np.mean(cv_scores) if cv_scores else float('inf')
```

## 3.2 ìµœì í™” ì‹¤í–‰
```python
# Optuna study ìƒì„± ë° ì‹¤í–‰
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=200)

# ìµœì  íŒŒë¼ë¯¸í„° ì¶œë ¥
best_params = study.best_params
best_score = study.best_value

print("ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
for key, value in best_params.items():
    print(f"  {key}: {value}")
print(f"ìµœì  CV ì ìˆ˜: {best_score}")
```

---

# Phase 4: ìµœì¢… ëª¨ë¸ í›ˆë ¨ & í‰ê°€ (3-4ì¼)

## 4.1 ìµœì  ëª¨ë¸ í›ˆë ¨
```python
# ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ êµ¬ì„±
lstm_params = {
    'input_size': len(feature_cols),
    'hidden_size': best_params['lstm_hidden'],
    'num_layers': best_params['lstm_layers'],
    'dropout': best_params['lstm_dropout']
}

mlp_params = {
    'input_size': best_params['lstm_hidden'],
    'hidden_size': best_params['mlp_hidden'],
    'output_size': 6,
    'num_layers': best_params['mlp_layers'],
    'dropout': best_params['mlp_dropout']
}

# ìµœì¢… ëª¨ë¸ í›ˆë ¨
final_model = BMEDModel(lstm_params, mlp_params)
optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['lr'])
criterion = nn.MSELoss()

# ì „ì²´ ë°ì´í„°ë¡œ í›ˆë ¨
epochs = 200
train_losses = []

for epoch in range(epochs):
    final_model.train()
    epoch_loss = 0
    batch_count = 0
    
    for i, (seq, target) in enumerate(zip(sequences, target_sequences)):
        if len(seq) < 2:
            continue
            
        optimizer.zero_grad()
        
        input_tensor = torch.FloatTensor(seq[:-1]).unsqueeze(0)
        target_tensor = torch.FloatTensor(target).unsqueeze(0)
        current_states = torch.FloatTensor(seq[1:, [7, 8, 10, 11, 5, 6]]).unsqueeze(0)
        
        predicted_deltas = teacher_forcing_step(final_model, input_tensor, target_tensor, current_states)
        loss = criterion(predicted_deltas, target_tensor)
        
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        batch_count += 1
    
    if batch_count > 0:
        avg_loss = epoch_loss / batch_count
        train_losses.append(avg_loss)
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}")
```

## 4.2 ê²°ê³¼ ì‹œê°í™”
```python
import matplotlib.pyplot as plt

# í›ˆë ¨ ì†ì‹¤ ê·¸ë˜í”„
plt.figure(figsize=(10, 6))
plt.plot(train_losses)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.grid(True)
plt.show()

# Optuna ê²°ê³¼ ì‹œê°í™”
optuna.visualization.plot_param_importances(study).show()
optuna.visualization.plot_optimization_history(study).show()
```

## 4.3 ë¬¼ë¦¬ ë²•ì¹™ ê²€ì¦
```python
# Mass balance ê²€ì¦
def validate_mass_balance(model, sequences, tolerance=1e-3):
    model.eval()
    violations = []
    
    with torch.no_grad():
        for seq in sequences:
            if len(seq) < 2:
                continue
                
            input_tensor = torch.FloatTensor(seq[:-1]).unsqueeze(0)
            current_states = torch.FloatTensor(seq[1:, [7, 8, 10, 11, 5, 6]]).unsqueeze(0)
            
            deltas, next_states = model(input_tensor, current_states[:, 0, :].unsqueeze(1))
            
            # ë¬¼ë¦¬ì  ì œì•½ ìœ„ë°˜ í™•ì¸
            negative_concentrations = (next_states < 0).sum().item()
            if negative_concentrations > 0:
                violations.append(negative_concentrations)
    
    print(f"ë¬¼ë¦¬ ë²•ì¹™ ìœ„ë°˜ ì‚¬ë¡€: {len(violations)}/{len(sequences)}")
    return violations

violations = validate_mass_balance(final_model, sequences)
```

---

# Phase 5: ëª¨ë¸ ì €ì¥ & ë°°í¬ ì¤€ë¹„ (1-2ì¼)

## 5.1 ëª¨ë¸ ì €ì¥
```python
# ëª¨ë¸ê³¼ ê´€ë ¨ ì •ë³´ ì €ì¥
torch.save({
    'model_state_dict': final_model.state_dict(),
    'lstm_params': lstm_params,
    'mlp_params': mlp_params,
    'best_params': best_params,
    'scalers': scalers,  # ë³€ìˆ˜ë³„ ìŠ¤ì¼€ì¼ëŸ¬ ë”•ì…”ë„ˆë¦¬
    'extension_ratios': extension_ratios,  # í™•ì¥ ë¹„ìœ¨ ì •ë³´
    'cv_score': best_score,
    'train_losses': train_losses,
    'feature_cols': feature_cols,
    'target_cols': target_cols
}, 'bmed_lstm_mlp_optimized.pth')

print("ëª¨ë¸ì´ 'bmed_lstm_mlp_optimized.pth'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

## 5.2 ì¶”ë¡  íŒŒì´í”„ë¼ì¸
```python
def predict_next_state(model, scalers, current_sequence, feature_cols, target_cols):
    """
    ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ í•¨ìˆ˜ (20% í™•ì¥ MinMaxScaler ì‚¬ìš©)
    """
    model.eval()
    
    # ë³€ìˆ˜ë³„ ê°œë³„ ì „ì²˜ë¦¬
    normalized_data = {}
    for col in feature_cols:
        scaler = scalers[col]
        normalized_data[col] = scaler.transform(current_sequence[[col]]).flatten()
    
    # ì •ê·œí™”ëœ ì‹œí€€ìŠ¤ êµ¬ì„±
    normalized_seq = np.column_stack([normalized_data[col] for col in feature_cols])
    input_tensor = torch.FloatTensor(normalized_seq).unsqueeze(0)
    
    # í˜„ì¬ ìƒíƒœ ì¶”ì¶œ (CALA, CBLA, CAK, CBK, VA, VB)
    target_indices = [feature_cols.index(col) for col in target_cols]
    current_state = torch.FloatTensor(normalized_seq[-1, target_indices]).unsqueeze(0)
    
    with torch.no_grad():
        deltas, next_state = model(input_tensor, current_state)
    
    # ì—­ì •ê·œí™” (ë³€ìˆ˜ë³„ ê°œë³„ ì²˜ë¦¬)
    next_state_denorm = np.zeros_like(next_state.numpy())
    for i, col in enumerate(target_cols):
        scaler = scalers[col]
        next_state_denorm[:, i] = scaler.inverse_transform(
            next_state[:, i:i+1].numpy()
        ).flatten()
    
    return next_state_denorm

def load_model_and_predict(model_path, new_sequence_data):
    """
    ì €ì¥ëœ ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡ í•¨ìˆ˜
    """
    # ëª¨ë¸ ë¡œë”©
    checkpoint = torch.load(model_path)
    
    # ëª¨ë¸ ì¬êµ¬ì„±
    lstm_params = checkpoint['lstm_params']
    mlp_params = checkpoint['mlp_params']
    model = BMEDModel(lstm_params, mlp_params)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë° ì„¤ì • ë¡œë”©
    scalers = checkpoint['scalers']
    feature_cols = checkpoint['feature_cols']
    target_cols = checkpoint['target_cols']
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    prediction = predict_next_state(model, scalers, new_sequence_data, 
                                  feature_cols, target_cols)
    
    return prediction

# ì‚¬ìš© ì˜ˆì‹œ
# ìƒˆë¡œìš´ ì‹¤í—˜ ë°ì´í„°ë¡œ ì˜ˆì¸¡
# new_prediction = load_model_and_predict('bmed_lstm_mlp_optimized.pth', new_experiment_data)
# print(f"ì˜ˆì¸¡ëœ ë‹¤ìŒ ìƒíƒœ: {new_prediction}")

# ì™¸ì‚½ í…ŒìŠ¤íŠ¸ (20% ë²”ìœ„ ë‚´)
# ì˜ˆë¥¼ ë“¤ì–´, í›ˆë ¨ ë°ì´í„° ì „ì•• ë²”ìœ„ê°€ 10-20Vì˜€ë‹¤ë©´
# ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ 8-24V ë²”ìœ„ê¹Œì§€ ì•ˆì •ì ìœ¼ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥
```

---

# ğŸ”§ êµ¬í˜„ ì „ëµ & íŒ

## ê°œë°œ ë°©ì‹
- **ìˆœì°¨ì  ê°œë°œ**: í•¨ìˆ˜/í´ë˜ìŠ¤ ì—†ì´ ì…€ë³„ë¡œ ì½”ë“œ ì‘ì„± â†’ ë‚˜ì¤‘ì— ë¦¬íŒ©í† ë§
- **ì ì§„ì  í…ŒìŠ¤íŠ¸**: ê° Phaseë§ˆë‹¤ ì¤‘ê°„ ê²°ê³¼ í™•ì¸
- **ë¬¼ë¦¬ ë²•ì¹™ ê²€ì¦**: Mass balance ì œì•½ ì¡°ê±´ ì§€ì†ì  ëª¨ë‹ˆí„°ë§

## ì„±ëŠ¥ ìµœì í™”
- **GPU í™œìš©**: CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìë™ í™œìš©
- **ë°°ì¹˜ ì²˜ë¦¬**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°
- **Early Stopping**: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ì¡°ê¸° ì¢…ë£Œ

## ì˜ˆìƒ ê²°ê³¼
- **ëª¨ë¸ ì •í™•ë„**: MSE < 0.01 (ì •ê·œí™”ëœ ë°ì´í„° ê¸°ì¤€)
- **ë¬¼ë¦¬ ë²•ì¹™ ì¤€ìˆ˜**: >95% ë¬¼ë¦¬ì  ì œì•½ ì¡°ê±´ ë§Œì¡±
- **ìµœì í™” íš¨ìœ¨**: 200íšŒ ì‹œí–‰ìœ¼ë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬

---

# ğŸ“š í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
pip install torch pandas scikit-learn optuna matplotlib numpy
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 2-3ì£¼  
**í•µì‹¬ ê²€ì¦ ì§€í‘œ**: MSE, ë¬¼ë¦¬ ë²•ì¹™ ì¤€ìˆ˜ìœ¨, Cross-validation ì ìˆ˜