{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe9d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optuna\n",
    "np.random.seed(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "306f65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set the device to GPU if available, otherwise use CPU.\n",
    "\n",
    "    Returns:\n",
    "        device (torch.device): The device to use for training.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'Using device: {device}')\n",
    "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    else:\n",
    "        print(f'Using device: {device}')\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c611c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(name):\n",
    "    \"\"\"\n",
    "    Load the data from the csv file and normalize the data.\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of the csv file.\n",
    "\n",
    "    Returns:\n",
    "        ndf (pd.DataFrame): The normalized data.\n",
    "        exp_num_list (list): List of experiment numbers in order.\n",
    "    \"\"\"\n",
    "    # raw data\n",
    "    df = pd.read_csv(name) \n",
    "\n",
    "    # normalized data\n",
    "    ndf = pd.DataFrame() \n",
    "\n",
    "    # the range of min-max normalization for each feature\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CBLA': {'min':0, 'max': df['CBLA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CAK': {'min':0, 'max': df['CAK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "    \n",
    "    # add experiment number and time\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t'] \n",
    "\n",
    "    # min-max normalization\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']: # min-max normalization\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "\n",
    "    # Get the unique experiment numbers in order\n",
    "    exp_num_list = sorted(ndf['exp'].unique())\n",
    "\n",
    "    return ndf, exp_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bdecd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_const(ndf):\n",
    "    \"\"\"\n",
    "    Set the data sequences.\n",
    "\n",
    "    Args:\n",
    "        ndf (pd.DataFrame): The normalized data.\n",
    "\n",
    "    Returns:\n",
    "        sequences (list): The sequences of the data.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']\n",
    "    \n",
    "    # get the sequences of the data for each experiment\n",
    "    for exp in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp].sort_values(by='t')\n",
    "        sequences.append(exp_data[feature_cols].values)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43e19a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequences(sequences):\n",
    "    \"\"\"\n",
    "    Pad the sequences.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): The sequences of the data.\n",
    "\n",
    "    Returns:\n",
    "        padded_sequences (torch.Tensor): The padded sequences.\n",
    "    \"\"\"\n",
    "    max_seq_len = max([len(seq) for seq in sequences])\n",
    "    seq_len = [len(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return padded_sequences, seq_len, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84d4489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(pad_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Generate the dataset.\n",
    "\n",
    "    Args:\n",
    "        pad_seq (torch.Tensor): The padded sequences.\n",
    "        seq_len (list): The length of the sequences.\n",
    "\n",
    "    Returns:\n",
    "        dataset (torch.utils.data.Dataset): The dataset.\n",
    "    \"\"\"\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86eb9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaders(dataset, exp_num_list, batch_size=4):\n",
    "    \"\"\"\n",
    "    Split the dataset into train/val/test with 8:1:1 ratio\n",
    "    \n",
    "    Args:\n",
    "        dataset: TensorDataset\n",
    "        exp_num_list: list of experiment numbers\n",
    "        batch_size: batch size\n",
    "        random_state: random seed\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # required train experiment numbers\n",
    "    required_train_exps = [1, 3, 5, 6, 11, 15, 17, 19, 20, 40, 41, 42]\n",
    "    \n",
    "    # all experiment numbers\n",
    "    all_exps = exp_num_list\n",
    "    total_exps = len(all_exps)\n",
    "    \n",
    "    # batch_size\n",
    "    batch_size = math.ceil(len(dataset)/10)\n",
    "\n",
    "    # 8:1:1 ratio\n",
    "    train_count = int(total_exps * 0.8)\n",
    "    val_count = math.ceil(total_exps * 0.1)\n",
    "    \n",
    "    # remaining experiments\n",
    "    remaining_exps = [exp for exp in all_exps if exp not in required_train_exps]\n",
    "    \n",
    "    # number of experiments to add to train\n",
    "    additional_train_needed = train_count - len(required_train_exps)\n",
    "    \n",
    "    if additional_train_needed < 0:\n",
    "        raise ValueError(\"The number of required train experiments is greater than the total train set. Please adjust required_train_exps.\")\n",
    "    \n",
    "    # shuffle remaining experiments\n",
    "    np.random.shuffle(remaining_exps)\n",
    "    \n",
    "    # split remaining experiments into train, val, test\n",
    "    train_exps = required_train_exps + remaining_exps[:additional_train_needed]\n",
    "    val_exps = remaining_exps[additional_train_needed:additional_train_needed + val_count]\n",
    "    test_exps = remaining_exps[additional_train_needed + val_count:]\n",
    "    \n",
    "    print(f\"Actual split:\")\n",
    "    print(f\"  Train: {sorted(train_exps)} ({len(train_exps)} experiments)\")\n",
    "    print(f\"  Val: {sorted(val_exps)} ({len(val_exps)} experiments)\")  \n",
    "    print(f\"  Test: {sorted(test_exps)} ({len(test_exps)} experiments)\")\n",
    "    \n",
    "    # find indices of each experiment (exp_num_list and dataset have the same order)\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for idx, exp in enumerate(all_exps):\n",
    "        if exp in train_exps:\n",
    "            train_indices.append(idx)\n",
    "        elif exp in val_exps:\n",
    "            val_indices.append(idx)\n",
    "        elif exp in test_exps:\n",
    "            test_indices.append(idx)\n",
    "    \n",
    "    # split dataset into train, val, test\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "    \n",
    "    # create DataLoader\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nCompleted DataLoader creation:\")\n",
    "    print(f\"  Train: {len(train_subset) if train_subset else 0} sequences\")\n",
    "    print(f\"  Val: {len(val_subset) if val_subset else 0} sequences\")\n",
    "    print(f\"  Test: {len(test_subset) if test_subset else 0} sequences\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bda98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialStateExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    The module based on LSTM to extract hidden dynamics from the sequential pattern of BMED.\n",
    "    The hidden state of each step accumulates the information of all previous steps.\n",
    "\n",
    "    Args:\n",
    "        input_nodes (int): The number of input nodes.\n",
    "        hidden_nodes (int): The number of hidden nodes.\n",
    "        num_layers (int): The number of layers.\n",
    "        dropout (float): The dropout rate.\n",
    "    \n",
    "    Output:\n",
    "        hidden_states: [batch_size, seq_len, hidden_nodes] - hidden state of each step\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nodes, hidden_nodes, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_nodes, hidden_nodes, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_nodes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"\n",
    "        Extract the hidden state of each step from the sequential pattern of BMED.\n",
    "\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, input_nodes] - state sequence of BMED system\n",
    "            seq_len [batch_size] - length of each sequence\n",
    "\n",
    "        Returns:\n",
    "            hidden_states: [batch_size, seq_len, hidden_nodes] - hidden state of each step\n",
    "        \"\"\"\n",
    "        # check the input shape\n",
    "        if x.size(0) != seq_len.size(0):\n",
    "            raise ValueError(f\"Batch size mismatch: input {x.size(0)} vs seq_len {seq_len.size(0)}\")\n",
    "        \n",
    "        # Move the seq_len to CPU and transfer to integer\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "\n",
    "        # check the length of sequence\n",
    "        if (seq_len_cpu <= 0).any():\n",
    "            invalid_lengths = seq_len_cpu[seq_len_cpu <= 0]\n",
    "            raise ValueError(f'Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths mut be positive')\n",
    "        \n",
    "        # pack the padded sequence\n",
    "        packed_input = pack_padded_sequence(x, seq_len_cpu, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "\n",
    "        # re-pad the sequence\n",
    "        lstm_out, output_lengths = pad_packed_sequence(packed_output, batch_first=True, total_length=x.size(1))\n",
    "\n",
    "        # Normalization and dropout\n",
    "        normed_output = self.layer_norm(lstm_out)\n",
    "        return self.dropout(normed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29c0d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The module based on MLP to decode the hidden state to the physical change.\n",
    "\n",
    "    Args:\n",
    "        hidden_nodes (int): The number of hidden nodes.\n",
    "        output_nodes (int): The number of output nodes.\n",
    "        num_layers (int): The number of layers.\n",
    "        num_nodes (int): The number of nodes in the hidden layers.\n",
    "        dropout (float): The dropout rate.\n",
    "    \n",
    "    Output:\n",
    "        physical_changes: [batch_size, seq_len, output_nodes] - [dVA, dVB, dNALA, dNAK, dNBK, nI]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_nodes, output_nodes, num_layers=2, num_nodes=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_nodes is None:\n",
    "            num_nodes = hidden_nodes\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input layer: hidden_nodes -> num_nodes\n",
    "        self.layers.append(nn.Linear(hidden_nodes, num_nodes))\n",
    "        self.layers.append(nn.LayerNorm(num_nodes))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # hidden layers: num_nodes -> num_nodes\n",
    "        for i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_nodes, num_nodes))\n",
    "            self.layers.append(nn.LayerNorm(num_nodes))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # output layer: num_nodes -> output_nodes\n",
    "        self.layers.append(nn.Linear(num_nodes, output_nodes))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Decode the hidden state to the physical change.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_nodes] - hidden state of each step\n",
    "\n",
    "        Returns:\n",
    "            physical_changes: [batch_size, seq_len, output_nodes] - [dVA, dVB, dNALA, dNAK, dNBK, nI]\n",
    "        \"\"\"\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24efe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The module based on MLP to apply the physical constraints to the physical changes.\n",
    "\n",
    "    Output:\n",
    "        new_state: [batch_size, seq_len, 12] - new state\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-1):\n",
    "        super().__init__()\n",
    "        self.eps = eps # prevent division by zero\n",
    "\n",
    "    def forward(self, physical_changes, current_state):\n",
    "        \"\"\"\n",
    "        Apply the physical constraints to the physical changes.\n",
    "\n",
    "        Args:\n",
    "            physical_changes: [batch_size, seq_len, 7] - physical changes\n",
    "            current_state: [batch_size, seq_len, 12] - current state\n",
    "\n",
    "        Returns:\n",
    "            new_state: [batch_size, seq_len, 12] - new state\n",
    "        \"\"\"\n",
    "        # check the input shape\n",
    "        if physical_changes.dim() != current_state.dim():\n",
    "            raise ValueError(f\"Dimension mismatch: physical_changes {physical_changes.shape} vs current_state {current_state.shape}\")\n",
    "        \n",
    "        if current_state.size(-1) != 12:\n",
    "            raise ValueError(f\"Expected 12 state features, got {current_state.size(-1)}\")\n",
    "        \n",
    "        if physical_changes.size(-1) != 7:\n",
    "            raise ValueError(f\"Expected 7 physical changes, got {physical_changes.size(-1)}\")\n",
    "\n",
    "        \n",
    "        # extract the current state variables (keep the dimension)\n",
    "        V = current_state[..., 0:1]     # Voltage (fixed)\n",
    "        E = current_state[..., 1:2]     # External electrolyte concentration (fixed)\n",
    "        VF = current_state[..., 2:3]    # Feed volume\n",
    "        VA = current_state[..., 3:4]    # Acid volume\n",
    "        VB = current_state[..., 4:5]    # Base volume\n",
    "        CFLA = current_state[..., 5:6]  # LA concentration in Feed tank\n",
    "        CALA = current_state[..., 6:7]  # LA concentration in Acid tank\n",
    "        CBLA = current_state[..., 7:8]  # LA concentration in Base tank\n",
    "        CFK = current_state[..., 8:9]   # K concentration in Feed tank\n",
    "        CAK = current_state[..., 9:10]  # K concentration in Acid tank\n",
    "        CBK = current_state[..., 10:11] # K concentration in Base tank\n",
    "        I = current_state[..., 11:12]   # Current\n",
    "\n",
    "        # calculate the mole of ion species\n",
    "        NFLA = CFLA * VF; NALA = CALA * VA; NBLA = CBLA * VB\n",
    "        NFK = CFK * VF; NAK = CAK * VA; NBK = CBK * VB\n",
    "\n",
    "        # calculate the physical changes\n",
    "        dVA = physical_changes[..., 0:1]    # Acid tank volume change (bidirectional)\n",
    "        dVB = physical_changes[..., 1:2]    # Base tank volume change (bidirectional)\n",
    "        dNALA = physical_changes[..., 2:3]  # LA change in Acid tank (unidirectional)\n",
    "        dNBLA = physical_changes[..., 3:4]  # LA change in Base tank (unidirectional)\n",
    "        dNAK = physical_changes[..., 4:5]   # K change in Acid tank (unidirectional)\n",
    "        dNBK = physical_changes[..., 5:6]   # K change in Base tank (unidirectional)\n",
    "        nI = physical_changes[..., 6:7]     # New current value\n",
    "        \n",
    "        # calculate the new volume\n",
    "        nVF = VF - dVA - dVB  # New Feed tank volume \n",
    "        nVA = VA + dVA        # New Acid tank volume\n",
    "        nVB = VB + dVB        # New Base tank volume\n",
    "\n",
    "        # limit the ion species changes (unidirectional flow only)\n",
    "        dNALA = torch.clamp(dNALA, min=0)\n",
    "        dNBLA = torch.clamp(dNBLA, min=0)\n",
    "        dNAK = torch.clamp(dNAK, min=0)\n",
    "        dNBK = torch.clamp(dNBK, min=0)\n",
    "\n",
    "        # calculate the new mole of ion species\n",
    "        nNFLA = NFLA - dNALA - dNBLA  # New LA mole in Feed tank\n",
    "        nNALA = NALA + dNALA         # New LA mole in Acid tank\n",
    "        nNBLA = NBLA + dNBLA         # New LA mole in Base tank\n",
    "        nNFK = NFK - dNAK - dNBK     # New K mole in Feed tank\n",
    "        nNAK = NAK + dNAK            # New K mole in Acid tank\n",
    "        nNBK = NBK + dNBK            # New K mole in Base tank\n",
    "\n",
    "        # limit the physical changes\n",
    "        nVF = torch.clamp(nVF, min=self.eps)\n",
    "        nVA = torch.clamp(nVA, min=self.eps)\n",
    "        nVB = torch.clamp(nVB, min=self.eps)\n",
    "        nNFLA = torch.clamp(nNFLA, min=0)\n",
    "        nNALA = torch.clamp(nNALA, min=0)\n",
    "        nNBLA = torch.clamp(nNBLA, min=0)\n",
    "        nNFK = torch.clamp(nNFK, min=0)\n",
    "        nNAK = torch.clamp(nNAK, min=0)\n",
    "        nNBK = torch.clamp(nNBK, min=0)\n",
    "        nI = torch.clamp(nI, min=0)\n",
    "\n",
    "        # calculate the new concentration\n",
    "        nCFLA = nNFLA / nVF  # New LA concentration in Feed tank\n",
    "        nCALA = nNALA / nVA  # New LA concentration in Acid tank\n",
    "        nCBLA = nNBLA / nVB  # New LA concentration in Base tank\n",
    "        nCFK = nNFK / nVF    # New K concentration in Feed tank\n",
    "        nCAK = nNAK / nVA    # New K concentration in Acid tank\n",
    "        nCBK = nNBK / nVB    # New K concentration in Base tank\n",
    "\n",
    "        # assemble the new state\n",
    "        new_state = torch.cat([\n",
    "            V, E,  # fixed: voltage, external electrolyte concentration\n",
    "            nVF, nVA, nVB,  # new volume\n",
    "            nCFLA, nCALA, nCBLA,  # new LA concentration\n",
    "            nCFK, nCAK, nCBK,     # new K concentration\n",
    "            nI  # new current\n",
    "        ], dim=-1)\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40224782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The autoregressive model to predict the state of BMED system.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_extractor_params, decoder_params):\n",
    "        super().__init__()\n",
    "        self.state_extractor = SequentialStateExtractor(**state_extractor_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer()\n",
    "\n",
    "    def forward(self, current_state, seq_lengths):\n",
    "        \"\"\"\n",
    "        Predict the next step from the all previous steps.\n",
    "\n",
    "        Args:\n",
    "            current_state: [batch_size, seq_len, 12] - current state\n",
    "            seq_lengths: [batch_size] - length of each sequence\n",
    "\n",
    "        Returns:\n",
    "            new_state: [batch_size, seq_len, 12] - new state\n",
    "        \"\"\"\n",
    "        # Extract the hidden state of each step using LSTM\n",
    "        hidden_states = self.state_extractor(current_state, seq_lengths)\n",
    "        # Decode the hidden state to the physical change\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        # Calculate the new state using physical constraints\n",
    "        new_state = self.physics_constraint(physical_changes, current_state)\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa444f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(pred, target, seq_len):\n",
    "    \"\"\"\n",
    "    Calculate the masked MSE loss for the autoregressive model.\n",
    "\n",
    "    Args:\n",
    "        pred: [batch_size, seq_len, 12] - predicted state\n",
    "        target: [batch_size, seq_len, 12] - target state\n",
    "        seq_len: [batch_size] - length of each sequence\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: average loss excluding the masked parts\n",
    "    \"\"\"\n",
    "    # check the input shape\n",
    "    if pred.shape != target.shape:\n",
    "        raise ValueError(f\"Shape mismatch: predictions {pred.shape} vs targets {target.shape}\")\n",
    "\n",
    "    if pred.size(0) != seq_len.size(0):\n",
    "        raise ValueError(f\"Batch size mismatch: predictions {pred.size(0)} vs sequence lengths {seq_len.size(0)}\")\n",
    "    \n",
    "    batch_size, max_len, features = pred.shape\n",
    "\n",
    "    # Move seq_len to CPU to be compatible with arange.\n",
    "    seq_len_cpu = seq_len.detach().cpu().long()\n",
    "\n",
    "    # Validation check on sequence lengths\n",
    "    if (seq_len_cpu <= 0).any():\n",
    "        invalid_lengths = seq_len_cpu[seq_len_cpu <= 0]\n",
    "        raise ValueError(f'Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths must be positive.')\n",
    "\n",
    "    # Check if any sequence length exceeds max_len\n",
    "    if (seq_len_cpu > max_len).any():\n",
    "        invalid_lengths = seq_len_cpu[seq_len_cpu > max_len]\n",
    "        raise ValueError(f'Sequence lengths exceed max_len: {invalid_lengths.tolist()} > {max_len}')\n",
    "\n",
    "    # Generate mask as long as the sequence length\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "    mask = mask.float().to(pred.device)\n",
    "\n",
    "    # Calculate the MSE of each feature\n",
    "    loss = F.mse_loss(pred, target, reduction='none')\n",
    "\n",
    "    # Apply the mask to exclude the masked parts\n",
    "    masked_loss_sum = (loss * mask.unsqueeze(-1)).sum()\n",
    "    valid_elements = mask.sum() * features\n",
    "\n",
    "    if valid_elements == 0:\n",
    "        raise ValueError('No valid elements found after masking. Check sequence lengths and data.')\n",
    "    \n",
    "    masked_loss = masked_loss_sum / valid_elements\n",
    "\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ae9fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_running_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Prepare the data for free running .\n",
    "\n",
    "    Args:\n",
    "        input_seq: [batch_size, seq_len, 12] - input sequences\n",
    "        seq_lengths: [batch_size] - length of each sequence\n",
    "\n",
    "    Returns:\n",
    "        init: [t0] initial state\n",
    "        targets: [t1, t2, ..., t_n] next states\n",
    "        target_seq_len: length of each target sequence\n",
    "    \"\"\"\n",
    "    # initial state\n",
    "    init = input_seq[:, 0, :]\n",
    "    # target states\n",
    "    targets = input_seq[:, 1:, :]\n",
    "    # length of each target sequence\n",
    "    if (seq_len - 1 < 1).any():\n",
    "        invalid_lengths = seq_len[seq_len - 1 < 1]\n",
    "        raise ValueError(f'The length of target sequence cannot be less than 1. Wrong seq_len: {invalid_lengths.tolist()}')\n",
    "    target_seq_len = seq_len - 1\n",
    "\n",
    "    return init, targets, target_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c09a5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_running_prediction(model, init, targets_shape, device, mode='eval'):\n",
    "    \"\"\"\n",
    "    Free running prediction using only initial state with different modes.\n",
    "    \n",
    "    Args:\n",
    "        model: BMEDAutoregressiveModel\n",
    "        initial_state: [batch_size, 12] - initial state\n",
    "        targets_shape: tuple - shape of targets to match (batch_size, seq_len, features)\n",
    "        device: computation device\n",
    "        mode: 'eval' (evaluation), 'train' (training), 'simulation' (pure inference)\n",
    "        \n",
    "    Returns:\n",
    "        predictions: [batch_size, targets_seq_len, 12] - predicted sequence\n",
    "    \"\"\"\n",
    "    # Set model mode\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "        context_manager = torch.enable_grad()\n",
    "    elif mode in ['eval', 'simulation']:\n",
    "        model.eval()\n",
    "        context_manager = torch.no_grad()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}. Choose from 'train', 'eval', 'simulation'\")\n",
    "    \n",
    "    batch_size = init.size(0)\n",
    "    num_steps = targets_shape[1]  # Use the actual targets sequence length\n",
    "    \n",
    "    # Initialize predictions with initial state\n",
    "    pred = [init.unsqueeze(1)]  # [batch_size, 1, 12]\n",
    "    current_state = init.unsqueeze(1)  # [batch_size, 1, 12]\n",
    "    \n",
    "    with context_manager:\n",
    "        for step in range(num_steps):\n",
    "            # Predict next state using current sequence\n",
    "            seq_len = torch.full((batch_size,), current_state.size(1), device=device)\n",
    "            next_state = model(current_state, seq_len)\n",
    "            \n",
    "            # Take the last predicted state\n",
    "            next_step = next_state[:, -1:, :]  # [batch_size, 1, 12]\n",
    "            pred.append(next_step)\n",
    "            \n",
    "            # Update current state sequence\n",
    "            current_state = torch.cat([current_state, next_step], dim=1)\n",
    "    \n",
    "    # Return all predictions except the initial state\n",
    "    return torch.cat(pred[1:], dim=1)  # [batch_size, num_steps, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bd5a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_free_running(model, train_loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model using free running approach for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: BMEDAutoregressiveModel\n",
    "        train_loader: training data loader\n",
    "        optimizer: optimizer\n",
    "        device: computation device\n",
    "        \n",
    "    Returns:\n",
    "        float: average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (input_seq, seq_len) in enumerate(train_loader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        seq_len = seq_len.to(device)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare free running data\n",
    "        init, targets, target_seq_len = free_running_data(input_seq, seq_len)\n",
    "        \n",
    "        # Free running prediction in train mode\n",
    "        pred = free_running_prediction(\n",
    "            model, init, targets.shape, device, mode='train'\n",
    "        )\n",
    "        \n",
    "        # Calculate masked loss\n",
    "        loss = masked_mse_loss(pred, targets, target_seq_len)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.000001)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate_epoch_free_running(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validate the model using free running approach for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: BMEDAutoregressiveModel\n",
    "        val_loader: validation data loader\n",
    "        device: computation device\n",
    "        \n",
    "    Returns:\n",
    "        float: average validation loss for the epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_seq, seq_len) in enumerate(val_loader):\n",
    "            input_seq = input_seq.to(device)\n",
    "            seq_len = seq_len.to(device)\n",
    "            \n",
    "            # Prepare free running data\n",
    "            init, targets, target_seq_len = free_running_data(input_seq, seq_len)\n",
    "            \n",
    "            # Free running prediction in eval mode\n",
    "            pred = free_running_prediction(\n",
    "                model, init, targets.shape, device, mode='eval'\n",
    "            )\n",
    "            \n",
    "            # Calculate masked loss\n",
    "            loss = masked_mse_loss(pred, targets, target_seq_len)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "lmpjx6tmfq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_free_running_model(model, train_loader, val_loader, optimizer, scheduler, device, \n",
    "#                              num_epochs=200, patience=20, min_epochs=10):\n",
    "def train_free_running_model(model, train_loader, val_loader, optimizer, device, \n",
    "                             num_epochs=200, patience=20, min_epochs=10):\n",
    "    \"\"\"\n",
    "    Complete training loop for free running model.\n",
    "    \n",
    "    Args:\n",
    "        model: BMEDAutoregressiveModel\n",
    "        train_loader: training data loader\n",
    "        val_loader: validation data loader\n",
    "        optimizer: optimizer\n",
    "        scheduler: learning rate scheduler\n",
    "        device: computation device\n",
    "        num_epochs: maximum number of epochs\n",
    "        patience: early stopping patience\n",
    "        min_epochs: minimum epochs before early stopping\n",
    "        \n",
    "    Returns:\n",
    "        dict: training history and best model state\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "    best_total_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Starting Free Running Training...\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Max Epochs: {num_epochs}, Patience: {patience}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = train_epoch_free_running(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = validate_epoch_free_running(model, val_loader, device)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = train_loss + val_loss\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        # if scheduler:\n",
    "        #     scheduler.step(val_loss)\n",
    "        \n",
    "        # Record history\n",
    "        train_history.append(train_loss)\n",
    "        val_history.append(val_loss)\n",
    "        \n",
    "        # Early stopping check based on total loss\n",
    "        if total_loss < best_total_loss:\n",
    "            best_total_loss = total_loss\n",
    "            best_val_loss = val_loss\n",
    "            best_train_loss = train_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch < 10:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}, Total Loss = {total_loss:.6f} | Best: Train = {best_train_loss:.6f}, Val = {best_val_loss:.6f}, Total = {best_total_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch >= min_epochs and patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Best train loss: {best_train_loss:.6f}\")\n",
    "    print(f\"Best val loss: {best_val_loss:.6f}\")\n",
    "    print(f\"Best total loss: {best_total_loss:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_history': train_history,\n",
    "        'val_history': val_history,\n",
    "        'best_train_loss': best_train_loss,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_total_loss': best_total_loss,\n",
    "        'best_model_state': best_model_state,\n",
    "        'final_epoch': epoch + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b63c56fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Dataset created with 39 experiments\n",
      "Max sequence length: 37\n",
      "Experiment numbers: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(27), np.int64(28), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35), np.int64(36), np.int64(37), np.int64(38), np.int64(40), np.int64(41), np.int64(42), np.int64(43)]\n",
      "Actual split:\n",
      "  Train: [1, 3, np.int64(4), 5, 6, np.int64(7), np.int64(8), np.int64(9), np.int64(10), 11, np.int64(13), np.int64(14), 15, 17, np.int64(18), 19, 20, np.int64(21), np.int64(22), np.int64(24), np.int64(27), np.int64(28), np.int64(30), np.int64(32), np.int64(34), np.int64(35), np.int64(37), np.int64(38), 40, 41, 42] (31 experiments)\n",
      "  Val: [np.int64(2), np.int64(16), np.int64(31), np.int64(33)] (4 experiments)\n",
      "  Test: [np.int64(12), np.int64(23), np.int64(25), np.int64(36), np.int64(43)] (5 experiments)\n",
      "\n",
      "Completed DataLoader creation:\n",
      "  Train: 30 sequences\n",
      "  Val: 4 sequences\n",
      "  Test: 5 sequences\n"
     ]
    }
   ],
   "source": [
    "# Load data and create dataloaders\n",
    "print(\"Loading and preprocessing data...\")\n",
    "ndf, exp_num_list = norm_data('BMED_DATA_AG.csv')\n",
    "sequences = seq_data_const(ndf)\n",
    "padded_seq, seq_len, max_seq_len = padded_sequences(sequences)\n",
    "dataset = gen_dataset(padded_seq, seq_len)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} experiments\")\n",
    "print(f\"Max sequence length: {max_seq_len}\")\n",
    "print(f\"Experiment numbers: {sorted(exp_num_list)}\")\n",
    "\n",
    "# Create train/val/test dataloaders with stratified split\n",
    "train_loader, val_loader, test_loader = dataloaders(dataset, exp_num_list, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12e02d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n",
      "Model initialized with 289767 parameters\n",
      "Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and training setup\n",
    "device = set_device()\n",
    "\n",
    "study = optuna.load_study(study_name=\"bmed_autoregressive_optimization\", storage=\"sqlite:///bmed_optuna_study.db\")\n",
    "best_params = study.best_params\n",
    "\n",
    "# Model parameters\n",
    "state_extractor_params = {\n",
    "    'input_nodes': 12,\n",
    "    'hidden_nodes': best_params['hidden_size'],\n",
    "    'num_layers': best_params['num_layers'],\n",
    "    'dropout': best_params['extractor_dropout']\n",
    "}\n",
    "\n",
    "decoder_params = {\n",
    "    'hidden_nodes': best_params['hidden_size'],\n",
    "    'output_nodes': 7,  # [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI]\n",
    "    'num_layers': best_params['decoder_layers'],\n",
    "    'num_nodes': best_params['decoder_nodes'],\n",
    "    'dropout': best_params['decoder_dropout']\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = BMEDAutoregressiveModel(state_extractor_params, decoder_params)\n",
    "model = model.to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c41192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Free Running Training...\n",
      "Starting Free Running Training...\n",
      "Device: cuda\n",
      "Max Epochs: 10000, Patience: 1500\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/10000: Train Loss = 11.825735, Val Loss = 0.014883, Total Loss = 11.840618 | Best: Train = 11.825735, Val = 0.014883, Total = 11.840618\n",
      "Epoch   2/10000: Train Loss = 0.076422, Val Loss = 0.012568, Total Loss = 0.088990 | Best: Train = 0.076422, Val = 0.012568, Total = 0.088990\n",
      "Epoch   3/10000: Train Loss = 0.066174, Val Loss = 0.014135, Total Loss = 0.080310 | Best: Train = 0.066174, Val = 0.014135, Total = 0.080310\n",
      "Epoch   4/10000: Train Loss = 0.039211, Val Loss = 0.010166, Total Loss = 0.049377 | Best: Train = 0.039211, Val = 0.010166, Total = 0.049377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Start free running training\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🚀 Starting Free Running Training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m training_results = \u001b[43mtrain_free_running_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#scheduler=scheduler,\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1500\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_results[\u001b[33m'\u001b[39m\u001b[33mbest_model_state\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain_free_running_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, patience, min_epochs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     train_loss = \u001b[43mtrain_epoch_free_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m     40\u001b[39m     val_loss = validate_epoch_free_running(model, val_loader, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain_epoch_free_running\u001b[39m\u001b[34m(model, train_loader, optimizer, device)\u001b[39m\n\u001b[32m     34\u001b[39m loss = masked_mse_loss(pred, targets, target_seq_len)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Gradient clipping for stability\u001b[39;00m\n\u001b[32m     40\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m0.000001\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Start free running training\n",
    "print(\"\\n🚀 Starting Free Running Training...\")\n",
    "training_results = train_free_running_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    #scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=10000,\n",
    "    patience=1500,\n",
    "    min_epochs=1500\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "if training_results['best_model_state'] is not None:\n",
    "    model.load_state_dict(training_results['best_model_state'])\n",
    "    print(\"✅ Best model loaded!\")\n",
    "else:\n",
    "    print(\"⚠️ No best model found, using current state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e2776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 Testing Free Running Simulation on Test Data...\n",
      "Test batch shape: torch.Size([4, 37, 12])\n",
      "Test sequence lengths: tensor([29, 29, 37, 25], device='cuda:0')\n",
      "Initial state shape: torch.Size([4, 12])\n",
      "Targets shape: torch.Size([4, 36, 12])\n",
      "🎯 Running free running simulation...\n",
      "Predictions shape: torch.Size([4, 36, 12])\n",
      "📊 Test Loss: 46914347204608.000000\n",
      "✅ Simulation completed!\n"
     ]
    }
   ],
   "source": [
    "# Test simulation on test data\n",
    "print(\"\\n🔬 Testing Free Running Simulation on Test Data...\")\n",
    "\n",
    "# Get first batch from test loader for simulation\n",
    "test_batch = next(iter(test_loader))\n",
    "test_input, test_seq_len = test_batch\n",
    "test_input = test_input.to(device)\n",
    "test_seq_len = test_seq_len.to(device)\n",
    "\n",
    "print(f\"Test batch shape: {test_input.shape}\")\n",
    "print(f\"Test sequence lengths: {test_seq_len}\")\n",
    "\n",
    "# Prepare test data for simulation\n",
    "initial_state, targets, target_seq_len = free_running_data(test_input, test_seq_len)\n",
    "\n",
    "print(f\"Initial state shape: {initial_state.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "\n",
    "# Run simulation\n",
    "print(\"🎯 Running free running simulation...\")\n",
    "with torch.no_grad():\n",
    "    predictions = free_running_prediction(\n",
    "        model, initial_state, targets.shape, device, mode='simulation'\n",
    "    )\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate test loss\n",
    "test_loss = masked_mse_loss(predictions, targets, target_seq_len)\n",
    "print(f\"📊 Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "print(\"✅ Simulation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kpu5yb8m6y",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 범위 확인\n",
    "print(\"=== 데이터 범위 분석 ===\")\n",
    "test_batch = next(iter(train_loader))\n",
    "test_input, test_seq_len = test_batch\n",
    "\n",
    "print(f\"Input 범위:\")\n",
    "print(f\"  Min: {test_input.min():.6f}\")\n",
    "print(f\"  Max: {test_input.max():.6f}\")\n",
    "print(f\"  Mean: {test_input.mean():.6f}\")\n",
    "print(f\"  Std: {test_input.std():.6f}\")\n",
    "\n",
    "# NaN 체크\n",
    "print(f\"  Has NaN: {torch.isnan(test_input).any()}\")\n",
    "print(f\"  Has Inf: {torch.isinf(test_input).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8wv0zrr3qnu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 모델 가중치 분석 ===\n",
      "state_extractor.lstm.weight_ih_l0:\n",
      "  Shape: torch.Size([1024, 12])\n",
      "  Min: -0.141811\n",
      "  Max: 0.124092\n",
      "  Mean: 0.000291\n",
      "  Std: 0.037918\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "state_extractor.lstm.weight_hh_l0:\n",
      "  Shape: torch.Size([1024, 256])\n",
      "  Min: -0.091946\n",
      "  Max: 0.088789\n",
      "  Mean: 0.000058\n",
      "  Std: 0.036187\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "state_extractor.lstm.bias_ih_l0:\n",
      "  Shape: torch.Size([1024])\n",
      "  Min: -0.168206\n",
      "  Max: 0.171752\n",
      "  Mean: -0.000069\n",
      "  Std: 0.042485\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "state_extractor.lstm.bias_hh_l0:\n",
      "  Shape: torch.Size([1024])\n",
      "  Min: -0.154577\n",
      "  Max: 0.133757\n",
      "  Mean: -0.000613\n",
      "  Std: 0.040472\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "state_extractor.layer_norm.weight:\n",
      "  Shape: torch.Size([256])\n",
      "  Min: 0.895137\n",
      "  Max: 1.116654\n",
      "  Mean: 1.000235\n",
      "  Std: 0.026615\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "state_extractor.layer_norm.bias:\n",
      "  Shape: torch.Size([256])\n",
      "  Min: -0.064364\n",
      "  Max: 0.076889\n",
      "  Mean: 0.000021\n",
      "  Std: 0.020561\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "physical_decoder.layers.0.weight:\n",
      "  Shape: torch.Size([48, 256])\n",
      "  Min: -0.296304\n",
      "  Max: 0.306258\n",
      "  Mean: -0.000151\n",
      "  Std: 0.049924\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "physical_decoder.layers.0.bias:\n",
      "  Shape: torch.Size([48])\n",
      "  Min: -0.075826\n",
      "  Max: 0.105171\n",
      "  Mean: 0.007851\n",
      "  Std: 0.044352\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "physical_decoder.layers.1.weight:\n",
      "  Shape: torch.Size([48])\n",
      "  Min: 0.833341\n",
      "  Max: 1.097316\n",
      "  Mean: 0.955900\n",
      "  Std: 0.050223\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "physical_decoder.layers.1.bias:\n",
      "  Shape: torch.Size([48])\n",
      "  Min: -0.095139\n",
      "  Max: 0.070440\n",
      "  Mean: -0.035598\n",
      "  Std: 0.036345\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "physical_decoder.layers.4.weight:\n",
      "  Shape: torch.Size([7, 48])\n",
      "  Min: -0.322914\n",
      "  Max: 0.264344\n",
      "  Mean: -0.024478\n",
      "  Std: 0.093588\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "physical_decoder.layers.4.bias:\n",
      "  Shape: torch.Size([7])\n",
      "  Min: -0.224431\n",
      "  Max: 0.060107\n",
      "  Mean: -0.099978\n",
      "  Std: 0.101630\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. 모델 가중치 초기화 상태 확인\n",
    "print(\"\\n=== 모델 가중치 분석 ===\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Shape: {param.shape}\")\n",
    "        print(f\"  Min: {param.data.min():.6f}\")\n",
    "        print(f\"  Max: {param.data.max():.6f}\")\n",
    "        print(f\"  Mean: {param.data.mean():.6f}\")\n",
    "        print(f\"  Std: {param.data.std():.6f}\")\n",
    "        print(f\"  Has NaN: {torch.isnan(param.data).any()}\")\n",
    "        print(f\"  Has Inf: {torch.isinf(param.data).any()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n5rm27m6l8k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward Pass 분석 ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model.eval()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     test_input = \u001b[43mtest_input\u001b[49m.to(device)\n\u001b[32m      6\u001b[39m     test_seq_len = test_seq_len.to(device)\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# 단계별 확인\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'test_input' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. Forward pass 중간 값들 체크\n",
    "print(\"=== Forward Pass 분석 ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = test_input.to(device)\n",
    "    test_seq_len = test_seq_len.to(device)\n",
    "    \n",
    "    # 단계별 확인\n",
    "    print(\"1. Input to LSTM:\")\n",
    "    print(f\"   Min: {test_input.min():.6f}, Max: {test_input.max():.6f}\")\n",
    "    \n",
    "    # LSTM output\n",
    "    hidden_states = model.state_extractor(test_input, test_seq_len)\n",
    "    print(\"2. LSTM output (hidden_states):\")\n",
    "    print(f\"   Min: {hidden_states.min():.6f}, Max: {hidden_states.max():.6f}\")\n",
    "    print(f\"   Mean: {hidden_states.mean():.6f}, Std: {hidden_states.std():.6f}\")\n",
    "    print(f\"   Has NaN: {torch.isnan(hidden_states).any()}\")\n",
    "    print(f\"   Has Inf: {torch.isinf(hidden_states).any()}\")\n",
    "    \n",
    "    # Physical decoder output\n",
    "    physical_changes = model.physical_decoder(hidden_states)\n",
    "    print(\"3. Physical decoder output:\")\n",
    "    print(f\"   Min: {physical_changes.min():.6f}, Max: {physical_changes.max():.6f}\")\n",
    "    print(f\"   Mean: {physical_changes.mean():.6f}, Std: {physical_changes.std():.6f}\")\n",
    "    print(f\"   Has NaN: {torch.isnan(physical_changes).any()}\")\n",
    "    print(f\"   Has Inf: {torch.isinf(physical_changes).any()}\")\n",
    "    \n",
    "    # Final output\n",
    "    new_state = model.physics_constraint(physical_changes, test_input)\n",
    "    print(\"4. Final output (new_state):\")\n",
    "    print(f\"   Min: {new_state.min():.6f}, Max: {new_state.max():.6f}\")\n",
    "    print(f\"   Mean: {new_state.mean():.6f}, Std: {new_state.std():.6f}\")\n",
    "    print(f\"   Has NaN: {torch.isnan(new_state).any()}\")\n",
    "    print(f\"   Has Inf: {torch.isinf(new_state).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n5kfrvde8l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward Pass 분석 ===\n",
      "1. Input to LSTM:\n",
      "   Min: -1.000000, Max: 0.809285\n",
      "2. LSTM output (hidden_states):\n",
      "   Min: -4.018674, Max: 2.729944\n",
      "   Mean: -0.000560, Std: 0.720518\n",
      "   Has NaN: False\n",
      "   Has Inf: False\n",
      "3. Physical decoder output:\n",
      "   Min: -3.358232, Max: 0.667063\n",
      "   Mean: -0.941814, Std: 1.013811\n",
      "   Has NaN: False\n",
      "   Has Inf: False\n",
      "4. Final output (new_state):\n",
      "   Min: -1.000000, Max: 100000000.000000\n",
      "   Mean: 16876116.000000, Std: 36757428.000000\n",
      "   Has NaN: False\n",
      "   Has Inf: False\n"
     ]
    }
   ],
   "source": [
    "# 다시 데이터 가져오기\n",
    "test_batch = next(iter(train_loader))\n",
    "test_input, test_seq_len = test_batch\n",
    "\n",
    "print(\"=== Forward Pass 분석 ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = test_input.to(device)\n",
    "    test_seq_len = test_seq_len.to(device)\n",
    "    \n",
    "    # 단계별 확인\n",
    "    print(\"1. Input to LSTM:\")\n",
    "    print(f\"   Min: {test_input.min():.6f}, Max: {test_input.max():.6f}\")\n",
    "    \n",
    "    # LSTM output\n",
    "    hidden_states = model.state_extractor(test_input, test_seq_len)\n",
    "    print(\"2. LSTM output (hidden_states):\")\n",
    "    print(f\"   Min: {hidden_states.min():.6f}, Max: {hidden_states.max():.6f}\")\n",
    "    print(f\"   Mean: {hidden_states.mean():.6f}, Std: {hidden_states.std():.6f}\")\n",
    "    print(f\"   Has NaN: {torch.isnan(hidden_states).any()}\")\n",
    "    print(f\"   Has Inf: {torch.isinf(hidden_states).any()}\")\n",
    "    \n",
    "    # Physical decoder output\n",
    "    physical_changes = model.physical_decoder(hidden_states)\n",
    "    print(\"3. Physical decoder output:\")\n",
    "    print(f\"   Min: {physical_changes.min():.6f}, Max: {physical_changes.max():.6f}\")\n",
    "    print(f\"   Mean: {physical_changes.mean():.6f}, Std: {physical_changes.std():.6f}\")\n",
    "    print(f\"   Has NaN: {torch.isnan(physical_changes).any()}\")\n",
    "    print(f\"   Has Inf: {torch.isinf(physical_changes).any()}\")\n",
    "    \n",
    "    # Final output\n",
    "    new_state = model.physics_constraint(physical_changes, test_input)\n",
    "    print(\"4. Final output (new_state):\")\n",
    "    print(f\"   Min: {new_state.min():.6f}, Max: {new_state.max():.6f}\")\n",
    "    print(f\"   Mean: {new_state.mean():.6f}, Std: {new_state.std():.6f}\")\n",
    "    print(f\"   Has NaN: {torch.isnan(new_state).any()}\")\n",
    "    print(f\"   Has Inf: {torch.isinf(new_state).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "slgc159kuss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Physical Constraint Layer 분석 ===\n",
      "Physical changes 분석:\n",
      "  dVA: Min=-1.133074, Max=0.667063, Mean=-0.376081\n",
      "  dVB: Min=-1.153411, Max=0.494161, Mean=-0.497235\n",
      "  dNALA: Min=-1.041727, Max=-0.246948, Mean=-0.684995\n",
      "  dNBLA: Min=-3.358232, Max=-1.970714, Mean=-2.630377\n",
      "  dNAK: Min=-1.416192, Max=-1.113917, Mean=-1.260747\n",
      "  dNBK: Min=-2.017585, Max=-0.834551, Mean=-1.419165\n",
      "  nI: Min=0.185245, Max=0.469077, Mean=0.275903\n",
      "\n",
      "Current state 분석:\n",
      "  V: Min=-1.000000, Max=0.794118, Mean=-0.200914\n",
      "  E: Min=-1.000000, Max=0.800000, Mean=-0.354392\n",
      "  VF: Min=-1.000000, Max=0.585891, Mean=-0.230745\n",
      "  VA: Min=-1.000000, Max=0.577676, Mean=-0.302716\n",
      "  VB: Min=-1.000000, Max=0.210233, Mean=-0.407226\n",
      "  CFLA: Min=-1.000000, Max=0.809285, Mean=-0.306026\n",
      "  CALA: Min=-1.000000, Max=0.633316, Mean=-0.433844\n",
      "  CBLA: Min=-1.000000, Max=0.590514, Mean=-0.435603\n",
      "  CFK: Min=-1.000000, Max=0.791922, Mean=-0.374055\n",
      "  CAK: Min=-1.000000, Max=0.248364, Mean=-0.442066\n",
      "  CBK: Min=-1.000000, Max=0.632817, Mean=-0.368929\n",
      "  I: Min=-1.000000, Max=0.776515, Mean=-0.317552\n",
      "\n",
      "부피 관련 분석:\n",
      "  VF: Min=-1.000000, Max=0.585891\n",
      "  nVF (after changes): Min=-0.855877, Max=1.795008\n",
      "  nVA (after changes): Min=-2.133074, Max=1.244739\n",
      "  nVB (after changes): Min=-2.153410, Max=0.688851\n",
      "  nVF (clamped): Min=0.00000001, Max=1.795008\n",
      "  nVA (clamped): Min=0.00000001, Max=1.244739\n",
      "  nVB (clamped): Min=0.00000001, Max=0.688851\n"
     ]
    }
   ],
   "source": [
    "# Physical constraint layer의 계산 과정을 자세히 분석\n",
    "print(\"=== Physical Constraint Layer 분석 ===\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Physical changes 분석\n",
    "    print(\"Physical changes 분석:\")\n",
    "    for i, name in enumerate(['dVA', 'dVB', 'dNALA', 'dNBLA', 'dNAK', 'dNBK', 'nI']):\n",
    "        values = physical_changes[..., i]\n",
    "        print(f\"  {name}: Min={values.min():.6f}, Max={values.max():.6f}, Mean={values.mean():.6f}\")\n",
    "    \n",
    "    # Current state 분석\n",
    "    print(\"\\nCurrent state 분석:\")\n",
    "    state_names = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']\n",
    "    for i, name in enumerate(state_names):\n",
    "        values = test_input[..., i]\n",
    "        print(f\"  {name}: Min={values.min():.6f}, Max={values.max():.6f}, Mean={values.mean():.6f}\")\n",
    "    \n",
    "    # 특히 부피 관련 값들 확인\n",
    "    print(\"\\n부피 관련 분석:\")\n",
    "    VF = test_input[..., 2]\n",
    "    VA = test_input[..., 3] \n",
    "    VB = test_input[..., 4]\n",
    "    dVA = physical_changes[..., 0]\n",
    "    dVB = physical_changes[..., 1]\n",
    "    \n",
    "    nVF = VF - dVA - dVB\n",
    "    nVA = VA + dVA\n",
    "    nVB = VB + dVB\n",
    "    \n",
    "    print(f\"  VF: Min={VF.min():.6f}, Max={VF.max():.6f}\")\n",
    "    print(f\"  nVF (after changes): Min={nVF.min():.6f}, Max={nVF.max():.6f}\")\n",
    "    print(f\"  nVA (after changes): Min={nVA.min():.6f}, Max={nVA.max():.6f}\")\n",
    "    print(f\"  nVB (after changes): Min={nVB.min():.6f}, Max={nVB.max():.6f}\")\n",
    "    \n",
    "    # eps 클램핑 후\n",
    "    eps = 1e-8\n",
    "    nVF_clamped = torch.clamp(nVF, min=eps)\n",
    "    nVA_clamped = torch.clamp(nVA, min=eps)\n",
    "    nVB_clamped = torch.clamp(nVB, min=eps)\n",
    "    \n",
    "    print(f\"  nVF (clamped): Min={nVF_clamped.min():.8f}, Max={nVF_clamped.max():.6f}\")\n",
    "    print(f\"  nVA (clamped): Min={nVA_clamped.min():.8f}, Max={nVA_clamped.max():.6f}\")\n",
    "    print(f\"  nVB (clamped): Min={nVB_clamped.min():.8f}, Max={nVB_clamped.max():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
