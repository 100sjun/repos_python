{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 모듈이 성공적으로 로드되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ 모든 모듈이 성공적으로 로드되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'Using device: {device}')\n",
    "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    else:\n",
    "        print(f'Using device: {device}')\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(name):\n",
    "    \"\"\"데이터 로드 및 정규화\"\"\"\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CBLA': {'min':0, 'max': df['CBLA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CAK': {'min':0, 'max': df['CAK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "    return ndf, range_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_const(ndf):\n",
    "    \"\"\"시퀀스 데이터 구성\"\"\"\n",
    "    sequences = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']\n",
    "    \n",
    "    for exp in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp].sort_values(by='t')\n",
    "        sequences.append(exp_data[feature_cols].values)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequences(sequences):\n",
    "    \"\"\"시퀀스 패딩 처리\"\"\"\n",
    "    max_seq_len = max([len(seq) for seq in sequences])\n",
    "    seq_len = [len(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=-1)\n",
    "\n",
    "    return padded_sequences, seq_len, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(pad_seq, seq_len):\n",
    "    \"\"\"데이터셋 생성\"\"\"\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_dataloaders(dataset, k_folds=5, batch_size=4, random_state=87):\n",
    "    \"\"\"K-fold 교차검증을 위한 데이터로더 생성\"\"\"\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "    dataloaders = []\n",
    "    batch_size = math.ceil(len(dataset)/k_folds)\n",
    "    \n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(range(len(dataset)))):\n",
    "        print(f\"Fold {fold + 1}: Train size = {len(train_indices)}, Val size = {len(val_indices)}\")\n",
    "        \n",
    "        # Create subsets for train and validation\n",
    "        train_subset = Subset(dataset, train_indices)\n",
    "        val_subset = Subset(dataset, val_indices)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        dataloaders.append((train_loader, val_loader))\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialStateExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    BMED 시스템의 시계열 패턴에서 숨겨진 dynamics를 추출하는 LSTM 기반 모듈\n",
    "    각 시점의 hidden state에는 해당 시점까지의 모든 과거 정보가 누적됨\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer with improved error handling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, \n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"\n",
    "        시계열 상태 시퀀스를 처리하여 각 시점의 hidden state 추출\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, input_size] - BMED 시스템 상태 시퀀스\n",
    "            seq_len: [batch_size] - 각 시퀀스의 실제 길이\n",
    "            \n",
    "        Returns:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size] - 각 시점의 누적된 hidden state\n",
    "        \"\"\"\n",
    "        \n",
    "        # 입력 검증\n",
    "        if x.size(0) != seq_len.size(0):\n",
    "            raise ValueError(f\"Batch size mismatch: input {x.size(0)} vs seq_len {seq_len.size(0)}\")\n",
    "        \n",
    "        # seq_len을 CPU로 이동하고 정수형으로 변환\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        \n",
    "        # 시퀀스 길이 유효성 검사\n",
    "        if (seq_len_cpu <= 0).any():\n",
    "            invalid_lengths = seq_len_cpu[seq_len_cpu <= 0]\n",
    "            raise ValueError(f\"Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths must be positive.\")\n",
    "        \n",
    "        # 패딩된 시퀀스를 pack하여 효율적 처리\n",
    "        packed_input = pack_padded_sequence(\n",
    "            x, seq_len_cpu, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "        \n",
    "        # 다시 패딩된 형태로 복원\n",
    "        lstm_out, output_lengths = pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=x.size(1)\n",
    "        )\n",
    "        \n",
    "        # Normalization and dropout\n",
    "        normalized = self.layer_norm(lstm_out)\n",
    "        return self.dropout(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Hidden state로부터 BMED 시스템의 물리적 변화량과 새로운 전류값을 디코딩하는 MLP\n",
    "    출력: [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI] - 7개 물리적 변화량\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, num_layers=2, num_nodes=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_nodes is None:\n",
    "            num_nodes = hidden_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # 첫 번째 레이어: hidden_size → num_nodes\n",
    "        self.layers.append(nn.Linear(hidden_size, num_nodes))\n",
    "        self.layers.append(nn.LayerNorm(num_nodes))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # 중간 은닉층들: num_nodes → num_nodes\n",
    "        for i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_nodes, num_nodes))\n",
    "            self.layers.append(nn.LayerNorm(num_nodes))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # 마지막 출력층: num_nodes → output_size (7개 물리적 변화량)\n",
    "        self.layers.append(nn.Linear(num_nodes, output_size))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Hidden state를 물리적 변화량으로 디코딩\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size] - 시점별 hidden state\n",
    "            \n",
    "        Returns:\n",
    "            physical_changes: [batch_size, seq_len, 7] - 물리적 변화량\n",
    "                [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI]\n",
    "        \"\"\"\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    물리적 변화량을 실제 시스템 상태로 변환하면서 물리적 제약 조건을 적용\n",
    "    Bipolar membrane electrodialysis 시스템의 물리 법칙 기반 상태 업데이트\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-1):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # division by zero 방지\n",
    "        \n",
    "    def forward(self, physical_changes, current_state):\n",
    "        \"\"\"\n",
    "        물리적 변화량을 현재 상태에 적용하여 다음 상태 계산\n",
    "        \n",
    "        Args:\n",
    "            physical_changes: [batch, seq, 7] - [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI]\n",
    "            current_state: [batch, seq, 12] - 현재 BMED 시스템 상태\n",
    "                V: 전압 (Voltage) - 실험 세트별 고정값\n",
    "                E: 외부 전해질 농도 (External electrolyte concentration) - 실험 세트별 고정값  \n",
    "                VF, VA, VB: Feed, Acid, Base 부피\n",
    "                CFLA, CALA, CBLA: Feed, Acid, Base의 LA 농도\n",
    "                CFK, CAK, CBK: Feed, Acid, Base의 K 농도\n",
    "                I: 전류\n",
    "                \n",
    "        Returns:\n",
    "            next_state: [batch, seq, 12] - 물리 제약이 적용된 다음 상태\n",
    "        \"\"\"\n",
    "        # 입력 차원 검증\n",
    "        if physical_changes.dim() != current_state.dim():\n",
    "            raise ValueError(f\"Dimension mismatch: physical_changes {physical_changes.shape} vs current_state {current_state.shape}\")\n",
    "        \n",
    "        if current_state.size(-1) != 12:\n",
    "            raise ValueError(f\"Expected 12 state features, got {current_state.size(-1)}\")\n",
    "            \n",
    "        if physical_changes.size(-1) != 7:\n",
    "            raise ValueError(f\"Expected 7 physical changes, got {physical_changes.size(-1)}\")\n",
    "        \n",
    "        # 현재 상태 변수 추출 (차원 유지)\n",
    "        V = current_state[..., 0:1]     # 전압 (고정값)\n",
    "        E = current_state[..., 1:2]     # 외부 전해질 농도 (고정값)\n",
    "        VF = current_state[..., 2:3]    # Feed 부피\n",
    "        VA = current_state[..., 3:4]    # Acid 부피\n",
    "        VB = current_state[..., 4:5]    # Base 부피\n",
    "        CFLA = current_state[..., 5:6]  # Feed LA 농도\n",
    "        CALA = current_state[..., 6:7]  # Acid LA 농도\n",
    "        CBLA = current_state[..., 7:8]  # Base LA 농도\n",
    "        CFK = current_state[..., 8:9]   # Feed K 농도\n",
    "        CAK = current_state[..., 9:10]  # Acid K 농도\n",
    "        CBK = current_state[..., 10:11] # Base K 농도\n",
    "        I = current_state[..., 11:12]   # 전류\n",
    "\n",
    "        # 물질량 계산 (농도 × 부피)\n",
    "        NFLA = CFLA * VF; NALA = CALA * VA; NBLA = CBLA * VB\n",
    "        NFK = CFK * VF; NAK = CAK * VA; NBK = CBK * VB\n",
    "\n",
    "        # 물리적 변화량 추출\n",
    "        dVA = physical_changes[..., 0:1]    # Acid 부피 변화량 (양방향 가능: 음수면 A→F)\n",
    "        dVB = physical_changes[..., 1:2]    # Base 부피 변화량 (양방향 가능: 음수면 B→F)\n",
    "        dNALA = physical_changes[..., 2:3]  # Acid LA 물질량 변화량 (일방향: F→A만)\n",
    "        dNBLA = physical_changes[..., 3:4]  # Base LA 물질량 변화량 (일방향: F→B만)\n",
    "        dNAK = physical_changes[..., 4:5]   # Acid K 물질량 변화량 (일방향: F→A만)\n",
    "        dNBK = physical_changes[..., 5:6]   # Base K 물질량 변화량 (일방향: F→B만)\n",
    "        nI = physical_changes[..., 6:7]     # 새로운 전류값\n",
    "\n",
    "        # 새로운 부피 계산 (양방향 흐름 허용)\n",
    "        nVF = VF - dVA - dVB  # dVA, dVB가 음수면 F로 역유입\n",
    "        nVA = VA + dVA        # dVA가 음수면 A에서 F로 유출\n",
    "        nVB = VB + dVB        # dVB가 음수면 B에서 F로 유출\n",
    "        \n",
    "        # 물질 이동량을 일방향으로 제한 (F→A, F→B만 허용)\n",
    "        dNALA_clipped = torch.clamp(dNALA, min=0)  # 음수 제거 (역방향 불가)\n",
    "        dNBLA_clipped = torch.clamp(dNBLA, min=0)\n",
    "        dNAK_clipped = torch.clamp(dNAK, min=0)\n",
    "        dNBK_clipped = torch.clamp(dNBK, min=0)\n",
    "        \n",
    "        # 새로운 물질량 계산 (일방향 이동만)\n",
    "        nNFLA = NFLA - dNALA_clipped - dNBLA_clipped  # Feed에서 유출만\n",
    "        nNALA = NALA + dNALA_clipped                  # Acid로 유입만\n",
    "        nNBLA = NBLA + dNBLA_clipped                  # Base로 유입만\n",
    "        nNFK = NFK - dNAK_clipped - dNBK_clipped      # K도 마찬가지\n",
    "        nNAK = NAK + dNAK_clipped\n",
    "        nNBK = NBK + dNBK_clipped\n",
    "        \n",
    "        # 물리적 제약 조건 적용 (양수 유지)\n",
    "        nVF = torch.clamp(nVF, min=self.eps)\n",
    "        nVA = torch.clamp(nVA, min=self.eps)\n",
    "        nVB = torch.clamp(nVB, min=self.eps)\n",
    "        \n",
    "        # 물질량 음수 방지\n",
    "        nNFLA = torch.clamp(nNFLA, min=0)\n",
    "        nNALA = torch.clamp(nNALA, min=0)\n",
    "        nNBLA = torch.clamp(nNBLA, min=0)\n",
    "        nNFK = torch.clamp(nNFK, min=0)\n",
    "        nNAK = torch.clamp(nNAK, min=0)\n",
    "        nNBK = torch.clamp(nNBK, min=0)\n",
    "        \n",
    "        # 새로운 농도 계산 (농도 = 물질량 / 부피)\n",
    "        nCFLA = nNFLA / nVF\n",
    "        nCALA = nNALA / nVA\n",
    "        nCBLA = nNBLA / nVB\n",
    "        nCFK = nNFK / nVF\n",
    "        nCAK = nNAK / nVA\n",
    "        nCBK = nNBK / nVB\n",
    "        \n",
    "        # 전류는 양수 제약\n",
    "        nI = torch.clamp(nI, min=0)\n",
    "\n",
    "        # 새로운 상태 조립 (V, E는 고정값이므로 그대로 유지)\n",
    "        next_state = torch.cat([\n",
    "            V, E,  # 고정값: 전압, 외부 전해질 농도\n",
    "            nVF, nVA, nVB,  # 새로운 부피 (양방향 흐름 반영)\n",
    "            nCFLA, nCALA, nCBLA,  # 새로운 LA 농도\n",
    "            nCFK, nCAK, nCBK,     # 새로운 K 농도\n",
    "            nI  # 새로운 전류\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BMED 시스템의 시계열 상태 예측을 위한 자기회귀 모델\n",
    "    \n",
    "    구조:\n",
    "    1. SequentialStateExtractor: LSTM으로 시계열 패턴의 hidden state 추출\n",
    "    2. PhysicalChangeDecoder: Hidden state를 물리적 변화량으로 디코딩  \n",
    "    3. PhysicsConstraintLayer: 물리 법칙 적용하여 다음 상태 계산\n",
    "    \"\"\"\n",
    "    def __init__(self, state_extractor_params, decoder_params):\n",
    "        super().__init__()\n",
    "        self.state_extractor = SequentialStateExtractor(**state_extractor_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer()\n",
    "\n",
    "    def forward(self, current_states, seq_lengths):\n",
    "        \"\"\"\n",
    "        현재 시점까지의 상태들로부터 다음 상태들 예측\n",
    "        \n",
    "        Args:\n",
    "            current_states: [batch, seq_len, 12] - 현재까지의 BMED 시스템 상태들\n",
    "            seq_lengths: [batch] - 각 시퀀스의 실제 길이\n",
    "            \n",
    "        Returns:\n",
    "            next_states: [batch, seq_len, 12] - 예측된 다음 시점 상태들\n",
    "        \"\"\"\n",
    "        # 1. LSTM으로 각 시점의 hidden state 추출 (과거 정보 누적)\n",
    "        hidden_states = self.state_extractor(current_states, seq_lengths)\n",
    "        \n",
    "        # 2. Hidden state를 물리적 변화량으로 디코딩\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        \n",
    "        # 3. 물리적 제약 조건을 적용하여 다음 상태 계산\n",
    "        next_states = self.physics_constraint(physical_changes, current_states)\n",
    "        \n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(predictions, targets, seq_lengths):\n",
    "    \"\"\"\n",
    "    개선된 마스킹된 MSE 손실 함수 - device 호환성 및 안정성 강화\n",
    "    \n",
    "    Args:\n",
    "        predictions: 모델 예측값 [batch_size, seq_len, features]\n",
    "        targets: 실제 타겟값 [batch_size, seq_len, features]  \n",
    "        seq_lengths: 각 시퀀스의 실제 길이 [batch_size]\n",
    "    \n",
    "    Returns:\n",
    "        masked_loss: 패딩 부분을 제외한 평균 MSE 손실\n",
    "    \"\"\"\n",
    "    # 입력 검증\n",
    "    if predictions.shape != targets.shape:\n",
    "        raise ValueError(f\"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}\")\n",
    "    \n",
    "    if predictions.size(0) != seq_lengths.size(0):\n",
    "        raise ValueError(f\"Batch size mismatch: predictions {predictions.size(0)} vs seq_lengths {seq_lengths.size(0)}\")\n",
    "    \n",
    "    batch_size, max_len, features = predictions.shape\n",
    "    \n",
    "    # seq_lengths를 CPU로 이동하여 arange와 호환되도록 처리\n",
    "    seq_lengths_cpu = seq_lengths.detach().cpu().long()\n",
    "    \n",
    "    # 시퀀스 길이 유효성 검사 - 데이터 구조 오류는 중단해야 함\n",
    "    if (seq_lengths_cpu <= 0).any():\n",
    "        invalid_lengths = seq_lengths_cpu[seq_lengths_cpu <= 0]\n",
    "        raise ValueError(f\"Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths must be positive.\")\n",
    "    \n",
    "    # 최대 길이 초과 검사\n",
    "    if (seq_lengths_cpu > max_len).any():\n",
    "        invalid_lengths = seq_lengths_cpu[seq_lengths_cpu > max_len]\n",
    "        raise ValueError(f\"Sequence lengths exceed max_len: {invalid_lengths.tolist()} > {max_len}\")\n",
    "    \n",
    "    # 마스크 생성: 실제 시퀀스 길이만큼만 True\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_lengths_cpu[:, None]\n",
    "    mask = mask.float().to(predictions.device)\n",
    "    \n",
    "    # 각 요소별 MSE 계산 (reduction='none')\n",
    "    loss = F.mse_loss(predictions, targets, reduction='none')\n",
    "    \n",
    "    # 마스크 적용하여 패딩 부분 제거\n",
    "    masked_loss_sum = (loss * mask.unsqueeze(-1)).sum()\n",
    "    valid_elements = mask.sum() * features\n",
    "    \n",
    "    # 0으로 나누기 방지\n",
    "    if valid_elements == 0:\n",
    "        raise ValueError(\"No valid elements found after masking. Check sequence lengths and data.\")\n",
    "    \n",
    "    masked_loss = masked_loss_sum / valid_elements\n",
    "    \n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_running_prediction_single(model, initial_state, num_steps, device, training=False):\n",
    "    \"\"\"\n",
    "    Free running prediction for a single experiment.\n",
    "    초기 상태에서 시작해서 모델이 자율적으로 시퀀스를 생성\n",
    "    \n",
    "    Args:\n",
    "        model: 학습된 모델\n",
    "        initial_state: 초기 상태 [12] - 시작점\n",
    "        num_steps: 예측할 스텝 수\n",
    "        device: torch device\n",
    "        training: 학습 모드인지 여부 (True면 gradient 계산)\n",
    "        \n",
    "    Returns:\n",
    "        predictions: [num_steps, 12] - 예측된 시퀀스 (초기값 제외)\n",
    "    \"\"\"\n",
    "    if not training:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Initialize with initial state\n",
    "            current_state = initial_state.unsqueeze(0).unsqueeze(0)  # [1, 1, 12]\n",
    "            predictions = []\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                seq_len = torch.tensor([current_state.size(1)], device=device)\n",
    "                next_state = model(current_state, seq_len)\n",
    "                next_step = next_state[:, -1:, :]  # 마지막 시점만 선택\n",
    "                predictions.append(next_step.squeeze(0).squeeze(0))  # [12]\n",
    "                \n",
    "                # 다음 입력으로 예측된 상태를 사용 (Free Running의 핵심)\n",
    "                current_state = torch.cat([current_state, next_step], dim=1)\n",
    "            \n",
    "            return torch.stack(predictions)  # [num_steps, 12]\n",
    "    else:\n",
    "        # 학습 모드: gradient 계산 필요\n",
    "        # Initialize with initial state\n",
    "        current_state = initial_state.unsqueeze(0).unsqueeze(0)  # [1, 1, 12]\n",
    "        predictions = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            seq_len = torch.tensor([current_state.size(1)], device=device)\n",
    "            next_state = model(current_state, seq_len)\n",
    "            next_step = next_state[:, -1:, :]  # 마지막 시점만 선택\n",
    "            predictions.append(next_step.squeeze(0).squeeze(0))  # [12]\n",
    "            \n",
    "            # 다음 입력으로 예측된 상태를 사용 (Free Running의 핵심)\n",
    "            # gradient를 유지하기 위해 detach하지 않음\n",
    "            current_state = torch.cat([current_state, next_step], dim=1)\n",
    "        \n",
    "        return torch.stack(predictions)  # [num_steps, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_free_running_data(input_sequences, seq_lengths):\n",
    "    \"\"\"\n",
    "    Free Running을 위한 입력-타겟 데이터 준비\n",
    "    \n",
    "    Args:\n",
    "        input_sequences: 전체 시퀀스 [batch_size, seq_len, features]\n",
    "        seq_lengths: 각 시퀀스의 실제 길이 [batch_size]\n",
    "    \n",
    "    Returns:\n",
    "        initial_states: [batch_size, 1, features] - 각 시퀀스의 초기 상태\n",
    "        targets: [batch_size, seq_len-1, features] - 예측해야 할 타겟들  \n",
    "        target_seq_lengths: 타겟 시퀀스 길이 (1씩 감소)\n",
    "    \"\"\"\n",
    "    # 초기 상태: 각 시퀀스의 첫 번째 시점\n",
    "    initial_states = input_sequences[:, :1, :]  # [batch, 1, features]\n",
    "    \n",
    "    # 타겟: 첫 번째 시점 제외한 나머지 [1:]\n",
    "    targets = input_sequences[:, 1:, :]  # [batch, seq_len-1, features]\n",
    "    \n",
    "    # 타겟 시퀀스 길이는 1씩 감소 (첫 시점은 초기 상태로 사용)\n",
    "    if (seq_lengths - 1 < 1).any():\n",
    "        invalid_lengths = seq_lengths[seq_lengths - 1 < 1]\n",
    "        raise ValueError(f\"타겟 시퀀스 길이가 0보다 작아질 수 없습니다. 잘못된 seq_lengths: {invalid_lengths.tolist()}\")\n",
    "    target_seq_lengths = seq_lengths - 1\n",
    "    \n",
    "    return initial_states, targets, target_seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_free_running(model, train_loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Free running 방식으로 한 에포크 학습\n",
    "    \n",
    "    Args:\n",
    "        model: 모델\n",
    "        train_loader: 학습 데이터로더\n",
    "        optimizer: 옵티마이저\n",
    "        device: 디바이스\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (input_seq, seq_lengths) in enumerate(train_loader):\n",
    "        try:\n",
    "            # 데이터를 디바이스로 이동\n",
    "            input_seq = input_seq.to(device)\n",
    "            seq_lengths = seq_lengths.to(device)\n",
    "            \n",
    "            # Free Running 데이터 준비\n",
    "            initial_states, targets, target_seq_lengths = prepare_free_running_data(input_seq, seq_lengths)\n",
    "            \n",
    "            batch_size = initial_states.size(0)\n",
    "            max_len = targets.size(1)  # 배치 내 최대 길이\n",
    "            predictions_list = []\n",
    "            \n",
    "            # 각 시퀀스에 대해 Free Running 예측 수행\n",
    "            for i in range(batch_size):\n",
    "                initial_state = initial_states[i, 0, :]  # [features]\n",
    "                num_steps = target_seq_lengths[i].item()\n",
    "                \n",
    "                if num_steps > 0:\n",
    "                    # Free running prediction (training=True로 gradient 계산)\n",
    "                    pred_seq = free_running_prediction_single(model, initial_state, num_steps, device, training=True)\n",
    "                    \n",
    "                    # 최대 길이에 맞춰 패딩\n",
    "                    if pred_seq.size(0) < max_len:\n",
    "                        padding = torch.zeros(max_len - pred_seq.size(0), pred_seq.size(1), device=device)\n",
    "                        pred_seq = torch.cat([pred_seq, padding], dim=0)\n",
    "                    elif pred_seq.size(0) > max_len:\n",
    "                        pred_seq = pred_seq[:max_len]\n",
    "                    \n",
    "                    predictions_list.append(pred_seq)\n",
    "                else:\n",
    "                    # 빈 시퀀스 처리\n",
    "                    empty_pred = torch.zeros(max_len, initial_state.size(0), device=device)\n",
    "                    predictions_list.append(empty_pred)\n",
    "            \n",
    "            # 배치로 스택\n",
    "            predictions = torch.stack(predictions_list, dim=0)  # [batch, max_len, features]\n",
    "            \n",
    "            # Loss 계산 (마스크 적용)\n",
    "            loss = masked_mse_loss(predictions, targets, target_seq_lengths)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 그래디언트 클리핑\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training batch {batch_idx} error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch_free_running(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Free running 방식으로 검증\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_seq, seq_lengths in val_loader:\n",
    "            try:\n",
    "                # 데이터를 디바이스로 이동\n",
    "                input_seq = input_seq.to(device)\n",
    "                seq_lengths = seq_lengths.to(device)\n",
    "                \n",
    "                # Free Running 데이터 준비\n",
    "                initial_states, targets, target_seq_lengths = prepare_free_running_data(input_seq, seq_lengths)\n",
    "                \n",
    "                batch_size = initial_states.size(0)\n",
    "                max_len = targets.size(1)  # 배치 내 최대 길이\n",
    "                predictions_list = []\n",
    "                \n",
    "                # 각 시퀀스에 대해 Free Running 예측 수행\n",
    "                for i in range(batch_size):\n",
    "                    initial_state = initial_states[i, 0, :]  # [features]\n",
    "                    num_steps = target_seq_lengths[i].item()\n",
    "                    \n",
    "                    if num_steps > 0:\n",
    "                        # Free running prediction (training=False로 검증)\n",
    "                        pred_seq = free_running_prediction_single(model, initial_state, num_steps, device, training=False)\n",
    "                        \n",
    "                        # 최대 길이에 맞춰 패딩\n",
    "                        if pred_seq.size(0) < max_len:\n",
    "                            padding = torch.zeros(max_len - pred_seq.size(0), pred_seq.size(1), device=device)\n",
    "                            pred_seq = torch.cat([pred_seq, padding], dim=0)\n",
    "                        elif pred_seq.size(0) > max_len:\n",
    "                            pred_seq = pred_seq[:max_len]\n",
    "                        \n",
    "                        predictions_list.append(pred_seq)\n",
    "                    else:\n",
    "                        # 빈 시퀀스 처리\n",
    "                        empty_pred = torch.zeros(max_len, initial_state.size(0), device=device)\n",
    "                        predictions_list.append(empty_pred)\n",
    "                \n",
    "                # 배치로 스택\n",
    "                predictions = torch.stack(predictions_list, dim=0)  # [batch, max_len, features]\n",
    "                \n",
    "                # Loss 계산\n",
    "                loss = masked_mse_loss(predictions, targets, target_seq_lengths)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Validation batch error: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDFreeRunningOptimizer:\n",
    "    \"\"\"\n",
    "    BMED 자기회귀 모델을 위한 Free Running 기반 K-fold CV 하이퍼파라미터 최적화 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloaders, device=None):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 하이퍼파라미터 범위 정의 (Free Running에 맞춰 조정)\n",
    "        self.param_ranges = {\n",
    "            'hidden_size': {'low': 16, 'high': 128, 'step': 16},  # 더 넓은 범위\n",
    "            'num_layers': {'low': 2, 'high': 5},  # 최소 2층부터 시작\n",
    "            'extractor_dropout': {'low': 0.1, 'high': 0.5},\n",
    "            'decoder_layers': {'low': 2, 'high': 5},  # 최소 2층부터 시작\n",
    "            'decoder_nodes': {'low': 16, 'high': 128, 'step': 16},  # 더 넓은 범위\n",
    "            'decoder_dropout': {'low': 0.1, 'high': 0.5},\n",
    "            'learning_rate': {'low': 1e-5, 'high': 1e-1, 'log': True},  # 더 낮은 학습률\n",
    "            'weight_decay': {'low': 1e-7, 'high': 1e-3, 'log': True}\n",
    "        }\n",
    "        \n",
    "        # 학습 설정 - Free Running에 맞춰 조정 (max_predict_length 제거)\n",
    "        self.train_config = {\n",
    "            'epochs': 70,      # Free Running은 더 많은 에포크 필요\n",
    "            'patience': 15,     # 더 관대한 early stopping\n",
    "            'min_epochs': 30   # 충분한 학습 보장\n",
    "        }\n",
    "    \n",
    "    def create_model(self, trial):\n",
    "        \"\"\"하이퍼파라미터 샘플링 및 모델 생성\"\"\"\n",
    "        # 하이퍼파라미터 샘플링\n",
    "        params = {}\n",
    "        params['hidden_size'] = trial.suggest_int('hidden_size', **self.param_ranges['hidden_size'])\n",
    "        params['num_layers'] = trial.suggest_int('num_layers', **self.param_ranges['num_layers'])\n",
    "        params['extractor_dropout'] = trial.suggest_float('extractor_dropout', **self.param_ranges['extractor_dropout'])\n",
    "        params['decoder_layers'] = trial.suggest_int('decoder_layers', **self.param_ranges['decoder_layers'])\n",
    "        params['decoder_nodes'] = trial.suggest_int('decoder_nodes', **self.param_ranges['decoder_nodes'])\n",
    "        params['decoder_dropout'] = trial.suggest_float('decoder_dropout', **self.param_ranges['decoder_dropout'])\n",
    "        params['learning_rate'] = trial.suggest_float('learning_rate', **self.param_ranges['learning_rate'])\n",
    "        params['weight_decay'] = trial.suggest_float('weight_decay', **self.param_ranges['weight_decay'])\n",
    "        \n",
    "        # 모델 파라미터 구성\n",
    "        model_params = {\n",
    "            'state_extractor': {\n",
    "                'input_size': 12,\n",
    "                'hidden_size': params['hidden_size'],\n",
    "                'num_layers': params['num_layers'],\n",
    "                'dropout': params['extractor_dropout']\n",
    "            },\n",
    "            'decoder': {\n",
    "                'hidden_size': params['hidden_size'],\n",
    "                'output_size': 7,\n",
    "                'num_layers': params['decoder_layers'],\n",
    "                'num_nodes': params['decoder_nodes'],\n",
    "                'dropout': params['decoder_dropout']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 옵티마이저 파라미터\n",
    "        optimizer_params = {\n",
    "            'lr': params['learning_rate'],\n",
    "            'weight_decay': params['weight_decay']\n",
    "        }\n",
    "        \n",
    "        return model_params, optimizer_params\n",
    "    \n",
    "    def train_single_fold(self, model_params, optimizer_params, train_loader, val_loader):\n",
    "        \"\"\"단일 fold 학습 (Free Running 방식)\"\"\"\n",
    "        try:\n",
    "            # 모델 초기화\n",
    "            model = BMEDAutoregressiveModel(model_params['state_extractor'], model_params['decoder']).to(self.device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), **optimizer_params)\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(self.train_config['epochs']):\n",
    "                # Free Running 학습 (max_predict_length 파라미터 제거)\n",
    "                print(f'epoch: {epoch}')\n",
    "                try:\n",
    "                    train_loss = train_epoch_free_running(model, train_loader, optimizer, self.device)\n",
    "                    \n",
    "                    # Free Running 검증 (max_predict_length 파라미터 제거)\n",
    "                    val_loss = validate_epoch_free_running(model, val_loader, self.device)\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    # 최소 에포크 후 early stopping 적용\n",
    "                    if epoch >= self.train_config['min_epochs'] and patience_counter >= self.train_config['patience']:\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Epoch {epoch} error: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            return best_val_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fold training error: {str(e)}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective 함수 - Free Running 기반\"\"\"\n",
    "        try:\n",
    "            # 모델 및 옵티마이저 파라미터 생성\n",
    "            model_params, optimizer_params = self.create_model(trial)\n",
    "            \n",
    "            # K-fold 교차검증 - 모든 fold 실행\n",
    "            fold_losses = []\n",
    "            for fold_idx, (train_loader, val_loader) in enumerate(self.dataloaders):\n",
    "                fold_loss = self.train_single_fold(\n",
    "                    model_params, optimizer_params, \n",
    "                    train_loader, val_loader\n",
    "                )\n",
    "                print(f'trial: {trial.number}, fold_idx: {fold_idx}')\n",
    "                if fold_loss == float('inf'):\n",
    "                    return float('inf')\n",
    "                \n",
    "                fold_losses.append(fold_loss)\n",
    "            \n",
    "            # 평균 검증 손실 반환 (모든 fold 완료)\n",
    "            mean_loss = sum(fold_losses) / len(fold_losses)\n",
    "            return mean_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return float('inf')\n",
    "\n",
    "def optimize_bmed_free_running(trial, dataloaders):\n",
    "    \"\"\"Optuna를 위한 래퍼 함수\"\"\"\n",
    "    optimizer = BMEDFreeRunningOptimizer(dataloaders)\n",
    "    return optimizer.objective(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 데이터 로드 및 전처리...\n",
      "Fold 1: Train size = 31, Val size = 8\n",
      "Fold 2: Train size = 31, Val size = 8\n",
      "Fold 3: Train size = 31, Val size = 8\n",
      "Fold 4: Train size = 31, Val size = 8\n",
      "Fold 5: Train size = 32, Val size = 7\n",
      "\n",
      "✅ 데이터 전처리 완료!\n",
      "- 시퀀스 개수: 39\n",
      "- 최대 시퀀스 길이: 37\n",
      "- K-fold 수: 5\n",
      "- 각 fold는 (train_loader, val_loader) 튜플\n",
      "- Free Running 기반 최적화 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 전처리\n",
    "print(\"📊 데이터 로드 및 전처리...\")\n",
    "\n",
    "# 데이터 로드 및 정규화\n",
    "ndf, range_mm = norm_data('BMED_DATA_AG.csv')\n",
    "\n",
    "# 시퀀스 데이터 구성\n",
    "seq = seq_data_const(ndf)\n",
    "pad_seq, seq_len, max_seq_len = padded_sequences(seq)\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = gen_dataset(pad_seq, seq_len)\n",
    "\n",
    "# K-fold 데이터로더 생성\n",
    "dataloaders = kfold_dataloaders(dataset, k_folds=5, batch_size=1, random_state=87)  # 배치 크기 감소\n",
    "\n",
    "print(f\"\\n✅ 데이터 전처리 완료!\")\n",
    "print(f\"- 시퀀스 개수: {len(seq)}\")\n",
    "print(f\"- 최대 시퀀스 길이: {max_seq_len}\")\n",
    "print(f\"- K-fold 수: {len(dataloaders)}\")\n",
    "print(f\"- 각 fold는 (train_loader, val_loader) 튜플\")\n",
    "print(f\"- Free Running 기반 최적화 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 BMED Free Running 하이퍼파라미터 최적화 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 23:56:12,456] A new study created in RDB with name: bmed_autoregressive_free_running_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study 이름: bmed_autoregressive_free_running_optimization\n",
      "저장 위치: sqlite:///bmed_optuna_study_FR.db\n",
      "최적화 방식: Free Running + K-fold Cross Validation\n",
      "Teacher Forcing과 다른 점:\n",
      "  - 초기 상태만 주어지고 모델이 자율적으로 시퀀스 생성\n",
      "  - 실제 inference와 동일한 조건에서 학습\n",
      "  - 더 복잡한 모델 구조 필요 (층수 증가)\n",
      "  - 더 낮은 학습률과 더 많은 에포크 사용\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n",
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-08-08 23:56:56,560] Trial 0 failed with parameters: {'hidden_size': 48, 'num_layers': 5, 'extractor_dropout': 0.39279757672456206, 'decoder_layers': 4, 'decoder_nodes': 32, 'decoder_dropout': 0.16239780813448107, 'learning_rate': 1.7073967431528103e-05, 'weight_decay': 0.0002915443189153752} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sjbaek/miniforge3/envs/torchenv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1556/181069634.py\", line 33, in <lambda>\n",
      "    lambda trial: optimize_bmed_free_running(trial, dataloaders),\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1556/3559349952.py\", line 138, in optimize_bmed_free_running\n",
      "    return optimizer.objective(trial)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1556/3559349952.py\", line 115, in objective\n",
      "    fold_loss = self.train_single_fold(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1556/3559349952.py\", line 80, in train_single_fold\n",
      "    train_loss = train_epoch_free_running(model, train_loader, optimizer, self.device)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1556/3209576604.py\", line 57, in train_epoch_free_running\n",
      "    loss.backward()\n",
      "  File \"/home/sjbaek/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/sjbaek/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/sjbaek/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-08-08 23:56:56,561] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[407]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m device = set_device()\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 최적화 실행 - Free Running 전용\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize_bmed_free_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Free Running은 더 시간이 오래 걸리므로 trial 수 조정\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🎯 Free Running 최적화 완료!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/optuna/study/study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/optuna/study/_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/optuna/study/_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/optuna/study/_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[407]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     29\u001b[39m device = set_device()\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 최적화 실행 - Free Running 전용\u001b[39;00m\n\u001b[32m     32\u001b[39m study.optimize(\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43moptimize_bmed_free_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[32m     34\u001b[39m     n_trials=\u001b[32m10\u001b[39m,  \u001b[38;5;66;03m# Free Running은 더 시간이 오래 걸리므로 trial 수 조정\u001b[39;00m\n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🎯 Free Running 최적화 완료!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[405]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36moptimize_bmed_free_running\u001b[39m\u001b[34m(trial, dataloaders)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Optuna를 위한 래퍼 함수\"\"\"\u001b[39;00m\n\u001b[32m    137\u001b[39m optimizer = BMEDFreeRunningOptimizer(dataloaders)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[405]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mBMEDFreeRunningOptimizer.objective\u001b[39m\u001b[34m(self, trial)\u001b[39m\n\u001b[32m    113\u001b[39m fold_losses = []\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx, (train_loader, val_loader) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataloaders):\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     fold_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_single_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, fold_idx: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fold_loss == \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[405]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mBMEDFreeRunningOptimizer.train_single_fold\u001b[39m\u001b[34m(self, model_params, optimizer_params, train_loader, val_loader)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     train_loss = \u001b[43mtrain_epoch_free_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Free Running 검증 (max_predict_length 파라미터 제거)\u001b[39;00m\n\u001b[32m     83\u001b[39m     val_loss = validate_epoch_free_running(model, val_loader, \u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[403]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mtrain_epoch_free_running\u001b[39m\u001b[34m(model, train_loader, optimizer, device)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     56\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)  \u001b[38;5;66;03m# 그래디언트 클리핑\u001b[39;00m\n\u001b[32m     59\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# BMED 자기회귀 모델 Free Running 하이퍼파라미터 최적화 실행\n",
    "print(\"🚀 BMED Free Running 하이퍼파라미터 최적화 시작...\")\n",
    "\n",
    "# SQLite 데이터베이스에 study 결과 저장\n",
    "study_name = \"bmed_autoregressive_free_running_optimization\"\n",
    "storage_name = \"sqlite:///bmed_optuna_study_FR.db\"\n",
    "\n",
    "# Optuna study 생성 - Free Running K-fold CV 기반\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    direction='minimize',  # 손실을 최소화\n",
    "    pruner=optuna.pruners.NopPruner(),  # K-fold CV와 호환을 위해 pruning 비활성화\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    load_if_exists=True  # 기존 study가 있으면 이어서 실행\n",
    ")\n",
    "\n",
    "print(f\"Study 이름: {study_name}\")\n",
    "print(f\"저장 위치: {storage_name}\")\n",
    "print(\"최적화 방식: Free Running + K-fold Cross Validation\")\n",
    "print(\"Teacher Forcing과 다른 점:\")\n",
    "print(\"  - 초기 상태만 주어지고 모델이 자율적으로 시퀀스 생성\")\n",
    "print(\"  - 실제 inference와 동일한 조건에서 학습\")\n",
    "print(\"  - 더 복잡한 모델 구조 필요 (층수 증가)\")\n",
    "print(\"  - 더 낮은 학습률과 더 많은 에포크 사용\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 디바이스 설정\n",
    "device = set_device()\n",
    "\n",
    "# 최적화 실행 - Free Running 전용\n",
    "study.optimize(\n",
    "    lambda trial: optimize_bmed_free_running(trial, dataloaders), \n",
    "    n_trials=10,  # Free Running은 더 시간이 오래 걸리므로 trial 수 조정\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🎯 Free Running 최적화 완료!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"\\n📊 최적의 하이퍼파라미터 (Free Running):\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n🏆 최적의 검증 손실: {study.best_value:.6f}\")\n",
    "print(f\"📈 완료된 trial 수: {len(study.trials)}\")\n",
    "print(f\"✅ 성공한 trial 수: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
    "print(f\"❌ 실패한 trial 수: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}\")\n",
    "\n",
    "# 결과를 JSON 파일로도 저장\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "results_dict = {\n",
    "    'study_name': study_name,\n",
    "    'model_type': 'BMED_Autoregressive_FreeRunning',\n",
    "    'optimization_method': 'Free_Running_K-fold_Cross_Validation',\n",
    "    'best_params': study.best_params,\n",
    "    'best_value': study.best_value,\n",
    "    'n_trials': len(study.trials),\n",
    "    'timestamp': datetime.datetime.now().isoformat(),\n",
    "    'hyperparameter_ranges': {\n",
    "        'hidden_size': '32-512 (step 32)',\n",
    "        'num_layers': '2-8',\n",
    "        'decoder_layers': '2-8',\n",
    "        'decoder_nodes': '64-512 (step 32)',\n",
    "        'learning_rate': '1e-5 to 1e-2',\n",
    "        'notes': 'Free Running optimized ranges - higher complexity than Teacher Forcing'\n",
    "    },\n",
    "    'trials': [\n",
    "        {\n",
    "            'number': trial.number,\n",
    "            'value': trial.value,\n",
    "            'params': trial.params,\n",
    "            'state': trial.state.name\n",
    "        }\n",
    "        for trial in study.trials\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('bmed_autoregressive_free_running_optimization_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n💾 결과 저장 완료:\")\n",
    "print(f\"  - 데이터베이스: bmed_optuna_study_FR.db\")\n",
    "print(f\"  - JSON 파일: bmed_autoregressive_free_running_optimization_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Running 최적화 결과 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"📈 최적화 결과 시각화...\")\n",
    "\n",
    "# 1. 최적화 과정 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1-1. Optimization History\n",
    "trials = study.trials\n",
    "trial_values = [t.value for t in trials if t.value is not None and t.value != float('inf')]\n",
    "trial_numbers = [t.number for t in trials if t.value is not None and t.value != float('inf')]\n",
    "\n",
    "if trial_values:\n",
    "    axes[0, 0].plot(trial_numbers, trial_values, 'b-', alpha=0.6, linewidth=1)\n",
    "    axes[0, 0].scatter(trial_numbers, trial_values, c='blue', alpha=0.7, s=30)\n",
    "    axes[0, 0].axhline(y=study.best_value, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'Best: {study.best_value:.6f}')\n",
    "    axes[0, 0].set_xlabel('Trial Number')\n",
    "    axes[0, 0].set_ylabel('Validation Loss (Free Running)')\n",
    "    axes[0, 0].set_title('Free Running Optimization History')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')  # 로그 스케일로 표시\n",
    "\n",
    "# 1-2. Parameter Importance\n",
    "try:\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    params = list(importance.keys())\n",
    "    values = list(importance.values())\n",
    "    \n",
    "    axes[0, 1].barh(params, values, color='skyblue')\n",
    "    axes[0, 1].set_xlabel('Importance')\n",
    "    axes[0, 1].set_title('Parameter Importance (Free Running)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "except:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Parameter importance\\nnot available\\n(need more completed trials)', \n",
    "                    ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('Parameter Importance (Free Running)')\n",
    "\n",
    "# 1-3. Best Parameters Comparison\n",
    "best_params = study.best_params\n",
    "param_names = ['hidden_size', 'num_layers', 'decoder_layers', 'decoder_nodes']\n",
    "param_values = [best_params.get(name, 0) for name in param_names]\n",
    "\n",
    "bars = axes[1, 0].bar(param_names, param_values, color=['orange', 'green', 'purple', 'brown'])\n",
    "axes[1, 0].set_ylabel('Parameter Value')\n",
    "axes[1, 0].set_title('Best Architecture Parameters (Free Running)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# 값 표시\n",
    "for bar, value in zip(bars, param_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{int(value)}', ha='center', va='bottom')\n",
    "\n",
    "# 1-4. Trial States Distribution\n",
    "states = [t.state.name for t in study.trials]\n",
    "unique_states, counts = np.unique(states, return_counts=True)\n",
    "\n",
    "colors = ['lightgreen', 'lightcoral', 'gold', 'lightblue']\n",
    "wedges, texts, autotexts = axes[1, 1].pie(counts, labels=unique_states, autopct='%1.1f%%', \n",
    "                                          startangle=90, colors=colors[:len(unique_states)])\n",
    "axes[1, 1].set_title('Trial States Distribution (Free Running)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bmed_free_running_optimization_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Teacher Forcing vs Free Running 비교 (만약 둘 다 있다면)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔍 Free Running 하이퍼파라미터 최적화 요약\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"총 실행된 trial 수: {len(study.trials)}\")\n",
    "print(f\"성공한 trial 수: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
    "print(f\"실패한 trial 수: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}\")\n",
    "print(f\"\\n🎯 최적의 검증 손실: {study.best_value:.6f}\")\n",
    "\n",
    "print(\"\\n🏗️ 최적 모델 구조 (Free Running):\")\n",
    "print(f\"  - LSTM 은닉 크기: {study.best_params['hidden_size']}\")\n",
    "print(f\"  - LSTM 레이어 수: {study.best_params['num_layers']}\")\n",
    "print(f\"  - MLP 레이어 수: {study.best_params['decoder_layers']}\")\n",
    "print(f\"  - MLP 노드 수: {study.best_params['decoder_nodes']}\")\n",
    "print(f\"  - 학습률: {study.best_params['learning_rate']:.2e}\")\n",
    "\n",
    "# 3. 상위 5개 trial 정보\n",
    "print(f\"\\n🏆 상위 5개 trial (Free Running):\")\n",
    "best_trials = sorted([t for t in study.trials if t.value is not None and t.value != float('inf')], \n",
    "                    key=lambda x: x.value)[:5]\n",
    "for i, trial in enumerate(best_trials, 1):\n",
    "    print(f\"  {i}. Trial {trial.number}: Loss = {trial.value:.6f}\")\n",
    "    print(f\"     Hidden: {trial.params.get('hidden_size', 'N/A')}, \"\n",
    "          f\"LSTM Layers: {trial.params.get('num_layers', 'N/A')}, \"\n",
    "          f\"MLP Layers: {trial.params.get('decoder_layers', 'N/A')}\")\n",
    "\n",
    "print(\"\\n✅ Free Running 하이퍼파라미터 최적화 완료!\")\n",
    "print(\"🚀 이제 BMED_Autoregressive_Model.ipynb에서 이 결과를 사용할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 하이퍼파라미터로 최종 Free Running 모델 학습 및 저장\n",
    "print(\"🔨 최적 하이퍼파라미터로 최종 Free Running 모델 학습...\")\n",
    "\n",
    "# 최적 파라미터로 모델 설정\n",
    "best_params = study.best_params\n",
    "final_model_params = {\n",
    "    'state_extractor': {\n",
    "        'input_size': 12,\n",
    "        'hidden_size': best_params['hidden_size'],\n",
    "        'num_layers': best_params['num_layers'],\n",
    "        'dropout': best_params['extractor_dropout']\n",
    "    },\n",
    "    'decoder': {\n",
    "        'hidden_size': best_params['hidden_size'],\n",
    "        'output_size': 7,\n",
    "        'num_layers': best_params['decoder_layers'],\n",
    "        'num_nodes': best_params['decoder_nodes'],\n",
    "        'dropout': best_params['decoder_dropout']\n",
    "    }\n",
    "}\n",
    "\n",
    "final_train_params = {\n",
    "    'epochs': 500,  # 최종 학습에서는 더 많은 에포크\n",
    "    'patience': 50,\n",
    "    'optimizer': {\n",
    "        'lr': best_params['learning_rate'],\n",
    "        'weight_decay': best_params['weight_decay']\n",
    "    }\n",
    "}\n",
    "\n",
    "# K-fold로 최종 평가 (Free Running)\n",
    "device = set_device()\n",
    "final_results = []\n",
    "\n",
    "for fold, (train_loader, val_loader) in enumerate(dataloaders):\n",
    "    print(f\"\\n=== Final Free Running Training Fold {fold + 1} ===\")\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = BMEDAutoregressiveModel(final_model_params['state_extractor'], final_model_params['decoder']).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **final_train_params['optimizer'])\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(final_train_params['epochs']):\n",
    "        # max_predict_length 파라미터 제거\n",
    "        train_loss = train_epoch_free_running(model, train_loader, optimizer, device)\n",
    "        val_loss = validate_epoch_free_running(model, val_loader, device)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}, Best = {best_val_loss:.6f}\")\n",
    "            \n",
    "        if patience_counter >= final_train_params['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    final_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'model_state': best_model_state\n",
    "    })\n",
    "\n",
    "# 최종 결과 요약\n",
    "final_val_losses = [result['best_val_loss'] for result in final_results]\n",
    "final_mean_loss = sum(final_val_losses) / len(final_val_losses)\n",
    "final_std_loss = (sum([(x - final_mean_loss)**2 for x in final_val_losses]) / len(final_val_losses))**0.5\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎯 최종 Free Running 결과 (전체 시퀀스 학습)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"평균 검증 손실: {final_mean_loss:.6f} ± {final_std_loss:.6f}\")\n",
    "print(f\"최고 성능 Fold: {min(enumerate(final_val_losses), key=lambda x: x[1])[0] + 1}\")\n",
    "print(f\"최고 검증 손실: {min(final_val_losses):.6f}\")\n",
    "\n",
    "for i, result in enumerate(final_results):\n",
    "    print(f\"Fold {i+1}: {result['best_val_loss']:.6f}\")\n",
    "\n",
    "# 최고 성능 모델 저장\n",
    "best_fold_idx = min(enumerate(final_val_losses), key=lambda x: x[1])[0]\n",
    "best_model_path = 'best_bmed_autoregressive_FREE_RUNNING_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': final_results[best_fold_idx]['model_state'],\n",
    "    'model_params': final_model_params,\n",
    "    'best_params': best_params,\n",
    "    'val_loss': min(final_val_losses),\n",
    "    'fold': best_fold_idx + 1,\n",
    "    'model_type': 'BMED_Autoregressive_FreeRunning',\n",
    "    'training_method': 'Free_Running_Full_Sequence',\n",
    "    'range_mm': range_mm  # 정규화 범위도 함께 저장\n",
    "}, best_model_path)\n",
    "\n",
    "print(f\"\\n💾 최고 성능 Free Running 모델 저장: {best_model_path}\")\n",
    "print(\"📊 모든 시퀀스 길이에 대해 완전한 학습이 수행되었습니다!\")\n",
    "print(\"\\n🚀 이제 이 모델을 result_processing.ipynb에서 성능을 평가할 수 있습니다!\")\n",
    "print(\"📊 Teacher Forcing 결과와 비교해보세요!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
