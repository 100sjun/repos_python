{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbe9d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optuna\n",
    "np.random.seed(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "306f65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set the device to GPU if available, otherwise use CPU.\n",
    "\n",
    "    Returns:\n",
    "        device (torch.device): The device to use for training.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'Using device: {device}')\n",
    "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    else:\n",
    "        print(f'Using device: {device}')\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15c611c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(name):\n",
    "    \"\"\"\n",
    "    Load the data from the csv file and normalize the data.\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of the csv file.\n",
    "\n",
    "    Returns:\n",
    "        ndf (pd.DataFrame): The normalized data.\n",
    "        exp_num_list (list): List of experiment numbers in order.\n",
    "    \"\"\"\n",
    "    # raw data\n",
    "    df = pd.read_csv(name) \n",
    "\n",
    "    # normalized data\n",
    "    ndf = pd.DataFrame() \n",
    "\n",
    "    # the range of min-max normalization for each feature\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CBLA': {'min':0, 'max': df['CBLA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CAK': {'min':0, 'max': df['CAK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "    \n",
    "    # add experiment number and time\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t'] \n",
    "\n",
    "    # min-max normalization\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']: # min-max normalization\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "\n",
    "    # Get the unique experiment numbers in order\n",
    "    exp_num_list = sorted(ndf['exp'].unique())\n",
    "\n",
    "    return ndf, exp_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bdecd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_const(ndf):\n",
    "    \"\"\"\n",
    "    Set the data sequences.\n",
    "\n",
    "    Args:\n",
    "        ndf (pd.DataFrame): The normalized data.\n",
    "\n",
    "    Returns:\n",
    "        sequences (list): The sequences of the data.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CBLA', 'CFK', 'CAK', 'CBK', 'I']\n",
    "    \n",
    "    # get the sequences of the data for each experiment\n",
    "    for exp in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp].sort_values(by='t')\n",
    "        sequences.append(exp_data[feature_cols].values)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43e19a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequences(sequences):\n",
    "    \"\"\"\n",
    "    Pad the sequences.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): The sequences of the data.\n",
    "\n",
    "    Returns:\n",
    "        padded_sequences (torch.Tensor): The padded sequences.\n",
    "    \"\"\"\n",
    "    max_seq_len = max([len(seq) for seq in sequences])\n",
    "    seq_len = [len(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return padded_sequences, seq_len, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84d4489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(pad_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Generate the dataset.\n",
    "\n",
    "    Args:\n",
    "        pad_seq (torch.Tensor): The padded sequences.\n",
    "        seq_len (list): The length of the sequences.\n",
    "\n",
    "    Returns:\n",
    "        dataset (torch.utils.data.Dataset): The dataset.\n",
    "    \"\"\"\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86eb9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaders(dataset, exp_num_list, batch_size=4):\n",
    "    \"\"\"\n",
    "    Split the dataset into train/val/test with 8:1:1 ratio\n",
    "    \n",
    "    Args:\n",
    "        dataset: TensorDataset\n",
    "        exp_num_list: list of experiment numbers\n",
    "        batch_size: batch size\n",
    "        random_state: random seed\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # required train experiment numbers\n",
    "    required_train_exps = [1, 3, 5, 6, 11, 15, 17, 19, 20, 40, 41, 42]\n",
    "    \n",
    "    # all experiment numbers\n",
    "    all_exps = exp_num_list\n",
    "    total_exps = len(all_exps)\n",
    "    \n",
    "    # batch_size\n",
    "    batch_size = math.ceil(len(dataset)/10)\n",
    "\n",
    "    # 8:1:1 ratio\n",
    "    train_count = int(total_exps * 0.8)\n",
    "    val_count = math.ceil(total_exps * 0.1)\n",
    "    \n",
    "    # remaining experiments\n",
    "    remaining_exps = [exp for exp in all_exps if exp not in required_train_exps]\n",
    "    \n",
    "    # number of experiments to add to train\n",
    "    additional_train_needed = train_count - len(required_train_exps)\n",
    "    \n",
    "    if additional_train_needed < 0:\n",
    "        raise ValueError(\"The number of required train experiments is greater than the total train set. Please adjust required_train_exps.\")\n",
    "    \n",
    "    # shuffle remaining experiments\n",
    "    np.random.shuffle(remaining_exps)\n",
    "    \n",
    "    # split remaining experiments into train, val, test\n",
    "    train_exps = required_train_exps + remaining_exps[:additional_train_needed]\n",
    "    val_exps = remaining_exps[additional_train_needed:additional_train_needed + val_count]\n",
    "    test_exps = remaining_exps[additional_train_needed + val_count:]\n",
    "    \n",
    "    print(f\"Actual split:\")\n",
    "    print(f\"  Train: {sorted(train_exps)} ({len(train_exps)} experiments)\")\n",
    "    print(f\"  Val: {sorted(val_exps)} ({len(val_exps)} experiments)\")  \n",
    "    print(f\"  Test: {sorted(test_exps)} ({len(test_exps)} experiments)\")\n",
    "    \n",
    "    # find indices of each experiment (exp_num_list and dataset have the same order)\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for idx, exp in enumerate(all_exps):\n",
    "        if exp in train_exps:\n",
    "            train_indices.append(idx)\n",
    "        elif exp in val_exps:\n",
    "            val_indices.append(idx)\n",
    "        elif exp in test_exps:\n",
    "            test_indices.append(idx)\n",
    "    \n",
    "    # split dataset into train, val, test\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "    \n",
    "    # create DataLoader\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nCompleted DataLoader creation:\")\n",
    "    print(f\"  Train: {len(train_subset) if train_subset else 0} sequences\")\n",
    "    print(f\"  Val: {len(val_subset) if val_subset else 0} sequences\")\n",
    "    print(f\"  Test: {len(test_subset) if test_subset else 0} sequences\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6bda98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialStateExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    The module based on LSTM to extract hidden dynamics from the sequential pattern of BMED.\n",
    "    The hidden state of each step accumulates the information of all previous steps.\n",
    "\n",
    "    Args:\n",
    "        input_nodes (int): The number of input nodes.\n",
    "        hidden_nodes (int): The number of hidden nodes.\n",
    "        num_layers (int): The number of layers.\n",
    "        dropout (float): The dropout rate.\n",
    "    \n",
    "    Output:\n",
    "        hidden_states: [batch_size, seq_len, hidden_nodes] - hidden state of each step\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nodes, hidden_nodes, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_nodes, hidden_nodes, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_nodes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"\n",
    "        Extract the hidden state of each step from the sequential pattern of BMED.\n",
    "\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, input_nodes] - state sequence of BMED system\n",
    "            seq_len [batch_size] - length of each sequence\n",
    "\n",
    "        Returns:\n",
    "            hidden_states: [batch_size, seq_len, hidden_nodes] - hidden state of each step\n",
    "        \"\"\"\n",
    "        # check the input shape\n",
    "        if x.size(0) != seq_len.size(0):\n",
    "            raise ValueError(f\"Batch size mismatch: input {x.size(0)} vs seq_len {seq_len.size(0)}\")\n",
    "        \n",
    "        # Move the seq_len to CPU and transfer to integer\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "\n",
    "        # check the length of sequence\n",
    "        if (seq_len_cpu <= 0).any():\n",
    "            invalid_lengths = seq_len_cpu[seq_len_cpu <= 0]\n",
    "            raise ValueError(f'Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths mut be positive')\n",
    "        \n",
    "        # pack the padded sequence\n",
    "        packed_input = pack_padded_sequence(x, seq_len_cpu, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "\n",
    "        # re-pad the sequence\n",
    "        lstm_out, output_lengths = pad_packed_sequence(packed_output, batch_first=True, total_length=x.size(1))\n",
    "\n",
    "        # Normalization and dropout\n",
    "        normed_output = self.layer_norm(lstm_out)\n",
    "        return self.dropout(normed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29c0d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The module based on MLP to decode the hidden state to the physical change.\n",
    "\n",
    "    Args:\n",
    "        hidden_nodes (int): The number of hidden nodes.\n",
    "        output_nodes (int): The number of output nodes.\n",
    "        num_layers (int): The number of layers.\n",
    "        num_nodes (int): The number of nodes in the hidden layers.\n",
    "        dropout (float): The dropout rate.\n",
    "    \n",
    "    Output:\n",
    "        physical_changes: [batch_size, seq_len, output_nodes] - [dVA, dVB, dNALA, dNAK, dNBK, nI]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_nodes, output_nodes, num_layers=2, num_nodes=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_nodes is None:\n",
    "            num_nodes = hidden_nodes\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input layer: hidden_nodes -> num_nodes\n",
    "        self.layers.append(nn.Linear(hidden_nodes, num_nodes))\n",
    "        self.layers.append(nn.LayerNorm(num_nodes))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # hidden layers: num_nodes -> num_nodes\n",
    "        for i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_nodes, num_nodes))\n",
    "            self.layers.append(nn.LayerNorm(num_nodes))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # output layer: num_nodes -> output_nodes\n",
    "        self.layers.append(nn.Linear(num_nodes, output_nodes))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Decode the hidden state to the physical change.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_nodes] - hidden state of each step\n",
    "\n",
    "        Returns:\n",
    "            physical_changes: [batch_size, seq_len, output_nodes] - [dVA, dVB, dNALA, dNAK, dNBK, nI]\n",
    "        \"\"\"\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24efe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The module based on MLP to apply the physical constraints to the physical changes.\n",
    "\n",
    "    Output:\n",
    "        new_state: [batch_size, seq_len, 12] - new state\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-1):\n",
    "        super().__init__()\n",
    "        self.eps = eps # prevent division by zero\n",
    "\n",
    "    def forward(self, physical_changes, current_state):\n",
    "        \"\"\"\n",
    "        Apply the physical constraints to the physical changes.\n",
    "\n",
    "        Args:\n",
    "            physical_changes: [batch_size, seq_len, 7] - physical changes\n",
    "            current_state: [batch_size, seq_len, 12] - current state\n",
    "\n",
    "        Returns:\n",
    "            new_state: [batch_size, seq_len, 12] - new state\n",
    "        \"\"\"\n",
    "        # check the input shape\n",
    "        if physical_changes.dim() != current_state.dim():\n",
    "            raise ValueError(f\"Dimension mismatch: physical_changes {physical_changes.shape} vs current_state {current_state.shape}\")\n",
    "        \n",
    "        if current_state.size(-1) != 12:\n",
    "            raise ValueError(f\"Expected 12 state features, got {current_state.size(-1)}\")\n",
    "        \n",
    "        if physical_changes.size(-1) != 7:\n",
    "            raise ValueError(f\"Expected 7 physical changes, got {physical_changes.size(-1)}\")\n",
    "\n",
    "        \n",
    "        # extract the current state variables (keep the dimension)\n",
    "        V = current_state[..., 0:1]     # Voltage (fixed)\n",
    "        E = current_state[..., 1:2]     # External electrolyte concentration (fixed)\n",
    "        VF = current_state[..., 2:3]    # Feed volume\n",
    "        VA = current_state[..., 3:4]    # Acid volume\n",
    "        VB = current_state[..., 4:5]    # Base volume\n",
    "        CFLA = current_state[..., 5:6]  # LA concentration in Feed tank\n",
    "        CALA = current_state[..., 6:7]  # LA concentration in Acid tank\n",
    "        CBLA = current_state[..., 7:8]  # LA concentration in Base tank\n",
    "        CFK = current_state[..., 8:9]   # K concentration in Feed tank\n",
    "        CAK = current_state[..., 9:10]  # K concentration in Acid tank\n",
    "        CBK = current_state[..., 10:11] # K concentration in Base tank\n",
    "        I = current_state[..., 11:12]   # Current\n",
    "\n",
    "        # calculate the mole of ion species\n",
    "        NFLA = CFLA * VF; NALA = CALA * VA; NBLA = CBLA * VB\n",
    "        NFK = CFK * VF; NAK = CAK * VA; NBK = CBK * VB\n",
    "\n",
    "        # calculate the physical changes\n",
    "        dVA = physical_changes[..., 0:1]    # Acid tank volume change (bidirectional)\n",
    "        dVB = physical_changes[..., 1:2]    # Base tank volume change (bidirectional)\n",
    "        dNALA = physical_changes[..., 2:3]  # LA change in Acid tank (unidirectional)\n",
    "        dNBLA = physical_changes[..., 3:4]  # LA change in Base tank (unidirectional)\n",
    "        dNAK = physical_changes[..., 4:5]   # K change in Acid tank (unidirectional)\n",
    "        dNBK = physical_changes[..., 5:6]   # K change in Base tank (unidirectional)\n",
    "        nI = physical_changes[..., 6:7]     # New current value\n",
    "        \n",
    "        # calculate the new volume\n",
    "        nVF = VF - dVA - dVB  # New Feed tank volume \n",
    "        nVA = VA + dVA        # New Acid tank volume\n",
    "        nVB = VB + dVB        # New Base tank volume\n",
    "\n",
    "        # limit the ion species changes (unidirectional flow only)\n",
    "        dNALA = torch.clamp(dNALA, min=0)\n",
    "        dNBLA = torch.clamp(dNBLA, min=0)\n",
    "        dNAK = torch.clamp(dNAK, min=0)\n",
    "        dNBK = torch.clamp(dNBK, min=0)\n",
    "\n",
    "        # calculate the new mole of ion species\n",
    "        nNFLA = NFLA - dNALA - dNBLA  # New LA mole in Feed tank\n",
    "        nNALA = NALA + dNALA         # New LA mole in Acid tank\n",
    "        nNBLA = NBLA + dNBLA         # New LA mole in Base tank\n",
    "        nNFK = NFK - dNAK - dNBK     # New K mole in Feed tank\n",
    "        nNAK = NAK + dNAK            # New K mole in Acid tank\n",
    "        nNBK = NBK + dNBK            # New K mole in Base tank\n",
    "\n",
    "        # limit the physical changes\n",
    "        nVF = torch.clamp(nVF, min=self.eps)\n",
    "        nVA = torch.clamp(nVA, min=self.eps)\n",
    "        nVB = torch.clamp(nVB, min=self.eps)\n",
    "        nNFLA = torch.clamp(nNFLA, min=0)\n",
    "        nNALA = torch.clamp(nNALA, min=0)\n",
    "        nNBLA = torch.clamp(nNBLA, min=0)\n",
    "        nNFK = torch.clamp(nNFK, min=0)\n",
    "        nNAK = torch.clamp(nNAK, min=0)\n",
    "        nNBK = torch.clamp(nNBK, min=0)\n",
    "        nI = torch.clamp(nI, min=0)\n",
    "\n",
    "        # calculate the new concentration\n",
    "        nCFLA = nNFLA / nVF  # New LA concentration in Feed tank\n",
    "        nCALA = nNALA / nVA  # New LA concentration in Acid tank\n",
    "        nCBLA = nNBLA / nVB  # New LA concentration in Base tank\n",
    "        nCFK = nNFK / nVF    # New K concentration in Feed tank\n",
    "        nCAK = nNAK / nVA    # New K concentration in Acid tank\n",
    "        nCBK = nNBK / nVB    # New K concentration in Base tank\n",
    "\n",
    "        # assemble the new state\n",
    "        new_state = torch.cat([\n",
    "            V, E,  # fixed: voltage, external electrolyte concentration\n",
    "            nVF, nVA, nVB,  # new volume\n",
    "            nCFLA, nCALA, nCBLA,  # new LA concentration\n",
    "            nCFK, nCAK, nCBK,     # new K concentration\n",
    "            nI  # new current\n",
    "        ], dim=-1)\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40224782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The autoregressive model to predict the state of BMED system.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_extractor_params, decoder_params):\n",
    "        super().__init__()\n",
    "        self.state_extractor = SequentialStateExtractor(**state_extractor_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer()\n",
    "\n",
    "    def forward(self, current_state, seq_lengths):\n",
    "        \"\"\"\n",
    "        Predict the next step from the all previous steps.\n",
    "\n",
    "        Args:\n",
    "            current_state: [batch_size, seq_len, 12] - current state\n",
    "            seq_lengths: [batch_size] - length of each sequence\n",
    "\n",
    "        Returns:\n",
    "            new_state: [batch_size, seq_len, 12] - new state\n",
    "        \"\"\"\n",
    "        # Extract the hidden state of each step using LSTM\n",
    "        hidden_states = self.state_extractor(current_state, seq_lengths)\n",
    "        # Decode the hidden state to the physical change\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        # Calculate the new state using physical constraints\n",
    "        new_state = self.physics_constraint(physical_changes, current_state)\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa444f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(pred, target, seq_len):\n",
    "    \"\"\"\n",
    "    Calculate the masked MSE loss for the autoregressive model.\n",
    "\n",
    "    Args:\n",
    "        pred: [batch_size, seq_len, 12] - predicted state\n",
    "        target: [batch_size, seq_len, 12] - target state\n",
    "        seq_len: [batch_size] - length of each sequence\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: average loss excluding the masked parts\n",
    "    \"\"\"\n",
    "    # check the input shape\n",
    "    if pred.shape != target.shape:\n",
    "        raise ValueError(f\"Shape mismatch: predictions {pred.shape} vs targets {target.shape}\")\n",
    "\n",
    "    if pred.size(0) != seq_len.size(0):\n",
    "        raise ValueError(f\"Batch size mismatch: predictions {pred.size(0)} vs sequence lengths {seq_len.size(0)}\")\n",
    "    \n",
    "    batch_size, max_len, features = pred.shape\n",
    "\n",
    "    # Move seq_len to CPU to be compatible with arange.\n",
    "    seq_len_cpu = seq_len.detach().cpu().long()\n",
    "\n",
    "    # Validation check on sequence lengths\n",
    "    if (seq_len_cpu <= 0).any():\n",
    "        invalid_lengths = seq_len_cpu[seq_len_cpu <= 0]\n",
    "        raise ValueError(f'Invalid sequence lengths detected: {invalid_lengths.tolist()}. All sequence lengths must be positive.')\n",
    "\n",
    "    # Check if any sequence length exceeds max_len\n",
    "    if (seq_len_cpu > max_len).any():\n",
    "        invalid_lengths = seq_len_cpu[seq_len_cpu > max_len]\n",
    "        raise ValueError(f'Sequence lengths exceed max_len: {invalid_lengths.tolist()} > {max_len}')\n",
    "\n",
    "    # Generate mask as long as the sequence length\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "    mask = mask.float().to(pred.device)\n",
    "\n",
    "    # Calculate the MSE of each feature\n",
    "    loss = F.mse_loss(pred, target, reduction='none')\n",
    "\n",
    "    # Apply the mask to exclude the masked parts\n",
    "    masked_loss_sum = (loss * mask.unsqueeze(-1)).sum()\n",
    "    valid_elements = mask.sum() * features\n",
    "\n",
    "    if valid_elements == 0:\n",
    "        raise ValueError('No valid elements found after masking. Check sequence lengths and data.')\n",
    "    \n",
    "    masked_loss = masked_loss_sum / valid_elements\n",
    "\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ae9fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_running_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Prepare the data for free running .\n",
    "\n",
    "    Args:\n",
    "        input_seq: [batch_size, seq_len, 12] - input sequences\n",
    "        seq_lengths: [batch_size] - length of each sequence\n",
    "\n",
    "    Returns:\n",
    "        init: [t0] initial state\n",
    "        targets: [t1, t2, ..., t_n] next states\n",
    "        target_seq_len: length of each target sequence\n",
    "    \"\"\"\n",
    "    # initial state\n",
    "    init = input_seq[:, 0, :]\n",
    "    # target states\n",
    "    targets = input_seq[:, 1:, :]\n",
    "    # length of each target sequence\n",
    "    if (seq_len - 1 < 1).any():\n",
    "        invalid_lengths = seq_len[seq_len - 1 < 1]\n",
    "        raise ValueError(f'The length of target sequence cannot be less than 1. Wrong seq_len: {invalid_lengths.tolist()}')\n",
    "    target_seq_len = seq_len - 1\n",
    "\n",
    "    return init, targets, target_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c09a5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_running_prediction(model, init, targets_shape, device, mode='eval'):\n",
    "    \"\"\"\n",
    "    Free running prediction using only initial state with different modes.\n",
    "    \n",
    "    Args:\n",
    "        model: BMEDAutoregressiveModel\n",
    "        initial_state: [batch_size, 12] - initial state\n",
    "        targets_shape: tuple - shape of targets to match (batch_size, seq_len, features)\n",
    "        device: computation device\n",
    "        mode: 'eval' (evaluation), 'train' (training), 'simulation' (pure inference)\n",
    "        \n",
    "    Returns:\n",
    "        predictions: [batch_size, targets_seq_len, 12] - predicted sequence\n",
    "    \"\"\"\n",
    "    # Set model mode\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "        context_manager = torch.enable_grad()\n",
    "    elif mode in ['eval', 'simulation']:\n",
    "        model.eval()\n",
    "        context_manager = torch.no_grad()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}. Choose from 'train', 'eval', 'simulation'\")\n",
    "    \n",
    "    batch_size = init.size(0)\n",
    "    num_steps = targets_shape[1]  # Use the actual targets sequence length\n",
    "    \n",
    "    # Initialize predictions with initial state\n",
    "    pred = [init.unsqueeze(1)]  # [batch_size, 1, 12]\n",
    "    current_state = init.unsqueeze(1)  # [batch_size, 1, 12]\n",
    "    \n",
    "    with context_manager:\n",
    "        for step in range(num_steps):\n",
    "            # Predict next state using current sequence\n",
    "            seq_len = torch.full((batch_size,), current_state.size(1), device=device)\n",
    "            next_state = model(current_state, seq_len)\n",
    "            \n",
    "            # Take the last predicted state\n",
    "            next_step = next_state[:, -1:, :]  # [batch_size, 1, 12]\n",
    "            pred.append(next_step)\n",
    "            \n",
    "            # Update current state sequence\n",
    "            current_state = torch.cat([current_state, next_step], dim=1)\n",
    "    \n",
    "    # Return all predictions except the initial state\n",
    "    return torch.cat(pred[1:], dim=1)  # [batch_size, num_steps, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8bd5a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_free_running(model, train_loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model using free running approach for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: BMEDAutoregressiveModel\n",
    "        train_loader: training data loader\n",
    "        optimizer: optimizer\n",
    "        device: computation device\n",
    "        \n",
    "    Returns:\n",
    "        float: average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (input_seq, seq_len) in enumerate(train_loader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        seq_len = seq_len.to(device)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare free running data\n",
    "        init, targets, target_seq_len = free_running_data(input_seq, seq_len)\n",
    "        \n",
    "        # Free running prediction in train mode\n",
    "        pred = free_running_prediction(\n",
    "            model, init, targets.shape, device, mode='train'\n",
    "        )\n",
    "        \n",
    "        # Calculate masked loss\n",
    "        loss = masked_mse_loss(pred, targets, target_seq_len)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.000001)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate_epoch_free_running(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validate the model using free running approach for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: BMEDAutoregressiveModel\n",
    "        val_loader: validation data loader\n",
    "        device: computation device\n",
    "        \n",
    "    Returns:\n",
    "        float: average validation loss for the epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_seq, seq_len) in enumerate(val_loader):\n",
    "            input_seq = input_seq.to(device)\n",
    "            seq_len = seq_len.to(device)\n",
    "            \n",
    "            # Prepare free running data\n",
    "            init, targets, target_seq_len = free_running_data(input_seq, seq_len)\n",
    "            \n",
    "            # Free running prediction in eval mode\n",
    "            pred = free_running_prediction(\n",
    "                model, init, targets.shape, device, mode='eval'\n",
    "            )\n",
    "            \n",
    "            # Calculate masked loss\n",
    "            loss = masked_mse_loss(pred, targets, target_seq_len)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "lmpjx6tmfq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_free_running_model(model, train_loader, val_loader, optimizer, scheduler, device, \n",
    "#                              num_epochs=200, patience=20, min_epochs=10):\n",
    "def train_free_running_model(model, train_loader, val_loader, optimizer, device, \n",
    "                             num_epochs=200, patience=20, min_epochs=10):\n",
    "    \"\"\"\n",
    "    Complete training loop for free running model.\n",
    "    \n",
    "    Args:\n",
    "        model: BMEDAutoregressiveModel\n",
    "        train_loader: training data loader\n",
    "        val_loader: validation data loader\n",
    "        optimizer: optimizer\n",
    "        scheduler: learning rate scheduler\n",
    "        device: computation device\n",
    "        num_epochs: maximum number of epochs\n",
    "        patience: early stopping patience\n",
    "        min_epochs: minimum epochs before early stopping\n",
    "        \n",
    "    Returns:\n",
    "        dict: training history and best model state\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "    best_total_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Starting Free Running Training...\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Max Epochs: {num_epochs}, Patience: {patience}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = train_epoch_free_running(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = validate_epoch_free_running(model, val_loader, device)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = train_loss + val_loss\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        # if scheduler:\n",
    "        #     scheduler.step(val_loss)\n",
    "        \n",
    "        # Record history\n",
    "        train_history.append(train_loss)\n",
    "        val_history.append(val_loss)\n",
    "        \n",
    "        # Early stopping check based on total loss\n",
    "        if total_loss < best_total_loss:\n",
    "            best_total_loss = total_loss\n",
    "            best_val_loss = val_loss\n",
    "            best_train_loss = train_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch < 10:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}, Total Loss = {total_loss:.6f} | Best: Train = {best_train_loss:.6f}, Val = {best_val_loss:.6f}, Total = {best_total_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch >= min_epochs and patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Best train loss: {best_train_loss:.6f}\")\n",
    "    print(f\"Best val loss: {best_val_loss:.6f}\")\n",
    "    print(f\"Best total loss: {best_total_loss:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_history': train_history,\n",
    "        'val_history': val_history,\n",
    "        'best_train_loss': best_train_loss,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_total_loss': best_total_loss,\n",
    "        'best_model_state': best_model_state,\n",
    "        'final_epoch': epoch + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b63c56fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Dataset created with 39 experiments\n",
      "Max sequence length: 37\n",
      "Experiment numbers: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35), np.int64(36), np.int64(37), np.int64(38)]\n",
      "Actual split:\n",
      "  Train: [1, np.int64(2), 3, np.int64(4), 5, 6, np.int64(7), np.int64(8), np.int64(9), 11, np.int64(12), np.int64(13), 15, np.int64(16), 17, np.int64(18), 19, 20, np.int64(21), np.int64(25), np.int64(26), np.int64(27), np.int64(29), np.int64(30), np.int64(32), np.int64(33), np.int64(35), np.int64(37), 40, 41, 42] (31 experiments)\n",
      "  Val: [np.int64(0), np.int64(14), np.int64(23), np.int64(31)] (4 experiments)\n",
      "  Test: [np.int64(10), np.int64(22), np.int64(24), np.int64(28), np.int64(34), np.int64(36), np.int64(38)] (7 experiments)\n",
      "\n",
      "Completed DataLoader creation:\n",
      "  Train: 28 sequences\n",
      "  Val: 4 sequences\n",
      "  Test: 7 sequences\n"
     ]
    }
   ],
   "source": [
    "# Load data and create dataloaders\n",
    "print(\"Loading and preprocessing data...\")\n",
    "ndf, exp_num_list = norm_data('BMED_DATA_AG.csv')\n",
    "sequences = seq_data_const(ndf)\n",
    "padded_seq, seq_len, max_seq_len = padded_sequences(sequences)\n",
    "dataset = gen_dataset(padded_seq, seq_len)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} experiments\")\n",
    "print(f\"Max sequence length: {max_seq_len}\")\n",
    "print(f\"Experiment numbers: {sorted(exp_num_list)}\")\n",
    "\n",
    "# Create train/val/test dataloaders with stratified split\n",
    "train_loader, val_loader, test_loader = dataloaders(dataset, exp_num_list, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "12e02d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n",
      "Model initialized with 655015 parameters\n",
      "Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and training setup\n",
    "device = set_device()\n",
    "\n",
    "study = optuna.load_study(study_name=\"bmed_autoregressive_optimization\", storage=\"sqlite:///bmed_optuna_study.db\")\n",
    "best_params = study.best_params\n",
    "\n",
    "# Model parameters\n",
    "state_extractor_params = {\n",
    "    'input_nodes': 12,\n",
    "    'hidden_nodes': best_params['hidden_size'],\n",
    "    #'num_layers': best_params['num_layers'],\n",
    "    'num_layers': 2,\n",
    "    'dropout': best_params['extractor_dropout']\n",
    "}\n",
    "\n",
    "decoder_params = {\n",
    "    'hidden_nodes': best_params['hidden_size'],\n",
    "    'output_nodes': 7,  # [dVA, dVB, dNALA, dNBLA, dNAK, dNBK, nI]\n",
    "    #'num_layers': best_params['decoder_layers'],\n",
    "    'num_layers': 2,\n",
    "    'num_nodes': best_params['decoder_nodes'],\n",
    "    'dropout': best_params['decoder_dropout']\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = BMEDAutoregressiveModel(state_extractor_params, decoder_params)\n",
    "model = model.to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6c41192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Free Running Training...\n",
      "Starting Free Running Training...\n",
      "Device: cuda\n",
      "Max Epochs: 10000, Patience: 1000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/10000: Train Loss = 90.282280, Val Loss = 1.800746, Total Loss = 92.083026 | Best: Train = 90.282280, Val = 1.800746, Total = 92.083026\n",
      "Epoch   2/10000: Train Loss = 1.104173, Val Loss = 0.205180, Total Loss = 1.309352 | Best: Train = 1.104173, Val = 0.205180, Total = 1.309352\n",
      "Epoch   3/10000: Train Loss = 0.523071, Val Loss = 0.182905, Total Loss = 0.705976 | Best: Train = 0.523071, Val = 0.182905, Total = 0.705976\n",
      "Epoch   4/10000: Train Loss = 0.326244, Val Loss = 0.013367, Total Loss = 0.339611 | Best: Train = 0.326244, Val = 0.013367, Total = 0.339611\n",
      "Epoch   5/10000: Train Loss = 0.242681, Val Loss = 0.044002, Total Loss = 0.286683 | Best: Train = 0.242681, Val = 0.044002, Total = 0.286683\n",
      "Epoch   6/10000: Train Loss = 0.206244, Val Loss = 0.021210, Total Loss = 0.227454 | Best: Train = 0.206244, Val = 0.021210, Total = 0.227454\n",
      "Epoch   7/10000: Train Loss = 0.170558, Val Loss = 0.014499, Total Loss = 0.185058 | Best: Train = 0.170558, Val = 0.014499, Total = 0.185058\n",
      "Epoch   8/10000: Train Loss = 0.157400, Val Loss = 0.179657, Total Loss = 0.337057 | Best: Train = 0.170558, Val = 0.014499, Total = 0.185058\n",
      "Epoch   9/10000: Train Loss = 0.248023, Val Loss = 0.065934, Total Loss = 0.313957 | Best: Train = 0.170558, Val = 0.014499, Total = 0.185058\n",
      "Epoch  10/10000: Train Loss = 0.162834, Val Loss = 0.021172, Total Loss = 0.184006 | Best: Train = 0.162834, Val = 0.021172, Total = 0.184006\n",
      "Epoch  20/10000: Train Loss = 0.077673, Val Loss = 0.011128, Total Loss = 0.088802 | Best: Train = 0.064471, Val = 0.011609, Total = 0.076080\n",
      "Epoch  30/10000: Train Loss = 0.037746, Val Loss = 0.011410, Total Loss = 0.049156 | Best: Train = 0.037746, Val = 0.011410, Total = 0.049156\n",
      "Epoch  40/10000: Train Loss = 0.038466, Val Loss = 0.018630, Total Loss = 0.057097 | Best: Train = 0.030577, Val = 0.009992, Total = 0.040570\n",
      "Epoch  50/10000: Train Loss = 0.029245, Val Loss = 0.013032, Total Loss = 0.042277 | Best: Train = 0.028206, Val = 0.010741, Total = 0.038946\n",
      "Epoch  60/10000: Train Loss = 0.026151, Val Loss = 0.011407, Total Loss = 0.037558 | Best: Train = 0.024668, Val = 0.010664, Total = 0.035332\n",
      "Epoch  70/10000: Train Loss = 0.024542, Val Loss = 0.011768, Total Loss = 0.036310 | Best: Train = 0.023233, Val = 0.010548, Total = 0.033781\n",
      "Epoch  80/10000: Train Loss = 0.025873, Val Loss = 0.011605, Total Loss = 0.037478 | Best: Train = 0.023233, Val = 0.010548, Total = 0.033781\n",
      "Epoch  90/10000: Train Loss = 0.021999, Val Loss = 0.009432, Total Loss = 0.031431 | Best: Train = 0.021999, Val = 0.009432, Total = 0.031431\n",
      "Epoch 100/10000: Train Loss = 0.022752, Val Loss = 0.016258, Total Loss = 0.039010 | Best: Train = 0.020464, Val = 0.010812, Total = 0.031276\n",
      "Epoch 110/10000: Train Loss = 0.025193, Val Loss = 0.010897, Total Loss = 0.036091 | Best: Train = 0.019899, Val = 0.009526, Total = 0.029425\n",
      "Epoch 120/10000: Train Loss = 0.019313, Val Loss = 0.010224, Total Loss = 0.029537 | Best: Train = 0.019860, Val = 0.009511, Total = 0.029371\n",
      "Epoch 130/10000: Train Loss = 0.021937, Val Loss = 0.009549, Total Loss = 0.031486 | Best: Train = 0.019576, Val = 0.009528, Total = 0.029104\n",
      "Epoch 140/10000: Train Loss = 0.021318, Val Loss = 0.010358, Total Loss = 0.031675 | Best: Train = 0.018668, Val = 0.009490, Total = 0.028158\n",
      "Epoch 150/10000: Train Loss = 0.020054, Val Loss = 0.011881, Total Loss = 0.031935 | Best: Train = 0.017930, Val = 0.009584, Total = 0.027514\n",
      "Epoch 160/10000: Train Loss = 0.018630, Val Loss = 0.011378, Total Loss = 0.030007 | Best: Train = 0.017930, Val = 0.009584, Total = 0.027514\n",
      "Epoch 170/10000: Train Loss = 0.018576, Val Loss = 0.009631, Total Loss = 0.028208 | Best: Train = 0.017462, Val = 0.009365, Total = 0.026828\n",
      "Epoch 180/10000: Train Loss = 0.018265, Val Loss = 0.010372, Total Loss = 0.028638 | Best: Train = 0.017354, Val = 0.009385, Total = 0.026739\n",
      "Epoch 190/10000: Train Loss = 0.018076, Val Loss = 0.012314, Total Loss = 0.030389 | Best: Train = 0.017354, Val = 0.009385, Total = 0.026739\n",
      "Epoch 200/10000: Train Loss = 0.018227, Val Loss = 0.010979, Total Loss = 0.029205 | Best: Train = 0.017354, Val = 0.009385, Total = 0.026739\n",
      "Epoch 210/10000: Train Loss = 0.018222, Val Loss = 0.009381, Total Loss = 0.027603 | Best: Train = 0.017354, Val = 0.009385, Total = 0.026739\n",
      "Epoch 220/10000: Train Loss = 0.018239, Val Loss = 0.010736, Total Loss = 0.028975 | Best: Train = 0.017354, Val = 0.009385, Total = 0.026739\n",
      "Epoch 230/10000: Train Loss = 0.017438, Val Loss = 0.009543, Total Loss = 0.026981 | Best: Train = 0.017354, Val = 0.009385, Total = 0.026739\n",
      "Epoch 240/10000: Train Loss = 0.017115, Val Loss = 0.009718, Total Loss = 0.026833 | Best: Train = 0.017030, Val = 0.009485, Total = 0.026515\n",
      "Epoch 250/10000: Train Loss = 0.018920, Val Loss = 0.011092, Total Loss = 0.030012 | Best: Train = 0.016799, Val = 0.009371, Total = 0.026170\n",
      "Epoch 260/10000: Train Loss = 0.018129, Val Loss = 0.009870, Total Loss = 0.027999 | Best: Train = 0.016799, Val = 0.009371, Total = 0.026170\n",
      "Epoch 270/10000: Train Loss = 0.017270, Val Loss = 0.009569, Total Loss = 0.026839 | Best: Train = 0.016799, Val = 0.009371, Total = 0.026170\n",
      "Epoch 280/10000: Train Loss = 0.016834, Val Loss = 0.010458, Total Loss = 0.027292 | Best: Train = 0.016799, Val = 0.009371, Total = 0.026170\n",
      "Epoch 290/10000: Train Loss = 0.017078, Val Loss = 0.010771, Total Loss = 0.027848 | Best: Train = 0.016799, Val = 0.009371, Total = 0.026170\n",
      "Epoch 300/10000: Train Loss = 0.018297, Val Loss = 0.010300, Total Loss = 0.028598 | Best: Train = 0.016799, Val = 0.009371, Total = 0.026170\n",
      "Epoch 310/10000: Train Loss = 0.017885, Val Loss = 0.010211, Total Loss = 0.028096 | Best: Train = 0.016717, Val = 0.009111, Total = 0.025828\n",
      "Epoch 320/10000: Train Loss = 0.016760, Val Loss = 0.009730, Total Loss = 0.026491 | Best: Train = 0.016717, Val = 0.009111, Total = 0.025828\n",
      "Epoch 330/10000: Train Loss = 0.017302, Val Loss = 0.010766, Total Loss = 0.028068 | Best: Train = 0.016306, Val = 0.009439, Total = 0.025745\n",
      "Epoch 340/10000: Train Loss = 0.017794, Val Loss = 0.009532, Total Loss = 0.027326 | Best: Train = 0.016306, Val = 0.009439, Total = 0.025745\n",
      "Epoch 350/10000: Train Loss = 0.016693, Val Loss = 0.009441, Total Loss = 0.026134 | Best: Train = 0.016306, Val = 0.009439, Total = 0.025745\n",
      "Epoch 360/10000: Train Loss = 0.016333, Val Loss = 0.009554, Total Loss = 0.025887 | Best: Train = 0.016306, Val = 0.009439, Total = 0.025745\n",
      "Epoch 370/10000: Train Loss = 0.017136, Val Loss = 0.009998, Total Loss = 0.027135 | Best: Train = 0.016535, Val = 0.009146, Total = 0.025682\n",
      "Epoch 380/10000: Train Loss = 0.016854, Val Loss = 0.010195, Total Loss = 0.027049 | Best: Train = 0.016068, Val = 0.009564, Total = 0.025632\n",
      "Epoch 390/10000: Train Loss = 0.016528, Val Loss = 0.009424, Total Loss = 0.025952 | Best: Train = 0.016336, Val = 0.009241, Total = 0.025577\n",
      "Epoch 400/10000: Train Loss = 0.016811, Val Loss = 0.009714, Total Loss = 0.026525 | Best: Train = 0.016137, Val = 0.009402, Total = 0.025540\n",
      "Epoch 410/10000: Train Loss = 0.018071, Val Loss = 0.009625, Total Loss = 0.027697 | Best: Train = 0.016137, Val = 0.009402, Total = 0.025540\n",
      "Epoch 420/10000: Train Loss = 0.017623, Val Loss = 0.009630, Total Loss = 0.027253 | Best: Train = 0.016137, Val = 0.009402, Total = 0.025540\n",
      "Epoch 430/10000: Train Loss = 0.017400, Val Loss = 0.009385, Total Loss = 0.026785 | Best: Train = 0.016137, Val = 0.009402, Total = 0.025540\n",
      "Epoch 440/10000: Train Loss = 0.016761, Val Loss = 0.009190, Total Loss = 0.025951 | Best: Train = 0.015908, Val = 0.009587, Total = 0.025495\n",
      "Epoch 450/10000: Train Loss = 0.016749, Val Loss = 0.009652, Total Loss = 0.026401 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n",
      "Epoch 460/10000: Train Loss = 0.016497, Val Loss = 0.011035, Total Loss = 0.027532 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n",
      "Epoch 470/10000: Train Loss = 0.016878, Val Loss = 0.009278, Total Loss = 0.026156 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n",
      "Epoch 480/10000: Train Loss = 0.016335, Val Loss = 0.009067, Total Loss = 0.025402 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n",
      "Epoch 490/10000: Train Loss = 0.017506, Val Loss = 0.009556, Total Loss = 0.027062 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n",
      "Epoch 500/10000: Train Loss = 0.016627, Val Loss = 0.009427, Total Loss = 0.026054 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n",
      "Epoch 510/10000: Train Loss = 0.016297, Val Loss = 0.009396, Total Loss = 0.025693 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n",
      "Epoch 520/10000: Train Loss = 0.016910, Val Loss = 0.009673, Total Loss = 0.026584 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n",
      "Epoch 530/10000: Train Loss = 0.016369, Val Loss = 0.009259, Total Loss = 0.025627 | Best: Train = 0.015837, Val = 0.009011, Total = 0.024848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Start free running training\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🚀 Starting Free Running Training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m training_results = \u001b[43mtrain_free_running_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#scheduler=scheduler,\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_results[\u001b[33m'\u001b[39m\u001b[33mbest_model_state\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain_free_running_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, patience, min_epochs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     train_loss = \u001b[43mtrain_epoch_free_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m     40\u001b[39m     val_loss = validate_epoch_free_running(model, val_loader, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_epoch_free_running\u001b[39m\u001b[34m(model, train_loader, optimizer, device)\u001b[39m\n\u001b[32m     26\u001b[39m init, targets, target_seq_len = free_running_data(input_seq, seq_len)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Free running prediction in train mode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m pred = \u001b[43mfree_running_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Calculate masked loss\u001b[39;00m\n\u001b[32m     34\u001b[39m loss = masked_mse_loss(pred, targets, target_seq_len)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mfree_running_prediction\u001b[39m\u001b[34m(model, init, targets_shape, device, mode)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Predict next state using current sequence\u001b[39;00m\n\u001b[32m     35\u001b[39m     seq_len = torch.full((batch_size,), current_state.size(\u001b[32m1\u001b[39m), device=device)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     next_state = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Take the last predicted state\u001b[39;00m\n\u001b[32m     39\u001b[39m     next_step = next_state[:, -\u001b[32m1\u001b[39m:, :]  \u001b[38;5;66;03m# [batch_size, 1, 12]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mBMEDAutoregressiveModel.forward\u001b[39m\u001b[34m(self, current_state, seq_lengths)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03mPredict the next step from the all previous steps.\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[33;03m    new_state: [batch_size, seq_len, 12] - new state\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Extract the hidden state of each step using LSTM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Decode the hidden state to the physical change\u001b[39;00m\n\u001b[32m     25\u001b[39m physical_changes = \u001b[38;5;28mself\u001b[39m.physical_decoder(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mSequentialStateExtractor.forward\u001b[39m\u001b[34m(self, x, seq_len)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInvalid sequence lengths detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_lengths.tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. All sequence lengths mut be positive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# pack the padded sequence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m packed_input = \u001b[43mpack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len_cpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_sorted\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m packed_output, (hidden, cell) = \u001b[38;5;28mself\u001b[39m.lstm(packed_input)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# re-pad the sequence\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/nn/utils/rnn.py:336\u001b[39m, in \u001b[36mpack_padded_sequence\u001b[39m\u001b[34m(input, lengths, batch_first, enforce_sorted)\u001b[39m\n\u001b[32m    334\u001b[39m     sorted_indices = sorted_indices.to(\u001b[38;5;28minput\u001b[39m.device)\n\u001b[32m    335\u001b[39m     batch_dim = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m data, batch_sizes = _VF._pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first)\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Start free running training\n",
    "print(\"\\n🚀 Starting Free Running Training...\")\n",
    "training_results = train_free_running_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    #scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=10000,\n",
    "    patience=1000,\n",
    "    min_epochs=1000\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "if training_results['best_model_state'] is not None:\n",
    "    model.load_state_dict(training_results['best_model_state'])\n",
    "    print(\"✅ Best model loaded!\")\n",
    "else:\n",
    "    print(\"⚠️ No best model found, using current state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0a22bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model이 'bmed_batch_best_model.pth' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# best model 저장\n",
    "import torch\n",
    "\n",
    "best_model_path = \"bmed_batch_best_model.pth\"\n",
    "torch.save(model.state_dict(), best_model_path)\n",
    "print(f\"✅ Best model이 '{best_model_path}' 파일에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
