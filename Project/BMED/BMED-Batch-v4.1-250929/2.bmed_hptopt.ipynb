{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9342def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "from optuna.trial import TrialState\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bbeb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with layer normalization\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    def __init__(self, input_node, hidden_node):\n",
    "        super().__init__()\n",
    "        self.input_node = input_node\n",
    "        self.hidden_node = hidden_node\n",
    "\n",
    "        self.w_i = nn.Linear(input_node, 4*hidden_node, bias=False)\n",
    "        self.w_h = nn.Linear(hidden_node, 4*hidden_node, bias=False)\n",
    "\n",
    "        self.ln_i = nn.LayerNorm(hidden_node)\n",
    "        self.ln_f = nn.LayerNorm(hidden_node)\n",
    "        self.ln_w = nn.LayerNorm(hidden_node)\n",
    "        self.ln_o = nn.LayerNorm(hidden_node)\n",
    "        self.ln_c = nn.LayerNorm(hidden_node)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        gi = self.w_i(input)\n",
    "        gh = self.w_h(h_prev)\n",
    "        i_i, i_f, i_w, i_o = gi.chunk(4, dim=-1)\n",
    "        h_i, h_f, h_w, h_o = gh.chunk(4, dim=-1)\n",
    "\n",
    "        i_g = torch.sigmoid(self.ln_i(i_i + h_i))\n",
    "        f_g = torch.sigmoid(self.ln_f(i_f + h_f))\n",
    "        w_g = torch.tanh(self.ln_w(i_w + h_w))\n",
    "        o_g = torch.sigmoid(self.ln_o(i_o + h_o))\n",
    "        \n",
    "\n",
    "        c_new = f_g * c_prev + i_g * w_g\n",
    "        c_new = self.ln_c(c_new)\n",
    "\n",
    "        h_new = o_g * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8de49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State feature extractor using LayerNorm LSTM\n",
    "class StateExtr(nn.Module):\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for i in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        batch_size, max_len, _ = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "\n",
    "        for _ in range(self.n_layer):\n",
    "            h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "            c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            layer_input = x_t # initialize layer input with input tensor\n",
    "            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "                h_new, c_new = lstm_cell(layer_input, (h_states[layer_idx], c_states[layer_idx]))\n",
    "                \n",
    "                h_states[layer_idx] = h_new\n",
    "                c_states[layer_idx] = c_new\n",
    "\n",
    "                if layer_idx < len(self.lstm_cells) - 1:\n",
    "                    layer_input = self.dropout(h_new)\n",
    "                else:\n",
    "                    layer_input = h_new\n",
    "\n",
    "            outputs.append(layer_input)\n",
    "\n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        return self.dropout(masked_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adf624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical change regressor\n",
    "class PhysRegr(nn.Module):\n",
    "    def __init__(self, input_node, output_node, n_layer, hidden_node, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.extend([\n",
    "            nn.Linear(input_node, hidden_node),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_node, hidden_node),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_node, output_node))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.layers(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564cd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current regressor\n",
    "class CurrRegr(nn.Module):\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.extend([\n",
    "            nn.Linear(input_node, hidden_node),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_node, hidden_node),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_node, 1))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.layers(hidden_states)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f65d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical Constraint Layer\n",
    "class PhysConstr(nn.Module):\n",
    "    def __init__(self, range_mm, curr_regr, eps=1e-2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.curr_regr = curr_regr\n",
    "        self.register_buffer('range_mm_tensor',self._range2tensor(range_mm))\n",
    "\n",
    "    def _range2tensor(self, range_mm):\n",
    "        feature_names = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "        ranges = torch.zeros(len(feature_names), 2)\n",
    "\n",
    "        for i, name in enumerate(feature_names):\n",
    "            ranges[i, 0] = range_mm[name]['min']\n",
    "            ranges[i, 1] = range_mm[name]['max']\n",
    "\n",
    "        return ranges\n",
    "\n",
    "    def _norm_tensor(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "    def _denorm_tensor(self, norm_data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return norm_data * (max_val - min_val) + min_val\n",
    "\n",
    "    def forward(self, physical_changes, current_state):\n",
    "        V_idx, E_idx, VF_idx, VA_idx, VB_idx = 0, 1, 2, 3, 4\n",
    "        CFLA_idx, CALA_idx, CFK_idx, CBK_idx, I_idx = 5, 6, 7, 8, 9\n",
    "\n",
    "        VF = self._denorm_tensor(current_state[..., 2:3], VF_idx)\n",
    "        VA = self._denorm_tensor(current_state[..., 3:4], VA_idx)\n",
    "        VB = self._denorm_tensor(current_state[..., 4:5], VB_idx)\n",
    "        CFLA = self._denorm_tensor(current_state[..., 5:6], CFLA_idx)\n",
    "        CALA = self._denorm_tensor(current_state[..., 6:7], CALA_idx)\n",
    "        CFK = self._denorm_tensor(current_state[..., 7:8], CFK_idx)\n",
    "        CBK = self._denorm_tensor(current_state[..., 8:9], CBK_idx)\n",
    "\n",
    "        NFLA = CFLA * VF\n",
    "        NALA = CALA * VA\n",
    "        NFK = CFK * VF\n",
    "        NBK = CBK * VB\n",
    "\n",
    "        rdVA = physical_changes[..., 0:1]\n",
    "        rdVB = physical_changes[..., 1:2]\n",
    "        rLA2K = physical_changes[..., 2:3]\n",
    "        rdNBK = physical_changes[..., 3:4]\n",
    "\n",
    "        dVA = VF*(rdVA -0.5)\n",
    "        dVB = VF*(rdVB - 0.5)\n",
    "        dNBK = NFK*rdNBK\n",
    "        dNALA = dNBK * rLA2K\n",
    "\n",
    "        nVF = VF - dVA - dVB\n",
    "        nVA = VA + dVA\n",
    "        nVB = VB + dVB\n",
    "        \n",
    "        nNFLA = NFLA - dNALA\n",
    "        nNALA = NALA + dNALA\n",
    "        nNFK = NFK - dNBK\n",
    "        nNBK = NBK + dNBK\n",
    "\n",
    "        nCFLA = nNFLA / nVF\n",
    "        nCALA = nNALA / nVA\n",
    "        nCFK = nNFK / nVF\n",
    "        nCBK = nNBK / nVB\n",
    "\n",
    "        V = current_state[..., 0:1]\n",
    "        E = current_state[..., 1:2]\n",
    "        nVF_norm = self._norm_tensor(nVF, VF_idx)\n",
    "        nVA_norm = self._norm_tensor(nVA, VA_idx)\n",
    "        nVB_norm = self._norm_tensor(nVB, VB_idx)\n",
    "        nCFLA_norm = self._norm_tensor(nCFLA, CFLA_idx)\n",
    "        nCALA_norm = self._norm_tensor(nCALA, CALA_idx)\n",
    "        nCFK_norm = self._norm_tensor(nCFK, CFK_idx)\n",
    "        nCBK_norm = self._norm_tensor(nCBK, CBK_idx)\n",
    "\n",
    "        temp_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        nI_pred_norm = self.curr_regr(temp_state)\n",
    "        nI_real = self._denorm_tensor(nI_pred_norm, I_idx)\n",
    "        nI_real = torch.clamp(nI_real, min=0.0)\n",
    "        nI_norm = self._norm_tensor(nI_real, I_idx)\n",
    "\n",
    "        next_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm, nI_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77ec51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    def __init__(self, state_extr_params, phys_regr_params, curr_regr_params, range_mm):\n",
    "        super().__init__()\n",
    "        self.state_extr = StateExtr(**state_extr_params)\n",
    "        self.phys_regr = PhysRegr(**phys_regr_params)\n",
    "        self.curr_regr = CurrRegr(**curr_regr_params)\n",
    "        self.phys_constr = PhysConstr(range_mm, self.curr_regr)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        hidden_states = self.state_extr(x, seq_len)\n",
    "        physical_changes = self.phys_regr(hidden_states)\n",
    "        new_x = self.phys_constr(physical_changes, x)\n",
    "        return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3917c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, model_size, warmup_epochs, factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_size = model_size\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.factor = 1\n",
    "        self.epoch_num = 0\n",
    "\n",
    "    def step_epoch(self):\n",
    "        self.epoch_num += 1\n",
    "        lr = self.factor * (\n",
    "            self.model_size ** (-0.5) *\n",
    "            min(self.epoch_num ** (-0.5), self.epoch_num * self.warmup_epochs ** (-1.5))\n",
    "        )\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b1c08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유틸리티 함수들\n",
    "def df_treat(name):\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "\n",
    "    exp_num_list = sorted(ndf['exp'].unique())\n",
    "    return df, ndf, range_mm, exp_num_list\n",
    "\n",
    "def seq_data(ndf, exp_num_list):\n",
    "    seq = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "    for exp in exp_num_list:\n",
    "        exp_df = ndf[ndf['exp'] == exp]\n",
    "        seq.append(exp_df[feature_cols].values)\n",
    "    return seq\n",
    "\n",
    "def pad_seq(seq):\n",
    "    max_len = max([len(s) for s in seq])\n",
    "    seq_len = [len(s) for s in seq]\n",
    "    pad_seq = pad_sequence([torch.tensor(s) for s in seq], batch_first=True, padding_value=-1)\n",
    "    return pad_seq, seq_len, max_len\n",
    "\n",
    "def gen_dataset(pad_seq, seq_len):\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset\n",
    "\n",
    "def masked_mse_loss(pred, target, seq_len):\n",
    "    batch_size, max_len, features = pred.shape\n",
    "    seq_len_cpu = seq_len.detach().cpu().long()\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "    mask = mask.float().to(pred.device)\n",
    "    loss = F.mse_loss(pred, target, reduction='none')\n",
    "    masked_loss = loss * mask.unsqueeze(-1)\n",
    "    total_loss = masked_loss.sum()\n",
    "    total_elements = mask.sum()\n",
    "    masked_loss = total_loss / total_elements\n",
    "    return masked_loss\n",
    "\n",
    "def tf_data(input_seq, seq_len):\n",
    "    inputs = input_seq[:, :-1, :-1]\n",
    "    targets = input_seq[:, 1:, :]\n",
    "    target_seq_len = seq_len - 1\n",
    "    return inputs, targets, target_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816cee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna 목적 함수\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna trial을 위한 목적 함수\n",
    "    K-fold cross validation을 사용하여 하이퍼파라미터 최적화\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 하이퍼파라미터 제안\n",
    "    # LSTM StateExtractor 파라미터\n",
    "    lstm_hidden_size = trial.suggest_categorical('lstm_hidden_size', [16, 32, 48, 64, 72, 96])\n",
    "    lstm_n_layers = trial.suggest_int('lstm_n_layers', 2, 6, step=1)\n",
    "    lstm_dropout = trial.suggest_float('lstm_dropout', 0.1, 0.5, step=0.1)\n",
    "    \n",
    "    # PhysicalChangeDecoder 파라미터\n",
    "    decoder_hidden_size = trial.suggest_categorical('decoder_hidden_size', [16, 32, 48, 64, 72, 96])\n",
    "    decoder_n_layers = trial.suggest_int('decoder_n_layers', 2, 6, step=1)\n",
    "    decoder_dropout = trial.suggest_float('decoder_dropout', 0.1, 0.6, step=0.1)\n",
    "    \n",
    "    # CurrentPredictor 파라미터\n",
    "    current_hidden_size = trial.suggest_categorical('current_hidden_size', [16, 32, 48, 64, 72, 96])\n",
    "    current_n_layers = trial.suggest_int('current_n_layers', 2, 6, step=1)\n",
    "    current_dropout = trial.suggest_float('current_dropout', 0.1, 0.6, step=0.1)\n",
    "    \n",
    "    # 2. K-fold Cross Validation\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    n_splits = 5\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_losses = []\n",
    "    \n",
    "    # 데이터 로드 (global 변수 사용)\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
    "        print(f\"  🔄 Trial {trial.number}, Fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # 폴드별 데이터셋 준비\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=3, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=3, shuffle=False)\n",
    "        \n",
    "        # 3. 모델 파라미터 설정\n",
    "        state_extr_params = {\n",
    "            'input_node': 9,\n",
    "            'hidden_node': lstm_hidden_size,\n",
    "            'n_layer': lstm_n_layers,\n",
    "            'dropout': lstm_dropout\n",
    "        }\n",
    "        \n",
    "        phys_regr_params = {\n",
    "            'input_node': lstm_hidden_size,\n",
    "            'hidden_node': decoder_hidden_size,\n",
    "            'n_layer': decoder_n_layers,\n",
    "            'dropout': decoder_dropout,\n",
    "            'output_node': 4\n",
    "        }\n",
    "        \n",
    "        curr_regr_params = {\n",
    "            'input_node': 9,\n",
    "            'hidden_node': current_hidden_size,\n",
    "            'n_layer': current_n_layers,\n",
    "            'dropout': current_dropout\n",
    "        }\n",
    "        \n",
    "        # 4. 모델 초기화\n",
    "        model = BMEDAutoregressiveModel(state_extr_params, phys_regr_params, curr_regr_params, range_mm)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 5. 옵티마이저 및 스케줄러 설정\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1.0)\n",
    "        \n",
    "        # 총 에포크 수와 warmup 에포크 계산\n",
    "        total_epochs = 100  # Optuna 최적화를 위해 에포크 수 감소\n",
    "        warmup_epochs = int(total_epochs * 0.1)\n",
    "        \n",
    "        scheduler = NoamScheduler(\n",
    "            optimizer, \n",
    "            model_size=lstm_hidden_size,\n",
    "            warmup_epochs=warmup_epochs,\n",
    "            factor=1\n",
    "        )\n",
    "        \n",
    "        # 6. 훈련\n",
    "        best_total_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(total_epochs):\n",
    "            # Learning rate 업데이트\n",
    "            current_lr = scheduler.step_epoch()\n",
    "            \n",
    "            # 훈련\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for input_seq, seq_len in train_loader:\n",
    "                try:\n",
    "                    input_seq = input_seq.to(device)\n",
    "                    seq_len = seq_len.to(device)\n",
    "                    \n",
    "                    inputs, targets, target_seq_len = tf_data(input_seq, seq_len)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(inputs, target_seq_len)\n",
    "                    loss = masked_mse_loss(pred, targets, target_seq_len)\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    train_batches += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error in training: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if train_batches == 0:\n",
    "                break\n",
    "                \n",
    "            train_loss = train_loss / train_batches\n",
    "            \n",
    "            # 검증\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for input_seq, seq_len in val_loader:\n",
    "                    try:\n",
    "                        input_seq = input_seq.to(device)\n",
    "                        seq_len = seq_len.to(device)\n",
    "                        \n",
    "                        inputs, targets, target_seq_len = tf_data(input_seq, seq_len)\n",
    "                        \n",
    "                        pred = model(inputs, target_seq_len)\n",
    "                        loss = masked_mse_loss(pred, targets, target_seq_len)\n",
    "                        \n",
    "                        val_loss += loss.item()\n",
    "                        val_batches += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            \n",
    "            if val_batches == 0:\n",
    "                break\n",
    "                \n",
    "            val_loss = val_loss / val_batches\n",
    "            \n",
    "            # Calculate total loss\n",
    "            total_loss = train_loss + val_loss\n",
    "            \n",
    "            # Early stopping\n",
    "            if total_loss < best_total_loss:\n",
    "                best_total_loss = total_loss\n",
    "        \n",
    "        fold_losses.append(best_total_loss)\n",
    "        print(f\"    Fold {fold+1} best total loss: {best_total_loss:.6f}\")\n",
    "        \n",
    "        # 메모리 정리\n",
    "        del model, optimizer, scheduler\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # 7. K-fold 평균 손실 반환\n",
    "    avg_loss = np.mean(fold_losses)\n",
    "    std_loss = np.std(fold_losses)\n",
    "    \n",
    "    print(f\"  📊 Trial {trial.number} - Average CV Loss: {avg_loss:.6f} (±{std_loss:.6f})\")\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17421f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-09 23:09:47,242] A new study created in RDB with name: bmed_tf_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 BMED TF Model Hyperparameter Optimization with Optuna\n",
      "================================================================================\n",
      "📋 데이터 로드 중...\n",
      "   - 총 실험 개수: 15\n",
      "   - 총 데이터 포인트: 15\n",
      "   - 최대 시퀀스 길이: 37\n",
      "🔍 최적화 시작 (총 100 trials)\n",
      "  🔄 Trial 0, Fold 1/5\n"
     ]
    }
   ],
   "source": [
    "# 메인 최적화 함수\n",
    "def run_optuna_optimization():\n",
    "    \"\"\"Optuna를 사용한 하이퍼파라미터 최적화 실행\"\"\"\n",
    "    \n",
    "    print(\"🚀 BMED TF Model Hyperparameter Optimization with Optuna\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 전역 데이터 로드\n",
    "    global dataset, range_mm\n",
    "    \n",
    "    print(\"📋 데이터 로드 중...\")\n",
    "    df, ndf, range_mm, exp_num_list = df_treat('BMED_DATA_AG.csv')\n",
    "    seq = seq_data(ndf, exp_num_list)\n",
    "    pad, seq_len, max_len = pad_seq(seq)\n",
    "    dataset = gen_dataset(pad, seq_len)\n",
    "    \n",
    "    print(f\"   - 총 실험 개수: {len(exp_num_list)}\")\n",
    "    print(f\"   - 총 데이터 포인트: {len(dataset)}\")\n",
    "    print(f\"   - 최대 시퀀스 길이: {max_len}\")\n",
    "    \n",
    "    # SQLite 데이터베이스를 사용한 Optuna study 생성\n",
    "    #timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    timestamp = '20250915_234452'\n",
    "    db_url = f\"sqlite:///bmed_hpopt_study.db\"\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name='bmed_tf_optimization',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        storage=db_url,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    # 최적화 실행\n",
    "    n_trials = 100\n",
    "    print(f\"🔍 최적화 시작 (총 {n_trials} trials)\")\n",
    "    \n",
    "    try:\n",
    "        study.optimize(objective, n_trials=n_trials, timeout=None)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ 최적화가 사용자에 의해 중단되었습니다.\")\n",
    "    \n",
    "    # 결과 분석\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 OPTIMIZATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"✅ 완료된 trials: {len(study.trials)}\")\n",
    "    print(f\"🏆 최고 성능 trial: {study.best_trial.number}\")\n",
    "    print(f\"💯 최고 성능 값: {study.best_value:.6f}\")\n",
    "    \n",
    "    print(f\"\\n🎯 최적 하이퍼파라미터:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # 상위 5개 trial 정보\n",
    "    print(f\"\\n📈 상위 5개 Trials:\")\n",
    "    trials_df = study.trials_dataframe().sort_values('value').head(5)\n",
    "    for idx, (_, trial) in enumerate(trials_df.iterrows()):\n",
    "        print(f\"   {idx+1}. Trial {int(trial['number'])}: {trial['value']:.6f}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    result_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Trials 결과 CSV로 저장\n",
    "    trials_file = f\"bmed_optuna_trials_{result_timestamp}.csv\"\n",
    "    trials_df = study.trials_dataframe()\n",
    "    trials_df.to_csv(trials_file, index=False)\n",
    "    print(f\"💾 모든 trials 결과가 저장되었습니다: {trials_file}\")\n",
    "    \n",
    "    # SQLite 데이터베이스 정보\n",
    "    print(f\"💾 SQLite 데이터베이스에 실시간 저장됨: {db_url}\")\n",
    "    print(f\"   - 중단 후 재시작 시 자동으로 기존 결과를 불러옵니다\")\n",
    "    print(f\"   - 다른 프로세스에서 진행상황 모니터링 가능합니다\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"🎉 하이퍼파라미터 최적화 완료!\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = run_optuna_optimization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
