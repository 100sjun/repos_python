{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Using device: {device}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"BMED_DB_augmented.csv\")\n",
    "df = df[df['exp'].isin([0])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust min-max scaling including safety margin\n",
    "ranges ={\n",
    "'V' : {'min':0, 'max':50},\n",
    "'E' : {'min':0, 'max':1},\n",
    "'VF' : {'min':0, 'max':2},\n",
    "'VA' : {'min':0, 'max':2},\n",
    "'VB' : {'min':0, 'max':8},\n",
    "'CF_LA' : {'min':-1, 'max':4},\n",
    "'CA_LA' : {'min':-1, 'max':4},\n",
    "'CF_K' : {'min':-1, 'max':7},\n",
    "'CB_K' : {'min':-1, 'max':2},\n",
    "'I' : {'min':0, 'max':5},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "ndf = pd.DataFrame()\n",
    "ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "for col in ['V', 'E', 'VF', 'VA', 'VB', 'CF_LA', 'CA_LA', 'CF_K', 'CB_K', 'I']:\n",
    "    if col in ranges:\n",
    "        ndf[col] = (df[col] - ranges[col]['min'])/(ranges[col]['max'] - ranges[col]['min'])\n",
    "    else:\n",
    "        ndf[col] = df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def prepare_data(ndf):\n",
    "    '''\n",
    "    prepare data list for each experiment\n",
    "\n",
    "    Args:\n",
    "        ndf: normalized dataframe\n",
    "    \n",
    "    Returns:\n",
    "        Vt_list: list of applied voltage\n",
    "        E_list: list of external electrolyte concentration\n",
    "        CFLA_list: list of feed LA concentration\n",
    "        CALA_list: list of acid LA concentration\n",
    "        CFK_list: list of feed K concentration\n",
    "        CBK_list: list of base K concentration\n",
    "        VF_list: list of feed volume\n",
    "        VA_list: list of acid volume\n",
    "        VB_list: list of base volume\n",
    "        I_list: list of current\n",
    "    '''\n",
    "    Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list, I_list = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    for exp_num in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp_num]\n",
    "\n",
    "        # operating conditions\n",
    "        Vt_list.append(exp_data['V'].values)\n",
    "        E_list.append(exp_data['E'].values)\n",
    "\n",
    "        # concentrations\n",
    "        CFLA_list.append(exp_data['CF_LA'].values)\n",
    "        CALA_list.append(exp_data['CA_LA'].values)\n",
    "        CFK_list.append(exp_data['CF_K'].values)\n",
    "        CBK_list.append(exp_data['CB_K'].values)\n",
    "\n",
    "        # volumes\n",
    "        VF_list.append(exp_data['VF'].values)\n",
    "        VA_list.append(exp_data['VA'].values)\n",
    "        VB_list.append(exp_data['VB'].values)\n",
    "\n",
    "        # current\n",
    "        I_list.append(exp_data['I'].values)\n",
    "\n",
    "    return Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list, I_list\n",
    "\n",
    "Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list, I_list = prepare_data(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "def pad_sequences(data_list, max_length=None, pad_value=-100.0):\n",
    "    '''\n",
    "    Pad variables length sequences to the same length\n",
    "\n",
    "    Args:\n",
    "        data_list: list of tensors with different sequence lengths\n",
    "        max_length: maximum length to pad to (default: longest sequence)\n",
    "        pad_value: value to use for padding\n",
    "\n",
    "    Returns:\n",
    "        padded_tensor: [batch_size, max_length, ...] - padded sequences\n",
    "        seq_lengths: [batch_size] - original sequence lengths\n",
    "    '''\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = max(data.shape[0] for data in data_list) # Auto-calculate the max length\n",
    "    \n",
    "    batch_size = len(data_list) # Batch size\n",
    "    seq_lengths = torch.tensor([data.shape[0] for data in data_list]) # Actual sequential length for each experiments\n",
    "    dimensions = data_list[0].shape[1:] # Get shape of individual elements\n",
    "    padded_tensor = torch.full((batch_size, max_length) + dimensions, pad_value, dtype=torch.float32) # generaste padded tensor filled with pad_value\n",
    "\n",
    "    # Fill with actual data\n",
    "    for i, data in enumerate(data_list):\n",
    "        padded_tensor[i, :data.shape[0]] = torch.tensor(data[:data.shape[0]], dtype=torch.float32)\n",
    "    \n",
    "    return padded_tensor, seq_lengths, max_length\n",
    "\n",
    "Vt, seq_lengths, max_length = pad_sequences(Vt_list)\n",
    "E, _, _ = pad_sequences(E_list,max_length = max_length)\n",
    "CFLA, _, _ = pad_sequences(CFLA_list,max_length = max_length)\n",
    "CALA, _, _ = pad_sequences(CALA_list,max_length = max_length)\n",
    "CFK, _, _ = pad_sequences(CFK_list,max_length = max_length)\n",
    "CBK, _, _ = pad_sequences(CBK_list,max_length = max_length)\n",
    "VF, _, _ = pad_sequences(VF_list,max_length = max_length)\n",
    "VA, _, _ = pad_sequences(VA_list,max_length = max_length)\n",
    "VB, _, _ = pad_sequences(VB_list,max_length = max_length)\n",
    "I, _, _ = pad_sequences(I_list,max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input tensor\n",
    "def prepare_input(Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB, seq_lengths):\n",
    "    '''\n",
    "    prepare input tensor for the model with padding support\n",
    "\n",
    "    Args:\n",
    "        Vt: applied voltage\n",
    "        E: external electrolyte concentration\n",
    "        CFLA: feed LA concentration\n",
    "        CALA: acid LA concentration\n",
    "        CFK: feed K concentration\n",
    "        CBK: base K concentration\n",
    "        VF: feed volume\n",
    "        VA: acid volume\n",
    "        VB: base volume\n",
    "        seq_lengths: actual sequence lengths\n",
    "\n",
    "    Returns:\n",
    "        input_tensor: [batch_size, seq_len, 3, 6] - formatted input for CNN-LSTM\n",
    "        initial_state: [batch_size, 3, 3] - initial concentrations and volumes\n",
    "        mask: [batch_size, seq_len] - padding mask\n",
    "        seq_lengths: [batch_size] - actual sequence lengths\n",
    "    '''\n",
    "\n",
    "    batch_size, seq_len = Vt.shape # Get batch size and sequence length for set the size of input tensor\n",
    "    input = torch.zeros(batch_size, seq_len, 9) # Generate input tensor\n",
    "\n",
    "    # Fill input tensor for each channel\n",
    "    input[:, :, 0] = Vt # Applied voltage\n",
    "    input[:, :, 1] = E # External electrolyte concentration\n",
    "    input[:, :, 2] = CFLA # Feed LA concentration\n",
    "    input[:, :, 3] = CALA # Acid LA concentration\n",
    "    input[:, :, 4] = CFK # Feed K concentration\n",
    "    input[:, :, 5] = CBK # Base K concentration\n",
    "    input[:, :, 6] = VF # Feed volume\n",
    "    input[:, :, 7] = VA # Acid volume\n",
    "    input[:, :, 8] = VB # Base volume\n",
    "\n",
    "    # initial state for each feature\n",
    "    init = torch.zeros(batch_size, 9)\n",
    "    init[:, 0] = Vt[:, 0] # Initial applied voltage\n",
    "    init[:, 1] = E[:, 0] # Initial external electrolyte concentration\n",
    "    init[:, 2] = CFLA[:, 0] # Initial feed LA concentration\n",
    "    init[:, 3] = CALA[:, 0] # Initial acid LA concentration\n",
    "    init[:, 4] = CFK[:, 0] # Initial feed K concentration\n",
    "    init[:, 5] = CBK[:, 0] # Initial base K concentration\n",
    "    init[:, 6] = VF[:, 0] # Initial feed volume\n",
    "    init[:, 7] = VA[:, 0] # Initial acid volume\n",
    "    init[:, 8] = VB[:, 0] # Initial base volume\n",
    "\n",
    "    # Create padding mask\n",
    "    mask = torch.zeros(batch_size, seq_len)\n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        mask[i, :length] = 1.0\n",
    "\n",
    "    return input, init, mask, seq_lengths\n",
    "\n",
    "input_tensor, init, mask, seq_lengths = prepare_input(Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB, seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataset by experiments\n",
    "class BMEDDataset(Dataset):\n",
    "    def __init__(self, inputs, init, masks, seq_lengths, I_exp, CFLA_exp, CALA_exp, CFK_exp, CBK_exp, VF_exp, VA_exp, VB_exp):\n",
    "        self.CV = inputs[:, :, :2] # Extract [Vt, E]\n",
    "        self.init = init[:, 2:] # Extract state variables\n",
    "        self.masks = masks\n",
    "        self.seq_lengths = seq_lengths\n",
    "\n",
    "        self.states = torch.stack([CFLA_exp, CALA_exp, CFK_exp, CBK_exp, VF_exp, VA_exp, VB_exp], dim=2)\n",
    "        self.current = I_exp.unsqueeze(2) # [batch, seq, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.CV)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'CV': self.CV[idx],\n",
    "            'init': self.init[idx],\n",
    "            'masks': self.masks[idx],\n",
    "            'seq_len': self.seq_lengths[idx],\n",
    "            'states': self.states[idx],\n",
    "            'current': self.current[idx]\n",
    "        }\n",
    "train_dataset = BMEDDataset(input_tensor, init, mask, seq_lengths, I, CFLA, CALA, CFK, CBK, VF, VA, VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BMEDModel(\n",
       "  (layer_norm): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "  (rnn_layers): LSTM(9, 128, num_layers=10, batch_first=True, dropout=0.2)\n",
       "  (flux_NN): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=103, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=103, out_features=78, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=78, out_features=53, bias=True)\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=53, out_features=28, bias=True)\n",
       "    (10): ELU(alpha=1.0)\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Linear(in_features=28, out_features=4, bias=True)\n",
       "    (13): ELU(alpha=1.0)\n",
       "    (14): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (current_NN): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=102, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=102, out_features=77, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=77, out_features=51, bias=True)\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=51, out_features=26, bias=True)\n",
       "    (10): ELU(alpha=1.0)\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Linear(in_features=26, out_features=1, bias=True)\n",
       "    (13): ELU(alpha=1.0)\n",
       "    (14): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Initialization\n",
    "class BMEDModel(nn.Module):\n",
    "    def __init__(self, hidden_nodes = 64, num_rnn_layers = 2, num_fnn_layers = 2,max_len = 37, dt = 0.25):\n",
    "        super(BMEDModel, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.input_features = 9 # [Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        self.control_features = 2 # [Vt, E]\n",
    "        self.state_features = 7 # [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        self.flux_features = 4 # [dLA, dK, dH2O_A, dH2O_B]\n",
    "        self.current_features = 1 # [I]\n",
    "        self.dt = dt # time step\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.ranges = {\n",
    "            'CFLA': {'min': -1, 'max': 4},\n",
    "            'CALA': {'min': -1, 'max': 4}, \n",
    "            'CFK': {'min': -1, 'max': 7},\n",
    "            'CBK': {'min': -1, 'max': 2},\n",
    "            'VF': {'min': 0, 'max': 2},\n",
    "            'VA': {'min': 0, 'max': 2},\n",
    "            'VB': {'min': 0, 'max': 8}\n",
    "        }\n",
    "\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(self.input_features)\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn_layers = nn.LSTM(\n",
    "            input_size = self.input_features,\n",
    "            hidden_size = hidden_nodes,\n",
    "            num_layers = num_rnn_layers,\n",
    "            batch_first = True,\n",
    "            dropout = 0.2 if num_rnn_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Flux Head\n",
    "        flux_layers = []\n",
    "        flux_sizes = [hidden_nodes]\n",
    "        flux_step = (hidden_nodes - self.flux_features) / (num_fnn_layers)\n",
    "        \n",
    "        for i in range(num_fnn_layers):\n",
    "            next_size = int(hidden_nodes - flux_step * (i + 1))\n",
    "            if i == num_fnn_layers - 1:\n",
    "                next_size = self.flux_features\n",
    "            \n",
    "            flux_layers.append(nn.Linear(flux_sizes[-1], next_size))\n",
    "            flux_layers.append(nn.ELU())\n",
    "            flux_layers.append(nn.Dropout(0.2))\n",
    "            flux_sizes.append(next_size)\n",
    "            \n",
    "        self.flux_NN = nn.Sequential(*flux_layers)\n",
    "        \n",
    "\n",
    "        # Current Head\n",
    "        current_layers = []\n",
    "        current_sizes = [hidden_nodes]\n",
    "        current_step = (hidden_nodes - self.current_features) / (num_fnn_layers)\n",
    "        \n",
    "        for i in range(num_fnn_layers):\n",
    "            next_size = int(hidden_nodes - current_step * (i + 1))\n",
    "            if i == num_fnn_layers - 1:\n",
    "                next_size = self.current_features\n",
    "            \n",
    "            current_layers.append(nn.Linear(current_sizes[-1], next_size))\n",
    "            current_layers.append(nn.ELU())\n",
    "            current_layers.append(nn.Dropout(0.2))\n",
    "            current_sizes.append(next_size)\n",
    "        \n",
    "        self.current_NN = nn.Sequential(*current_layers)\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initialize hidden states for RNN layers\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(self.num_rnn_layers, batch_size, self.hidden_nodes, device=device)\n",
    "        c0 = torch.zeros(self.num_rnn_layers, batch_size, self.hidden_nodes, device=device)\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def denormalize_state(self, norm_state):\n",
    "        '''\n",
    "        Normalized state to real values \n",
    "        '''\n",
    "        batch_size = norm_state.shape[0]\n",
    "        real_state = torch.zeros_like(norm_state)\n",
    "\n",
    "        state_names = ['CFLA', 'CALA', 'CFK', 'CBK', 'VF', 'VA', 'VB']\n",
    "\n",
    "        for i, name in enumerate(state_names):\n",
    "            min_val = self.ranges[name]['min']\n",
    "            max_val = self.ranges[name]['max']\n",
    "            real_state[:, i] = norm_state[:, i] * (max_val - min_val) + min_val\n",
    "        \n",
    "        return real_state\n",
    "    \n",
    "    def normalize_state(self, real_state):\n",
    "        '''\n",
    "        Real state to normalized values\n",
    "        '''\n",
    "        batch_size = real_state.shape[0]\n",
    "        norm_state = torch.zeros_like(real_state)\n",
    "\n",
    "        state_names = ['CFLA', 'CALA', 'CFK', 'CBK', 'VF', 'VA', 'VB']\n",
    "\n",
    "        for i, name in enumerate(state_names):\n",
    "            min_val = self.ranges[name]['min']\n",
    "            max_val = self.ranges[name]['max']\n",
    "            norm_state[:, i] = (real_state[:, i] - min_val) / (max_val - min_val)\n",
    "\n",
    "        return norm_state\n",
    "\n",
    "    def MB_step(self, norm_state, LA_flux, K_flux, VFA_flux, VFB_flux):\n",
    "        '''\n",
    "        Perform one time step of mass balance\n",
    "        state: [batch, 7] - [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        '''\n",
    "        real_state = self.denormalize_state(norm_state)\n",
    "\n",
    "        # extract current values\n",
    "        # channel 0: feed, channel 1: acid, channel 2: base\n",
    "        # property 0: LA_conc, property 1: K_conc, property 2: volume\n",
    "\n",
    "        CFLA = real_state[:, 0]\n",
    "        CALA = real_state[:, 1]\n",
    "        CFK = real_state[:, 2]\n",
    "        CBK = real_state[:, 3]\n",
    "        VF = real_state[:, 4]\n",
    "        VA = real_state[:, 5]\n",
    "        VB = real_state[:, 6]\n",
    "\n",
    "        # volume changes due to water flux\n",
    "        # Assuming positive flux means water moves from feed to acid or base\n",
    "        nVF = VF - (VFA_flux + VFB_flux) * self.dt # Feed Volume\n",
    "        nVA = VA + VFA_flux * self.dt # Acid Volume\n",
    "        nVB = VB + VFB_flux * self.dt # Base Volume\n",
    "\n",
    "        # LA mass balance\n",
    "        nNFLA = CFLA*VF - LA_flux*self.dt\n",
    "        nNALA = CALA*VA + LA_flux*self.dt\n",
    "        nNFK = CFK*VF - K_flux*self.dt\n",
    "        nNBK = CBK*VB + K_flux*self.dt\n",
    "\n",
    "        # update states\n",
    "        new_real_state = torch.zeros_like(real_state)\n",
    "        new_real_state[:, 0] = nNFLA / (nVF + 1e-8) # new Feed LA concentration\n",
    "        new_real_state[:, 1] = nNALA / (nVA + 1e-8) # new Acid LA concentration\n",
    "        new_real_state[:, 2] = nNFK / (nVF + 1e-8) # new Feed K concentration\n",
    "        new_real_state[:, 3] = nNBK / (nVB + 1e-8) # new Base K concentration\n",
    "        new_real_state[:, 4] = nVF # new Feed Volume\n",
    "        new_real_state[:, 5] = nVA # new Acid Volume\n",
    "        new_real_state[:, 6] = nVB # new Base Volume\n",
    "\n",
    "        new_norm_state = self.normalize_state(new_real_state)\n",
    "    \n",
    "        return new_norm_state\n",
    "    \n",
    "    def forward_single_step(self, CV, prev_state, hidden_state):\n",
    "        '''\n",
    "        Predict Single Step of BMED\n",
    "        CV: [batch_size, 2] - [Vt, E]\n",
    "        prev_state: [batch_size, 7] - [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        hidden_state: [h, c] - hidden state of RNN layers\n",
    "        '''\n",
    "\n",
    "        full_input = torch.cat([CV, prev_state], dim=1)\n",
    "        full_input = full_input.unsqueeze(1) # [batch_size, 1, 9]\n",
    "\n",
    "        # Layer Normalization\n",
    "        rnn_input = self.layer_norm(full_input)\n",
    "        \n",
    "        # RNN forward - pack all information of previous hidden state\n",
    "        rnn_out, new_hidden_state = self.rnn_layers(rnn_input, hidden_state)\n",
    "\n",
    "        # Predict flux and current\n",
    "        flux = self.flux_NN(rnn_out.squeeze(1)) # [batch_size, 4]\n",
    "        current = self.current_NN(rnn_out.squeeze(1)) # [batch_size, 1]\n",
    "\n",
    "        # Physical update\n",
    "        LA_flux = flux[:, 0]\n",
    "        K_flux = flux[:, 1]\n",
    "        VFA_flux = flux[:, 2]\n",
    "        VFB_flux = flux[:, 3]\n",
    "\n",
    "        new_state = self.MB_step(prev_state, LA_flux, K_flux, VFA_flux, VFB_flux)\n",
    "\n",
    "        return current, new_state, new_hidden_state\n",
    "    \n",
    "    def next_seq_pred_train(self, CV, init, states, current, masks):\n",
    "        '''\n",
    "        Autoregressive training\n",
    "        '''\n",
    "        batch_size, max_seq_len, _ = CV.shape\n",
    "        device= CV.device\n",
    "\n",
    "        mask = masks # [batch_size, max_seq_len]\n",
    "\n",
    "        CVf = CV[:, 0, :] # [batch_size, 2]\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden_state = self.init_hidden(batch_size,device)\n",
    "        current_state = init.clone() # [batch_size, 7]\n",
    "\n",
    "        # predicted results\n",
    "        predicted_current = []\n",
    "        predicted_states = []\n",
    "\n",
    "        # use init_state at t=0\n",
    "        predicted_states.append(init)\n",
    "\n",
    "        # use 0 at t=0\n",
    "        first_current = torch.zeros(batch_size, 1, device=device)\n",
    "        predicted_current.append(first_current)\n",
    "\n",
    "        # Calculate for each timestep with the entire batch simultaneously\n",
    "        for t in range(1,max_seq_len):\n",
    "\n",
    "            # Predict current and states of the entire batch\n",
    "            pred_current, pred_state, hidden_state = self.forward_single_step(CVf, current_state, hidden_state)\n",
    "\n",
    "            predicted_current.append(pred_current)\n",
    "            predicted_states.append(pred_state)\n",
    "\n",
    "            # update current state with predicted states\n",
    "            current_state = pred_state\n",
    "\n",
    "        # Convert to tensor\n",
    "        predicted_current = torch.stack(predicted_current, dim=1) # [batch_size, max_seq_len, 1]\n",
    "        predicted_states = torch.stack(predicted_states, dim=1) # [batch_size, max_seq_len, 7]\n",
    "\n",
    "        # Loss calculation: \n",
    "        states_exp = states[:, 1:, :] # [batch_size, seq_len-1, 7]\n",
    "        current_exp = current[:, 1:, :] # [batch_size, seq_len-1, 1]\n",
    "        states_pred = predicted_states[:, 1:, :] # [batch_size, seq_len-1, 7]\n",
    "        current_pred = predicted_current[:, 1:, :] # [batch_size, seq_len-1, 1]\n",
    "        mask_adj = mask[:, 1:] # [batch_size, seq_len-1]\n",
    "        \n",
    "        # Calculate loss\n",
    "        states_loss = F.mse_loss(states_pred, states_exp, reduction='none')\n",
    "        current_loss = F.mse_loss(current_pred, current_exp, reduction='none')\n",
    "\n",
    "        # Apply mask and calculate mean loss\n",
    "        mask_states = mask_adj.unsqueeze(-1).expand(-1, -1, 7)\n",
    "        mask_current = mask_adj.unsqueeze(-1)\n",
    "\n",
    "        masked_states_loss = (states_loss * mask_states).sum() / mask_states.sum()\n",
    "        masked_current_loss = (current_loss * mask_current).sum() / mask_current.sum()\n",
    "\n",
    "        total_loss = masked_current_loss + masked_states_loss*7\n",
    "\n",
    "        return total_loss, predicted_current, predicted_states\n",
    "    \n",
    "    def generate_sequence(self, CV, init, max_steps):\n",
    "        '''\n",
    "        Prediction mode from initial state\n",
    "\n",
    "        Args:\n",
    "            CV: [2] or [batch_size, 2] - [Vt, E]\n",
    "            init_state: initial state ([7] or [batch_size, 7])\n",
    "            max_steps: maximum number of steps to generate\n",
    "        '''\n",
    "\n",
    "        if len(init.shape) == 1:\n",
    "            # if input is single exp, add a batch dimension\n",
    "            init = init.unsqueeze(0)\n",
    "            batch_size = 1\n",
    "            single_exp = True\n",
    "        else:\n",
    "            batch_size = init.shape[0]\n",
    "            single_exp = False\n",
    "        \n",
    "        device = init.device\n",
    "\n",
    "        # check CV batch size and modification\n",
    "        if len(CV.shape) == 1:\n",
    "            CV = CV.unsqueeze(0) # [1, 2]\n",
    "        \n",
    "        CVf = CV[:, 0, :].to(device)\n",
    "        \n",
    "\n",
    "        # initialize hidden state\n",
    "        hidden_state = self.init_hidden(batch_size, device)\n",
    "        current_state = init.clone() # [batch_size, 7]\n",
    "\n",
    "        predicted_current = []\n",
    "        predicted_states = []\n",
    "        \n",
    "        predicted_states.append(init)\n",
    "        first_current = torch.zeros(batch_size, 1, device=device)  # 첫 번째 전류는 0으로 설정\n",
    "        predicted_current.append(first_current)\n",
    "\n",
    "        for t in range(1, max_steps):\n",
    "            pred_current, pred_state, hidden_state = self.forward_single_step(CVf, current_state, hidden_state)\n",
    "\n",
    "            predicted_current.append(pred_current) # [batch_size, 1]\n",
    "            predicted_states.append(pred_state) # [batch_size, 7]\n",
    "\n",
    "            current_state = pred_state\n",
    "\n",
    "        predicted_current = torch.stack(predicted_current, dim=1) # [batch_size, max_steps, 1]\n",
    "        predicted_states = torch.stack(predicted_states, dim=1) # [batch_size, max_steps, 7]\n",
    "\n",
    "        if single_exp:\n",
    "            predicted_current = predicted_current.squeeze(0) # [max_steps, 1]\n",
    "            predicted_states = predicted_states.squeeze(0) # [max_steps, 7]\n",
    "        \n",
    "        return predicted_current, predicted_states\n",
    "\n",
    "    def forward(self, CV, init, states, current, masks, mode='train', max_steps=None):\n",
    "        '''\n",
    "        Integrated forward method\n",
    "        '''\n",
    "\n",
    "        if mode == 'train':\n",
    "            if states is None or current is None or masks is None:\n",
    "                raise ValueError(\"states, current, and masks are required for training\")\n",
    "            return self.next_seq_pred_train(CV, init, states, current, masks)\n",
    "        elif mode == 'inference':\n",
    "            if max_steps is None:\n",
    "                raise ValueError(\"max_steps is required for inference\")\n",
    "            fixed_control = CV[:, 0, :] # [batch_size, 2]\n",
    "            return self.generate_sequence(CV, init, max_steps)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Use 'train' or 'inference'\")\n",
    "dmodel = 128\n",
    "model = BMEDModel(\n",
    "    hidden_nodes=dmodel,\n",
    "    num_rnn_layers=10,\n",
    "    num_fnn_layers=5,\n",
    "    max_len=max_length,\n",
    "    dt=0.25\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noam Scheduler\n",
    "class NoamLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    '''\n",
    "    Pytorch LRScheduler 스타일으 Noam Scheduler\n",
    "    '''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=40, last_epoch=-1):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(NoamLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        if step == 0:\n",
    "            step  = 1\n",
    "\n",
    "        lr_scale = (self.d_model ** -0.5) *min(\n",
    "            step ** -0.5,\n",
    "            step * (self.warmup_steps ** -1.5)\n",
    "        )\n",
    "        return [base_lr * lr_scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "num_epochs = 1000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "scheduler = NoamLR(optimizer, d_model=dmodel, warmup_steps=0.1*num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "==================================================\n",
      "Epoch 1/1000, Loss: 0.085248, LR: 0.000018, best_loss: 0.085248\n",
      "Epoch 11/1000, Loss: 0.058940, LR: 0.000106, best_loss: 0.058940\n",
      "Epoch 21/1000, Loss: 0.042523, LR: 0.000194, best_loss: 0.035290\n",
      "Epoch 31/1000, Loss: 0.013484, LR: 0.000283, best_loss: 0.013484\n",
      "Epoch 41/1000, Loss: 0.010979, LR: 0.000371, best_loss: 0.010864\n",
      "Epoch 51/1000, Loss: 0.011528, LR: 0.000460, best_loss: 0.006420\n",
      "Epoch 61/1000, Loss: 0.010328, LR: 0.000548, best_loss: 0.005917\n",
      "Epoch 71/1000, Loss: 0.006661, LR: 0.000636, best_loss: 0.005545\n",
      "Epoch 81/1000, Loss: 0.010631, LR: 0.000725, best_loss: 0.004291\n",
      "Epoch 91/1000, Loss: 0.008232, LR: 0.000813, best_loss: 0.004291\n",
      "Epoch 101/1000, Loss: 0.004064, LR: 0.000875, best_loss: 0.003078\n",
      "Epoch 111/1000, Loss: 0.004076, LR: 0.000835, best_loss: 0.002710\n",
      "Epoch 121/1000, Loss: 0.002747, LR: 0.000800, best_loss: 0.002710\n",
      "Epoch 131/1000, Loss: 0.005533, LR: 0.000769, best_loss: 0.002043\n",
      "Epoch 141/1000, Loss: 0.005461, LR: 0.000742, best_loss: 0.002043\n",
      "Epoch 151/1000, Loss: 0.005734, LR: 0.000717, best_loss: 0.002043\n",
      "Epoch 161/1000, Loss: 0.002715, LR: 0.000694, best_loss: 0.002043\n",
      "Epoch 171/1000, Loss: 0.003176, LR: 0.000674, best_loss: 0.001565\n",
      "Epoch 181/1000, Loss: 0.004344, LR: 0.000655, best_loss: 0.001565\n",
      "Epoch 191/1000, Loss: 0.002790, LR: 0.000638, best_loss: 0.001565\n",
      "Epoch 201/1000, Loss: 0.002052, LR: 0.000622, best_loss: 0.001565\n",
      "Epoch 211/1000, Loss: 0.006174, LR: 0.000607, best_loss: 0.001565\n",
      "Epoch 221/1000, Loss: 0.005311, LR: 0.000593, best_loss: 0.001565\n",
      "Epoch 231/1000, Loss: 0.004197, LR: 0.000580, best_loss: 0.001565\n",
      "Epoch 241/1000, Loss: 0.006202, LR: 0.000568, best_loss: 0.001565\n",
      "Epoch 251/1000, Loss: 0.002592, LR: 0.000557, best_loss: 0.001565\n",
      "Epoch 261/1000, Loss: 0.001504, LR: 0.000546, best_loss: 0.001410\n",
      "Epoch 271/1000, Loss: 0.003817, LR: 0.000536, best_loss: 0.001410\n",
      "Epoch 281/1000, Loss: 0.003466, LR: 0.000526, best_loss: 0.001410\n",
      "Epoch 291/1000, Loss: 0.002209, LR: 0.000517, best_loss: 0.001410\n",
      "Epoch 301/1000, Loss: 0.003589, LR: 0.000509, best_loss: 0.001410\n",
      "Epoch 311/1000, Loss: 0.002646, LR: 0.000500, best_loss: 0.001410\n",
      "Epoch 321/1000, Loss: 0.002310, LR: 0.000493, best_loss: 0.001410\n",
      "Epoch 331/1000, Loss: 0.002950, LR: 0.000485, best_loss: 0.001410\n",
      "Epoch 341/1000, Loss: 0.004851, LR: 0.000478, best_loss: 0.001410\n",
      "Epoch 351/1000, Loss: 0.004794, LR: 0.000471, best_loss: 0.001410\n",
      "Epoch 361/1000, Loss: 0.003051, LR: 0.000465, best_loss: 0.001410\n",
      "Epoch 371/1000, Loss: 0.002934, LR: 0.000458, best_loss: 0.001410\n",
      "Epoch 381/1000, Loss: 0.001981, LR: 0.000452, best_loss: 0.001410\n",
      "Epoch 391/1000, Loss: 0.003205, LR: 0.000446, best_loss: 0.001410\n",
      "Epoch 401/1000, Loss: 0.004959, LR: 0.000441, best_loss: 0.001063\n",
      "Epoch 411/1000, Loss: 0.003110, LR: 0.000435, best_loss: 0.001063\n",
      "Epoch 421/1000, Loss: 0.002918, LR: 0.000430, best_loss: 0.001063\n",
      "Epoch 431/1000, Loss: 0.001441, LR: 0.000425, best_loss: 0.001063\n",
      "Epoch 441/1000, Loss: 0.003806, LR: 0.000420, best_loss: 0.001063\n",
      "Epoch 451/1000, Loss: 0.003484, LR: 0.000416, best_loss: 0.001053\n",
      "Epoch 461/1000, Loss: 0.004087, LR: 0.000411, best_loss: 0.001053\n",
      "Epoch 471/1000, Loss: 0.003797, LR: 0.000407, best_loss: 0.001053\n",
      "Epoch 481/1000, Loss: 0.002763, LR: 0.000403, best_loss: 0.000894\n",
      "Epoch 491/1000, Loss: 0.004037, LR: 0.000398, best_loss: 0.000894\n",
      "Epoch 501/1000, Loss: 0.005068, LR: 0.000394, best_loss: 0.000894\n",
      "Epoch 511/1000, Loss: 0.003832, LR: 0.000391, best_loss: 0.000886\n",
      "Epoch 521/1000, Loss: 0.002493, LR: 0.000387, best_loss: 0.000886\n",
      "Epoch 531/1000, Loss: 0.002588, LR: 0.000383, best_loss: 0.000886\n",
      "Epoch 541/1000, Loss: 0.001098, LR: 0.000380, best_loss: 0.000886\n",
      "Epoch 551/1000, Loss: 0.003224, LR: 0.000376, best_loss: 0.000886\n",
      "Epoch 561/1000, Loss: 0.004250, LR: 0.000373, best_loss: 0.000886\n",
      "Epoch 571/1000, Loss: 0.003486, LR: 0.000370, best_loss: 0.000886\n",
      "Epoch 581/1000, Loss: 0.002868, LR: 0.000366, best_loss: 0.000576\n",
      "Epoch 591/1000, Loss: 0.002384, LR: 0.000363, best_loss: 0.000576\n",
      "Epoch 601/1000, Loss: 0.002455, LR: 0.000360, best_loss: 0.000576\n",
      "Epoch 611/1000, Loss: 0.001118, LR: 0.000357, best_loss: 0.000576\n",
      "Epoch 621/1000, Loss: 0.004045, LR: 0.000354, best_loss: 0.000576\n",
      "Epoch 631/1000, Loss: 0.001434, LR: 0.000352, best_loss: 0.000576\n",
      "Epoch 641/1000, Loss: 0.002595, LR: 0.000349, best_loss: 0.000576\n",
      "Epoch 651/1000, Loss: 0.001758, LR: 0.000346, best_loss: 0.000576\n",
      "Epoch 661/1000, Loss: 0.001428, LR: 0.000344, best_loss: 0.000576\n",
      "Epoch 671/1000, Loss: 0.003339, LR: 0.000341, best_loss: 0.000576\n",
      "Epoch 681/1000, Loss: 0.003959, LR: 0.000338, best_loss: 0.000576\n",
      "Epoch 691/1000, Loss: 0.003715, LR: 0.000336, best_loss: 0.000576\n",
      "Epoch 701/1000, Loss: 0.003586, LR: 0.000334, best_loss: 0.000576\n",
      "Epoch 711/1000, Loss: 0.001916, LR: 0.000331, best_loss: 0.000576\n",
      "Epoch 721/1000, Loss: 0.001798, LR: 0.000329, best_loss: 0.000576\n",
      "Epoch 731/1000, Loss: 0.003234, LR: 0.000327, best_loss: 0.000576\n",
      "Epoch 741/1000, Loss: 0.002756, LR: 0.000324, best_loss: 0.000576\n",
      "Epoch 751/1000, Loss: 0.002329, LR: 0.000322, best_loss: 0.000576\n",
      "Epoch 761/1000, Loss: 0.002435, LR: 0.000320, best_loss: 0.000576\n",
      "Epoch 771/1000, Loss: 0.002301, LR: 0.000318, best_loss: 0.000576\n",
      "Epoch 781/1000, Loss: 0.002350, LR: 0.000316, best_loss: 0.000576\n",
      "Epoch 791/1000, Loss: 0.003096, LR: 0.000314, best_loss: 0.000576\n",
      "Epoch 801/1000, Loss: 0.003580, LR: 0.000312, best_loss: 0.000576\n",
      "Epoch 811/1000, Loss: 0.002592, LR: 0.000310, best_loss: 0.000576\n",
      "Epoch 821/1000, Loss: 0.002192, LR: 0.000308, best_loss: 0.000576\n",
      "Epoch 831/1000, Loss: 0.002819, LR: 0.000306, best_loss: 0.000576\n",
      "Epoch 841/1000, Loss: 0.003865, LR: 0.000305, best_loss: 0.000576\n",
      "Epoch 851/1000, Loss: 0.003115, LR: 0.000303, best_loss: 0.000576\n",
      "Epoch 861/1000, Loss: 0.003190, LR: 0.000301, best_loss: 0.000576\n",
      "Epoch 871/1000, Loss: 0.001802, LR: 0.000299, best_loss: 0.000576\n",
      "Epoch 881/1000, Loss: 0.003217, LR: 0.000298, best_loss: 0.000576\n",
      "Epoch 891/1000, Loss: 0.001968, LR: 0.000296, best_loss: 0.000576\n",
      "Epoch 901/1000, Loss: 0.003415, LR: 0.000294, best_loss: 0.000576\n",
      "Epoch 911/1000, Loss: 0.002282, LR: 0.000293, best_loss: 0.000576\n",
      "Epoch 921/1000, Loss: 0.002058, LR: 0.000291, best_loss: 0.000576\n",
      "Epoch 931/1000, Loss: 0.002448, LR: 0.000290, best_loss: 0.000576\n",
      "Epoch 941/1000, Loss: 0.004232, LR: 0.000288, best_loss: 0.000576\n",
      "Epoch 951/1000, Loss: 0.003016, LR: 0.000286, best_loss: 0.000576\n",
      "Epoch 961/1000, Loss: 0.002079, LR: 0.000285, best_loss: 0.000576\n",
      "Epoch 971/1000, Loss: 0.000887, LR: 0.000284, best_loss: 0.000576\n",
      "Epoch 981/1000, Loss: 0.002061, LR: 0.000282, best_loss: 0.000576\n",
      "Epoch 991/1000, Loss: 0.002056, LR: 0.000281, best_loss: 0.000576\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, device):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    best_loss = np.inf\n",
    "    best_states = None\n",
    "    best_current = None\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # move to device\n",
    "        CV = batch['CV'].to(device)\n",
    "        init = batch['init'].to(device)\n",
    "        states = batch['states'].to(device)\n",
    "        current = batch['current'].to(device)\n",
    "        masks = batch['masks'].to(device)\n",
    "\n",
    "        # gradient initialization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss, pred_current, pred_states = model(CV, init, states, current, masks, mode='train')\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 1)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # update scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    return total_loss / num_batches, pred_current, pred_states\n",
    "\n",
    "print(\"Start training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_loss = np.inf\n",
    "best_current = None\n",
    "best_states = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, pred_current, pred_states = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        best_current = pred_current\n",
    "        best_states = pred_states\n",
    "\n",
    "    if epoch % 10 == 0: \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.6f}, LR: {current_lr:.6f}, best_loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_idx = 0\n",
    "test_states = train_dataset.states[test_idx:test_idx+1]\n",
    "test_current = train_dataset.current[test_idx:test_idx+1]\n",
    "test_seq_len = train_dataset.seq_lengths[test_idx:test_idx+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_current_cpu = best_current.detach().cpu().numpy()\n",
    "pred_states_cpu = best_states.detach().cpu().numpy()\n",
    "test_states_cpu = test_states.detach().cpu().numpy()\n",
    "test_current_cpu = test_current.detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_current_cpu[0, :test_seq_len[0], 0], label='True Current')\n",
    "plt.plot(pred_current_cpu[0, :test_seq_len[0], 0], label='Predicted Current')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))  # 가로 크기를 늘려서 2열로 표시\n",
    "\n",
    "# 첫 번째 열 (왼쪽)\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.plot(test_states_cpu[0, :test_seq_len[0], 0], label='True CFLA')\n",
    "plt.plot(pred_states_cpu[0, :test_seq_len[0], 0], label='Predicted CFLA')\n",
    "plt.legend()\n",
    "plt.title('CFLA')\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.plot(test_states_cpu[0, :test_seq_len[0], 1], label='True CALA')\n",
    "plt.plot(pred_states_cpu[0, :test_seq_len[0], 1], label='Predicted CALA')\n",
    "plt.legend()\n",
    "plt.title('CALA')\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "plt.plot(test_states_cpu[0, :test_seq_len[0], 2], label='True CFK')\n",
    "plt.plot(pred_states_cpu[0, :test_seq_len[0], 2], label='Predicted CFK')\n",
    "plt.legend()\n",
    "plt.title('CFK')\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "plt.plot(test_states_cpu[0, :test_seq_len[0], 3], label='True CBK')\n",
    "plt.plot(pred_states_cpu[0, :test_seq_len[0], 3], label='Predicted CBK')\n",
    "plt.legend()\n",
    "plt.title('CBK')\n",
    "\n",
    "# 두 번째 열 (오른쪽)\n",
    "plt.subplot(4, 2, 2)\n",
    "plt.plot(test_states_cpu[0, :test_seq_len[0], 4], label='True VF')\n",
    "plt.plot(pred_states_cpu[0, :test_seq_len[0], 4], label='Predicted VF')\n",
    "plt.legend()\n",
    "plt.title('VF')\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "plt.plot(test_states_cpu[0, :test_seq_len[0], 5], label='True VA')\n",
    "plt.plot(pred_states_cpu[0, :test_seq_len[0], 5], label='Predicted VA')\n",
    "plt.legend()\n",
    "plt.title('VA')\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "plt.plot(test_states_cpu[0, :test_seq_len[0], 6], label='True VB')\n",
    "plt.plot(pred_states_cpu[0, :test_seq_len[0], 6], label='Predicted VB')\n",
    "plt.legend()\n",
    "plt.title('VB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
