{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Using device: {device}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"BMED_DB_augmented.csv\")\n",
    "#df = df[df['exp'].isin([0,1,2,3,4])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust min-max scaling including safety margin\n",
    "ranges ={\n",
    "'V' : {'min':0, 'max':50},\n",
    "'E' : {'min':0, 'max':1},\n",
    "'VF' : {'min':0, 'max':2},\n",
    "'VA' : {'min':0, 'max':2},\n",
    "'VB' : {'min':0, 'max':8},\n",
    "'CF_LA' : {'min':-1, 'max':4},\n",
    "'CA_LA' : {'min':-1, 'max':4},\n",
    "'CF_K' : {'min':-1, 'max':7},\n",
    "'CB_K' : {'min':-1, 'max':2},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "ndf = pd.DataFrame()\n",
    "ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "for col in ['V', 'E', 'VF', 'VA', 'VB', 'CF_LA', 'CA_LA', 'CF_K', 'CB_K']:\n",
    "    if col in ranges:\n",
    "        ndf[col] = (df[col] - ranges[col]['min'])/(ranges[col]['max'] - ranges[col]['min'])\n",
    "    else:\n",
    "        ndf[col] = df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def prepare_data(ndf):\n",
    "    '''\n",
    "    prepare data list for each experiment\n",
    "\n",
    "    Args:\n",
    "        ndf: normalized dataframe\n",
    "    \n",
    "    Returns:\n",
    "        Vt_list: list of applied voltage\n",
    "        E_list: list of external electrolyte concentration\n",
    "        CFLA_list: list of feed LA concentration\n",
    "        CALA_list: list of acid LA concentration\n",
    "        CFK_list: list of feed K concentration\n",
    "        CBK_list: list of base K concentration\n",
    "        VF_list: list of feed volume\n",
    "        VA_list: list of acid volume\n",
    "        VB_list: list of base volume\n",
    "    '''\n",
    "    Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    for exp_num in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp_num]\n",
    "\n",
    "        # operating conditions\n",
    "        Vt_list.append(exp_data['V'].values)\n",
    "        E_list.append(exp_data['E'].values)\n",
    "\n",
    "        # concentrations\n",
    "        CFLA_list.append(exp_data['CF_LA'].values)\n",
    "        CALA_list.append(exp_data['CA_LA'].values)\n",
    "        CFK_list.append(exp_data['CF_K'].values)\n",
    "        CBK_list.append(exp_data['CB_K'].values)\n",
    "\n",
    "        # volumes\n",
    "        VF_list.append(exp_data['VF'].values)\n",
    "        VA_list.append(exp_data['VA'].values)\n",
    "        VB_list.append(exp_data['VB'].values)\n",
    "\n",
    "\n",
    "    return Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list\n",
    "\n",
    "Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list = prepare_data(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "def pad_sequences(data_list, max_length=None, pad_value=-100.0):\n",
    "    '''\n",
    "    Pad variables length sequences to the same length\n",
    "\n",
    "    Args:\n",
    "        data_list: list of tensors with different sequence lengths\n",
    "        max_length: maximum length to pad to (default: longest sequence)\n",
    "        pad_value: value to use for padding\n",
    "\n",
    "    Returns:\n",
    "        padded_tensor: [batch_size, max_length, ...] - padded sequences\n",
    "        seq_lengths: [batch_size] - original sequence lengths\n",
    "    '''\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = max(data.shape[0] for data in data_list) # Auto-calculate the max length\n",
    "    \n",
    "    batch_size = len(data_list) # Batch size\n",
    "    seq_lengths = torch.tensor([data.shape[0] for data in data_list]) # Actual sequential length for each experiments\n",
    "    dimensions = data_list[0].shape[1:] # Get shape of individual elements\n",
    "    padded_tensor = torch.full((batch_size, max_length) + dimensions, pad_value, dtype=torch.float32) # generaste padded tensor filled with pad_value\n",
    "\n",
    "    # Fill with actual data\n",
    "    for i, data in enumerate(data_list):\n",
    "        padded_tensor[i, :data.shape[0]] = torch.tensor(data[:data.shape[0]], dtype=torch.float32)\n",
    "    \n",
    "    return padded_tensor, seq_lengths, max_length\n",
    "\n",
    "Vt, seq_lengths, max_length = pad_sequences(Vt_list)\n",
    "E, _, _ = pad_sequences(E_list,max_length = max_length)\n",
    "CFLA, _, _ = pad_sequences(CFLA_list,max_length = max_length)\n",
    "CALA, _, _ = pad_sequences(CALA_list,max_length = max_length)\n",
    "CFK, _, _ = pad_sequences(CFK_list,max_length = max_length)\n",
    "CBK, _, _ = pad_sequences(CBK_list,max_length = max_length)\n",
    "VF, _, _ = pad_sequences(VF_list,max_length = max_length)\n",
    "VA, _, _ = pad_sequences(VA_list,max_length = max_length)\n",
    "VB, _, _ = pad_sequences(VB_list,max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input tensor\n",
    "def prepare_input(Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB, seq_lengths):\n",
    "    '''\n",
    "    prepare input tensor for the model with padding support\n",
    "\n",
    "    Args:\n",
    "        Vt: applied voltage\n",
    "        E: external electrolyte concentration\n",
    "        CFLA: feed LA concentration\n",
    "        CALA: acid LA concentration\n",
    "        CFK: feed K concentration\n",
    "        CBK: base K concentration\n",
    "        VF: feed volume\n",
    "        VA: acid volume\n",
    "        VB: base volume\n",
    "        seq_lengths: actual sequence lengths\n",
    "\n",
    "    Returns:\n",
    "        input_tensor: [batch_size, seq_len, 3, 6] - formatted input for CNN-LSTM\n",
    "        initial_state: [batch_size, 3, 3] - initial concentrations and volumes\n",
    "        mask: [batch_size, seq_len] - padding mask\n",
    "        seq_lengths: [batch_size] - actual sequence lengths\n",
    "    '''\n",
    "\n",
    "    batch_size, seq_len = Vt.shape # Get batch size and sequence length for set the size of input tensor\n",
    "    input = torch.zeros(batch_size, seq_len, 9) # Generate input tensor\n",
    "\n",
    "    # Fill input tensor for each channel\n",
    "    input[:, :, 0] = Vt # Applied voltage\n",
    "    input[:, :, 1] = E # External electrolyte concentration\n",
    "    input[:, :, 2] = CFLA # Feed LA concentration\n",
    "    input[:, :, 3] = CALA # Acid LA concentration\n",
    "    input[:, :, 4] = CFK # Feed K concentration\n",
    "    input[:, :, 5] = CBK # Base K concentration\n",
    "    input[:, :, 6] = VF # Feed volume\n",
    "    input[:, :, 7] = VA # Acid volume\n",
    "    input[:, :, 8] = VB # Base volume\n",
    "\n",
    "    # initial state for each feature\n",
    "    init = torch.zeros(batch_size, 9)\n",
    "    init[:, 0] = Vt[:, 0] # Initial applied voltage\n",
    "    init[:, 1] = E[:, 0] # Initial external electrolyte concentration\n",
    "    init[:, 2] = CFLA[:, 0] # Initial feed LA concentration\n",
    "    init[:, 3] = CALA[:, 0] # Initial acid LA concentration\n",
    "    init[:, 4] = CFK[:, 0] # Initial feed K concentration\n",
    "    init[:, 5] = CBK[:, 0] # Initial base K concentration\n",
    "    init[:, 6] = VF[:, 0] # Initial feed volume\n",
    "    init[:, 7] = VA[:, 0] # Initial acid volume\n",
    "    init[:, 8] = VB[:, 0] # Initial base volume\n",
    "\n",
    "    # Create padding mask\n",
    "    mask = torch.zeros(batch_size, seq_len)\n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        mask[i, :length] = 1.0\n",
    "\n",
    "    return input, init, mask, seq_lengths\n",
    "\n",
    "input_tensor, init, mask, seq_lengths = prepare_input(Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB, seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataset by experiments\n",
    "class BMEDDataset(Dataset):\n",
    "    def __init__(self, inputs, init, masks, seq_lengths, CFLA_exp, CALA_exp, CFK_exp, CBK_exp, VF_exp, VA_exp, VB_exp):\n",
    "        self.CV = inputs[:, :, :2] # Extract [Vt, E]\n",
    "        self.init = init[:, 2:] # Extract state variables\n",
    "        self.masks = masks\n",
    "        self.seq_lengths = seq_lengths\n",
    "\n",
    "        self.states = torch.stack([CFLA_exp, CALA_exp, CFK_exp, CBK_exp, VF_exp, VA_exp, VB_exp], dim=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.CV)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'CV': self.CV[idx],\n",
    "            'init': self.init[idx],\n",
    "            'masks': self.masks[idx],\n",
    "            'seq_len': self.seq_lengths[idx],\n",
    "            'states': self.states[idx],\n",
    "        }\n",
    "train_dataset = BMEDDataset(input_tensor, init, mask, seq_lengths, CFLA, CALA, CFK, CBK, VF, VA, VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BMEDModel(\n",
       "  (layer_norm): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "  (rnn_layers): LSTM(9, 128, num_layers=5, batch_first=True, dropout=0.2)\n",
       "  (flux_NN): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=86, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=86, out_features=45, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=45, out_features=4, bias=True)\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Initialization\n",
    "class BMEDModel(nn.Module):\n",
    "    def __init__(self, hidden_nodes = 64, num_rnn_layers = 2, num_fnn_layers = 2,max_len = 37, dt = 0.25):\n",
    "        super(BMEDModel, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.input_features = 9 # [Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        self.control_features = 2 # [Vt, E]\n",
    "        self.state_features = 7 # [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        self.flux_features = 4 # [dLA, dK, dH2O_A, dH2O_B]\n",
    "        self.dt = dt # time step\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.ranges = {\n",
    "            'CFLA': {'min': -1, 'max': 4},\n",
    "            'CALA': {'min': -1, 'max': 4}, \n",
    "            'CFK': {'min': -1, 'max': 7},\n",
    "            'CBK': {'min': -1, 'max': 2},\n",
    "            'VF': {'min': 0, 'max': 2},\n",
    "            'VA': {'min': 0, 'max': 2},\n",
    "            'VB': {'min': 0, 'max': 8}\n",
    "        }\n",
    "\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(self.input_features)\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn_layers = nn.LSTM(\n",
    "            input_size = self.input_features,\n",
    "            hidden_size = hidden_nodes,\n",
    "            num_layers = num_rnn_layers,\n",
    "            batch_first = True,\n",
    "            dropout = 0.2 if num_rnn_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Flux Head\n",
    "        flux_layers = []\n",
    "        flux_sizes = [hidden_nodes]\n",
    "        flux_step = (hidden_nodes - self.flux_features) / (num_fnn_layers)\n",
    "        \n",
    "        for i in range(num_fnn_layers):\n",
    "            next_size = int(hidden_nodes - flux_step * (i + 1))\n",
    "            if i == num_fnn_layers - 1:\n",
    "                next_size = self.flux_features\n",
    "            \n",
    "            flux_layers.append(nn.Linear(flux_sizes[-1], next_size))\n",
    "            flux_layers.append(nn.ELU())\n",
    "            flux_layers.append(nn.Dropout(0.2))\n",
    "            flux_sizes.append(next_size)\n",
    "            \n",
    "        self.flux_NN = nn.Sequential(*flux_layers)\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initialize hidden states for RNN layers\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(self.num_rnn_layers, batch_size, self.hidden_nodes, device=device)\n",
    "        c0 = torch.zeros(self.num_rnn_layers, batch_size, self.hidden_nodes, device=device)\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def denormalize_state(self, norm_state):\n",
    "        '''\n",
    "        Normalized state to real values \n",
    "        '''\n",
    "        batch_size = norm_state.shape[0]\n",
    "        real_state = torch.zeros_like(norm_state)\n",
    "\n",
    "        state_names = ['CFLA', 'CALA', 'CFK', 'CBK', 'VF', 'VA', 'VB']\n",
    "\n",
    "        for i, name in enumerate(state_names):\n",
    "            min_val = self.ranges[name]['min']\n",
    "            max_val = self.ranges[name]['max']\n",
    "            real_state[:, i] = norm_state[:, i] * (max_val - min_val) + min_val\n",
    "        \n",
    "        return real_state\n",
    "    \n",
    "    def normalize_state(self, real_state):\n",
    "        '''\n",
    "        Real state to normalized values\n",
    "        '''\n",
    "        batch_size = real_state.shape[0]\n",
    "        norm_state = torch.zeros_like(real_state)\n",
    "\n",
    "        state_names = ['CFLA', 'CALA', 'CFK', 'CBK', 'VF', 'VA', 'VB']\n",
    "\n",
    "        for i, name in enumerate(state_names):\n",
    "            min_val = self.ranges[name]['min']\n",
    "            max_val = self.ranges[name]['max']\n",
    "            norm_state[:, i] = (real_state[:, i] - min_val) / (max_val - min_val)\n",
    "\n",
    "        return norm_state\n",
    "\n",
    "    def MB_step(self, norm_state, LA_flux, K_flux, VFA_flux, VFB_flux):\n",
    "        '''\n",
    "        Perform one time step of mass balance\n",
    "        state: [batch, 7] - [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        '''\n",
    "        real_state = self.denormalize_state(norm_state)\n",
    "\n",
    "        # extract current values\n",
    "        # channel 0: feed, channel 1: acid, channel 2: base\n",
    "        # property 0: LA_conc, property 1: K_conc, property 2: volume\n",
    "\n",
    "        CFLA = real_state[:, 0]\n",
    "        CALA = real_state[:, 1]\n",
    "        CFK = real_state[:, 2]\n",
    "        CBK = real_state[:, 3]\n",
    "        VF = real_state[:, 4]\n",
    "        VA = real_state[:, 5]\n",
    "        VB = real_state[:, 6]\n",
    "\n",
    "        # volume changes due to water flux\n",
    "        # Assuming positive flux means water moves from feed to acid or base\n",
    "        nVF = VF - (VFA_flux + VFB_flux) * self.dt # Feed Volume\n",
    "        nVA = VA + VFA_flux * self.dt # Acid Volume\n",
    "        nVB = VB + VFB_flux * self.dt # Base Volume\n",
    "\n",
    "        # LA mass balance\n",
    "        nNFLA = CFLA*VF - LA_flux*self.dt\n",
    "        nNALA = CALA*VA + LA_flux*self.dt\n",
    "        nNFK = CFK*VF - K_flux*self.dt\n",
    "        nNBK = CBK*VB + K_flux*self.dt\n",
    "\n",
    "        # update states\n",
    "        new_real_state = torch.zeros_like(real_state)\n",
    "        new_real_state[:, 0] = nNFLA / (nVF + 1e-8) # new Feed LA concentration\n",
    "        new_real_state[:, 1] = nNALA / (nVA + 1e-8) # new Acid LA concentration\n",
    "        new_real_state[:, 2] = nNFK / (nVF + 1e-8) # new Feed K concentration\n",
    "        new_real_state[:, 3] = nNBK / (nVB + 1e-8) # new Base K concentration\n",
    "        new_real_state[:, 4] = nVF # new Feed Volume\n",
    "        new_real_state[:, 5] = nVA # new Acid Volume\n",
    "        new_real_state[:, 6] = nVB # new Base Volume\n",
    "\n",
    "        new_norm_state = self.normalize_state(new_real_state)\n",
    "    \n",
    "        return new_norm_state\n",
    "    \n",
    "    def forward_single_step(self, CV, prev_state, hidden_state):\n",
    "        '''\n",
    "        Predict Single Step of BMED\n",
    "        CV: [batch_size, 2] - [Vt, E]\n",
    "        prev_state: [batch_size, 7] - [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        hidden_state: [h, c] - hidden state of RNN layers\n",
    "        '''\n",
    "\n",
    "        full_input = torch.cat([CV, prev_state], dim=1)\n",
    "        full_input = full_input.unsqueeze(1) # [batch_size, 1, 9]\n",
    "\n",
    "        # Layer Normalization\n",
    "        rnn_input = self.layer_norm(full_input)\n",
    "        \n",
    "        # RNN forward - pack all information of previous hidden state\n",
    "        rnn_out, new_hidden_state = self.rnn_layers(rnn_input, hidden_state)\n",
    "\n",
    "        # Predict flux and current\n",
    "        flux = self.flux_NN(rnn_out.squeeze(1)) # [batch_size, 4]\n",
    "\n",
    "        # Physical update\n",
    "        LA_flux = flux[:, 0]\n",
    "        K_flux = flux[:, 1]\n",
    "        VFA_flux = flux[:, 2]\n",
    "        VFB_flux = flux[:, 3]\n",
    "\n",
    "        new_state = self.MB_step(prev_state, LA_flux, K_flux, VFA_flux, VFB_flux)\n",
    "\n",
    "        return new_state, new_hidden_state\n",
    "    \n",
    "    def next_seq_pred_train(self, CV, init, states, masks):\n",
    "        '''\n",
    "        Autoregressive training\n",
    "        '''\n",
    "        batch_size, max_seq_len, _ = CV.shape\n",
    "        device= CV.device\n",
    "\n",
    "        mask = masks # [batch_size, max_seq_len]\n",
    "\n",
    "        CVf = CV[:, 0, :] # [batch_size, 2]\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden_state = self.init_hidden(batch_size,device)\n",
    "        current_state = init.clone() # [batch_size, 7]\n",
    "\n",
    "        # predicted results\n",
    "        predicted_states = []\n",
    "\n",
    "        # use init_state at t=0\n",
    "        predicted_states.append(init)\n",
    "\n",
    "        # Calculate for each timestep with the entire batch simultaneously\n",
    "        for t in range(1,max_seq_len):\n",
    "\n",
    "            # Predict current and states of the entire batch\n",
    "            pred_state, hidden_state = self.forward_single_step(CVf, current_state, hidden_state)\n",
    "\n",
    "            predicted_states.append(pred_state)\n",
    "\n",
    "            # update current state with predicted states\n",
    "            current_state = pred_state\n",
    "\n",
    "        # Convert to tensor\n",
    "        predicted_states = torch.stack(predicted_states, dim=1) # [batch_size, max_seq_len, 7]\n",
    "\n",
    "        # Loss calculation: \n",
    "        states_exp = states[:, 1:, :] # [batch_size, seq_len-1, 7]\n",
    "        states_pred = predicted_states[:, 1:, :] # [batch_size, seq_len-1, 7]\n",
    "        mask_adj = mask[:, 1:] # [batch_size, seq_len-1]\n",
    "\n",
    "        # selected loss features\n",
    "        selected_indices = [0, 2, 5, 6]\n",
    "        states_exp_loss = states_exp[:, :, selected_indices]\n",
    "        states_pred_loss = states_pred[:, :, selected_indices]\n",
    "        \n",
    "        # Calculate loss\n",
    "        states_loss = F.mse_loss(states_pred_loss, states_exp_loss, reduction='none')\n",
    "\n",
    "        # Apply mask and calculate mean loss\n",
    "        mask_states = mask_adj.unsqueeze(-1).expand(-1, -1, len(selected_indices))\n",
    "\n",
    "        masked_states_loss = (states_loss * mask_states).sum() / mask_states.sum()\n",
    "\n",
    "        # total_loss = masked_current_loss + masked_states_loss*7\n",
    "        total_loss = masked_states_loss\n",
    "\n",
    "        return total_loss, predicted_states\n",
    "    \n",
    "    def generate_sequence(self, CV, init, max_steps):\n",
    "        '''\n",
    "        Prediction mode from initial state\n",
    "\n",
    "        Args:\n",
    "            CV: [2] or [batch_size, 2] - [Vt, E]\n",
    "            init_state: initial state ([7] or [batch_size, 7])\n",
    "            max_steps: maximum number of steps to generate\n",
    "        '''\n",
    "\n",
    "        if len(init.shape) == 1:\n",
    "            # if input is single exp, add a batch dimension\n",
    "            init = init.unsqueeze(0)\n",
    "            batch_size = 1\n",
    "            single_exp = True\n",
    "        else:\n",
    "            batch_size = init.shape[0]\n",
    "            single_exp = False\n",
    "        \n",
    "        device = init.device\n",
    "\n",
    "        # check CV batch size and modification\n",
    "        if len(CV.shape) == 1:\n",
    "            CV = CV.unsqueeze(0).unsqueeze(0) # [1, 1, 2]\n",
    "        elif len(CV.shape) == 2:\n",
    "            CV = CV.unsqueeze(1) # [batch_size, 1, 2]\n",
    "        \n",
    "        CVf = CV[:, 0, :].to(device) # [batch_size, 2]\n",
    "        \n",
    "\n",
    "        # initialize hidden state\n",
    "        hidden_state = self.init_hidden(batch_size, device)\n",
    "        current_state = init.clone() # [batch_size, 7]\n",
    "\n",
    "        predicted_states = []\n",
    "        \n",
    "        predicted_states.append(init)\n",
    "\n",
    "        for t in range(1, max_steps):\n",
    "            pred_state, hidden_state = self.forward_single_step(CVf, current_state, hidden_state)\n",
    "\n",
    "            predicted_states.append(pred_state) # [batch_size, 7]\n",
    "\n",
    "            current_state = pred_state\n",
    "\n",
    "        predicted_states = torch.stack(predicted_states, dim=1) # [batch_size, max_steps, 7]\n",
    "\n",
    "        if single_exp:\n",
    "            predicted_states = predicted_states.squeeze(0) # [max_steps, 7]\n",
    "        \n",
    "        return predicted_states\n",
    "\n",
    "    def forward(self, CV, init, states, masks, mode='train', max_steps=None):\n",
    "        '''\n",
    "        Integrated forward method\n",
    "        '''\n",
    "\n",
    "        if mode == 'train':\n",
    "            if states is None or masks is None:\n",
    "                raise ValueError(\"states, and masks are required for training\")\n",
    "            return self.next_seq_pred_train(CV, init, states, masks)\n",
    "        elif mode == 'inference':\n",
    "            if max_steps is None:\n",
    "                raise ValueError(\"max_steps is required for inference\")\n",
    "            fixed_control = CV[:, 0, :] # [batch_size, 2]\n",
    "            return self.generate_sequence(CV, init, max_steps)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Use 'train' or 'inference'\")\n",
    "dmodel = 128\n",
    "model = BMEDModel(\n",
    "    hidden_nodes=dmodel,\n",
    "    num_rnn_layers=5,\n",
    "    num_fnn_layers=3,\n",
    "    max_len=max_length,\n",
    "    dt=0.25\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noam Scheduler\n",
    "class NoamLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    '''\n",
    "    Pytorch LRScheduler 스타일으 Noam Scheduler\n",
    "    '''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=40, last_epoch=-1):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(NoamLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        if step == 0:\n",
    "            step  = 1\n",
    "\n",
    "        lr_scale = (self.d_model ** -0.5) *min(\n",
    "            step ** -0.5,\n",
    "            step * (self.warmup_steps ** -1.5)\n",
    "        )\n",
    "        return [base_lr * lr_scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "num_epochs = 1000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1)\n",
    "scheduler = NoamLR(optimizer, d_model=dmodel, warmup_steps=0.1*num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "==================================================\n",
      "Epoch 1/1000, Loss: 0.028145, LR: 0.000177, best_loss: 0.028145\n",
      "Epoch 11/1000, Loss: 0.003583, LR: 0.001061, best_loss: 0.003583\n",
      "Epoch 21/1000, Loss: 0.002735, LR: 0.001945, best_loss: 0.002426\n",
      "Epoch 31/1000, Loss: 0.002085, LR: 0.002828, best_loss: 0.002085\n",
      "Epoch 41/1000, Loss: 0.002392, LR: 0.003712, best_loss: 0.001763\n",
      "Epoch 51/1000, Loss: 0.002995, LR: 0.004596, best_loss: 0.001552\n",
      "Epoch 61/1000, Loss: 0.001556, LR: 0.005480, best_loss: 0.001448\n",
      "Epoch 71/1000, Loss: 0.001907, LR: 0.006364, best_loss: 0.001159\n",
      "Epoch 81/1000, Loss: 0.001259, LR: 0.007248, best_loss: 0.001159\n",
      "Epoch 91/1000, Loss: 0.001153, LR: 0.008132, best_loss: 0.001054\n",
      "Epoch 101/1000, Loss: 0.001595, LR: 0.008752, best_loss: 0.001054\n",
      "Epoch 111/1000, Loss: 0.000926, LR: 0.008352, best_loss: 0.000926\n",
      "Epoch 121/1000, Loss: 0.001020, LR: 0.008002, best_loss: 0.000873\n",
      "Epoch 131/1000, Loss: 0.001464, LR: 0.007693, best_loss: 0.000775\n",
      "Epoch 141/1000, Loss: 0.001038, LR: 0.007417, best_loss: 0.000722\n",
      "Epoch 151/1000, Loss: 0.000975, LR: 0.007169, best_loss: 0.000722\n",
      "Epoch 161/1000, Loss: 0.001165, LR: 0.006944, best_loss: 0.000722\n",
      "Epoch 171/1000, Loss: 0.000812, LR: 0.006740, best_loss: 0.000722\n",
      "Epoch 181/1000, Loss: 0.000960, LR: 0.006552, best_loss: 0.000722\n",
      "Epoch 191/1000, Loss: 0.000883, LR: 0.006379, best_loss: 0.000706\n",
      "Epoch 201/1000, Loss: 0.000957, LR: 0.006219, best_loss: 0.000706\n",
      "Epoch 211/1000, Loss: 0.000827, LR: 0.006071, best_loss: 0.000668\n",
      "Epoch 221/1000, Loss: 0.000682, LR: 0.005932, best_loss: 0.000668\n",
      "Epoch 231/1000, Loss: 0.000775, LR: 0.005803, best_loss: 0.000636\n",
      "Epoch 241/1000, Loss: 0.000859, LR: 0.005682, best_loss: 0.000602\n",
      "Epoch 251/1000, Loss: 0.000795, LR: 0.005568, best_loss: 0.000602\n",
      "Epoch 261/1000, Loss: 0.000708, LR: 0.005461, best_loss: 0.000602\n",
      "Epoch 271/1000, Loss: 0.000733, LR: 0.005359, best_loss: 0.000602\n",
      "Epoch 281/1000, Loss: 0.000745, LR: 0.005263, best_loss: 0.000602\n",
      "Epoch 291/1000, Loss: 0.000762, LR: 0.005173, best_loss: 0.000602\n",
      "Epoch 301/1000, Loss: 0.000752, LR: 0.005086, best_loss: 0.000602\n",
      "Epoch 311/1000, Loss: 0.000814, LR: 0.005004, best_loss: 0.000562\n",
      "Epoch 321/1000, Loss: 0.000738, LR: 0.004926, best_loss: 0.000509\n",
      "Epoch 331/1000, Loss: 0.000670, LR: 0.004851, best_loss: 0.000509\n",
      "Epoch 341/1000, Loss: 0.000613, LR: 0.004779, best_loss: 0.000509\n",
      "Epoch 351/1000, Loss: 0.000777, LR: 0.004711, best_loss: 0.000509\n",
      "Epoch 361/1000, Loss: 0.000689, LR: 0.004646, best_loss: 0.000509\n",
      "Epoch 371/1000, Loss: 0.000746, LR: 0.004583, best_loss: 0.000509\n",
      "Epoch 381/1000, Loss: 0.000719, LR: 0.004522, best_loss: 0.000509\n",
      "Epoch 391/1000, Loss: 0.000743, LR: 0.004464, best_loss: 0.000509\n",
      "Epoch 401/1000, Loss: 0.000766, LR: 0.004408, best_loss: 0.000509\n",
      "Epoch 411/1000, Loss: 0.000764, LR: 0.004355, best_loss: 0.000509\n",
      "Epoch 421/1000, Loss: 0.000641, LR: 0.004303, best_loss: 0.000509\n",
      "Epoch 431/1000, Loss: 0.000769, LR: 0.004253, best_loss: 0.000509\n",
      "Epoch 441/1000, Loss: 0.000715, LR: 0.004204, best_loss: 0.000493\n",
      "Epoch 451/1000, Loss: 0.000895, LR: 0.004157, best_loss: 0.000493\n",
      "Epoch 461/1000, Loss: 0.000665, LR: 0.004112, best_loss: 0.000493\n",
      "Epoch 471/1000, Loss: 0.000625, LR: 0.004068, best_loss: 0.000493\n",
      "Epoch 481/1000, Loss: 0.000670, LR: 0.004026, best_loss: 0.000493\n",
      "Epoch 491/1000, Loss: 0.000733, LR: 0.003985, best_loss: 0.000493\n",
      "Epoch 501/1000, Loss: 0.000731, LR: 0.003945, best_loss: 0.000493\n",
      "Epoch 511/1000, Loss: 0.000684, LR: 0.003906, best_loss: 0.000493\n",
      "Epoch 521/1000, Loss: 0.000636, LR: 0.003869, best_loss: 0.000493\n",
      "Epoch 531/1000, Loss: 0.000538, LR: 0.003832, best_loss: 0.000493\n",
      "Epoch 541/1000, Loss: 0.000699, LR: 0.003797, best_loss: 0.000481\n",
      "Epoch 551/1000, Loss: 0.000700, LR: 0.003762, best_loss: 0.000481\n",
      "Epoch 561/1000, Loss: 0.000585, LR: 0.003728, best_loss: 0.000481\n",
      "Epoch 571/1000, Loss: 0.000650, LR: 0.003696, best_loss: 0.000481\n",
      "Epoch 581/1000, Loss: 0.000643, LR: 0.003664, best_loss: 0.000481\n",
      "Epoch 591/1000, Loss: 0.000621, LR: 0.003633, best_loss: 0.000481\n",
      "Epoch 601/1000, Loss: 0.000635, LR: 0.003602, best_loss: 0.000481\n",
      "Epoch 611/1000, Loss: 0.000681, LR: 0.003573, best_loss: 0.000481\n",
      "Epoch 621/1000, Loss: 0.000490, LR: 0.003544, best_loss: 0.000481\n",
      "Epoch 631/1000, Loss: 0.000556, LR: 0.003516, best_loss: 0.000472\n",
      "Epoch 641/1000, Loss: 0.000592, LR: 0.003488, best_loss: 0.000472\n",
      "Epoch 651/1000, Loss: 0.000471, LR: 0.003462, best_loss: 0.000471\n",
      "Epoch 661/1000, Loss: 0.000646, LR: 0.003435, best_loss: 0.000471\n",
      "Epoch 671/1000, Loss: 0.000484, LR: 0.003410, best_loss: 0.000471\n",
      "Epoch 681/1000, Loss: 0.000536, LR: 0.003385, best_loss: 0.000434\n",
      "Epoch 691/1000, Loss: 0.000753, LR: 0.003360, best_loss: 0.000434\n",
      "Epoch 701/1000, Loss: 0.000517, LR: 0.003336, best_loss: 0.000434\n",
      "Epoch 711/1000, Loss: 0.000578, LR: 0.003312, best_loss: 0.000434\n",
      "Epoch 721/1000, Loss: 0.000627, LR: 0.003289, best_loss: 0.000434\n",
      "Epoch 731/1000, Loss: 0.000491, LR: 0.003267, best_loss: 0.000434\n",
      "Epoch 741/1000, Loss: 0.000636, LR: 0.003245, best_loss: 0.000434\n",
      "Epoch 751/1000, Loss: 0.000557, LR: 0.003223, best_loss: 0.000434\n",
      "Epoch 761/1000, Loss: 0.000581, LR: 0.003202, best_loss: 0.000434\n",
      "Epoch 771/1000, Loss: 0.000813, LR: 0.003181, best_loss: 0.000434\n",
      "Epoch 781/1000, Loss: 0.000474, LR: 0.003161, best_loss: 0.000434\n",
      "Epoch 791/1000, Loss: 0.000471, LR: 0.003141, best_loss: 0.000434\n",
      "Epoch 801/1000, Loss: 0.000548, LR: 0.003121, best_loss: 0.000434\n",
      "Epoch 811/1000, Loss: 0.000583, LR: 0.003102, best_loss: 0.000403\n",
      "Epoch 821/1000, Loss: 0.000551, LR: 0.003083, best_loss: 0.000403\n",
      "Epoch 831/1000, Loss: 0.000600, LR: 0.003064, best_loss: 0.000403\n",
      "Epoch 841/1000, Loss: 0.000455, LR: 0.003046, best_loss: 0.000403\n",
      "Epoch 851/1000, Loss: 0.000534, LR: 0.003028, best_loss: 0.000403\n",
      "Epoch 861/1000, Loss: 0.000514, LR: 0.003011, best_loss: 0.000403\n",
      "Epoch 871/1000, Loss: 0.000460, LR: 0.002993, best_loss: 0.000403\n",
      "Epoch 881/1000, Loss: 0.000503, LR: 0.002976, best_loss: 0.000403\n",
      "Epoch 891/1000, Loss: 0.000503, LR: 0.002959, best_loss: 0.000403\n",
      "Epoch 901/1000, Loss: 0.000481, LR: 0.002943, best_loss: 0.000371\n",
      "Epoch 911/1000, Loss: 0.000493, LR: 0.002927, best_loss: 0.000371\n",
      "Epoch 921/1000, Loss: 0.000514, LR: 0.002911, best_loss: 0.000371\n",
      "Epoch 931/1000, Loss: 0.000465, LR: 0.002895, best_loss: 0.000371\n",
      "Epoch 941/1000, Loss: 0.000653, LR: 0.002880, best_loss: 0.000371\n",
      "Epoch 951/1000, Loss: 0.000407, LR: 0.002865, best_loss: 0.000371\n",
      "Epoch 961/1000, Loss: 0.000443, LR: 0.002850, best_loss: 0.000371\n",
      "Epoch 971/1000, Loss: 0.000456, LR: 0.002835, best_loss: 0.000371\n",
      "Epoch 981/1000, Loss: 0.000443, LR: 0.002821, best_loss: 0.000371\n",
      "Epoch 991/1000, Loss: 0.000372, LR: 0.002806, best_loss: 0.000371\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, device):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    best_loss = np.inf\n",
    "    best_states = None\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # move to device\n",
    "        CV = batch['CV'].to(device)\n",
    "        init = batch['init'].to(device)\n",
    "        states = batch['states'].to(device)\n",
    "        masks = batch['masks'].to(device)\n",
    "\n",
    "        # gradient initialization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss, pred_states = model(CV, init, states, masks, mode='train')\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 1)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # update scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    return total_loss / num_batches, pred_states\n",
    "\n",
    "print(\"Start training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_loss = np.inf\n",
    "best_current = None\n",
    "best_states = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, pred_states = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        best_states = pred_states\n",
    "\n",
    "    if epoch % 10 == 0: \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.6f}, LR: {current_lr:.6f}, best_loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터 구조 확인 ===\n",
      "train_dataset.init.shape: torch.Size([30, 7])\n",
      "train_dataset.states.shape: torch.Size([30, 37, 7])\n",
      "train_dataset.CV.shape: torch.Size([30, 37, 2])\n",
      "train_dataset.seq_lengths.shape: torch.Size([30])\n",
      "\n",
      "=== 테스트 데이터 차원 확인 ===\n",
      "test_init.shape: torch.Size([1, 7])\n",
      "test_states.shape: torch.Size([1, 37, 7])\n",
      "test_CV.shape: torch.Size([1, 37, 2])\n",
      "test_seq_len: tensor([27])\n"
     ]
    }
   ],
   "source": [
    "# 먼저 데이터 구조 확인 (디버깅)\n",
    "test_idx = 0\n",
    "print(\"=== 데이터 구조 확인 ===\")\n",
    "print(f\"train_dataset.init.shape: {train_dataset.init.shape}\")\n",
    "print(f\"train_dataset.states.shape: {train_dataset.states.shape}\")\n",
    "print(f\"train_dataset.CV.shape: {train_dataset.CV.shape}\")\n",
    "print(f\"train_dataset.seq_lengths.shape: {train_dataset.seq_lengths.shape}\")\n",
    "\n",
    "# 테스트 데이터 준비 (수정된 방법)\n",
    "test_init = train_dataset.init[test_idx:test_idx+1]  # 올바른 init 사용\n",
    "test_states = train_dataset.states[test_idx:test_idx+1]  # 전체 상태 시퀀스\n",
    "test_CV = train_dataset.CV[test_idx:test_idx+1]  # 제어 변수\n",
    "test_seq_len = train_dataset.seq_lengths[test_idx:test_idx+1]  # 시퀀스 길이\n",
    "\n",
    "print(f\"\\n=== 테스트 데이터 차원 확인 ===\")\n",
    "print(f\"test_init.shape: {test_init.shape}\")\n",
    "print(f\"test_states.shape: {test_states.shape}\")\n",
    "print(f\"test_CV.shape: {test_CV.shape}\")\n",
    "print(f\"test_seq_len: {test_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 준비\n",
    "test_idx = 0\n",
    "test_init = train_dataset.init[test_idx:test_idx+1]  # 올바른 init 사용\n",
    "test_states = train_dataset.states[test_idx:test_idx+1]\n",
    "test_seq_len = train_dataset.seq_lengths[test_idx:test_idx+1]\n",
    "test_CV = train_dataset.CV[test_idx:test_idx+1]\n",
    "\n",
    "# 데이터 유효성 검사\n",
    "if len(test_seq_len) == 0:\n",
    "    raise ValueError(\"테스트 시퀀스 길이가 비어있습니다.\")\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 초기 상태 설정\n",
    "init = test_init.to(device)  # [1, 7] - 올바른 차원\n",
    "seq_length = test_seq_len[0].item()\n",
    "\n",
    "CV = test_CV.to(device)\n",
    "\n",
    "# 예측 수행\n",
    "with torch.no_grad():\n",
    "    pred_states = model(CV, init, states=None, masks= None,max_steps=seq_length, mode='inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 30개의 실험 데이터가 있습니다.\n",
      "모든 실험에 대한 예측을 수행 중...\n",
      "실험 0 완료 (시퀀스 길이: 27)\n",
      "실험 1 완료 (시퀀스 길이: 17)\n",
      "실험 2 완료 (시퀀스 길이: 25)\n",
      "실험 3 완료 (시퀀스 길이: 21)\n",
      "실험 4 완료 (시퀀스 길이: 21)\n",
      "실험 5 완료 (시퀀스 길이: 25)\n",
      "실험 6 완료 (시퀀스 길이: 21)\n",
      "실험 7 완료 (시퀀스 길이: 17)\n",
      "실험 8 완료 (시퀀스 길이: 21)\n",
      "실험 9 완료 (시퀀스 길이: 29)\n",
      "실험 10 완료 (시퀀스 길이: 25)\n",
      "실험 11 완료 (시퀀스 길이: 17)\n",
      "실험 12 완료 (시퀀스 길이: 21)\n",
      "실험 13 완료 (시퀀스 길이: 29)\n",
      "실험 14 완료 (시퀀스 길이: 29)\n",
      "실험 15 완료 (시퀀스 길이: 37)\n",
      "실험 16 완료 (시퀀스 길이: 29)\n",
      "실험 17 완료 (시퀀스 길이: 33)\n",
      "실험 18 완료 (시퀀스 길이: 33)\n",
      "실험 19 완료 (시퀀스 길이: 29)\n",
      "실험 20 완료 (시퀀스 길이: 29)\n",
      "실험 21 완료 (시퀀스 길이: 25)\n",
      "실험 22 완료 (시퀀스 길이: 25)\n",
      "실험 23 완료 (시퀀스 길이: 25)\n",
      "실험 24 완료 (시퀀스 길이: 29)\n",
      "실험 25 완료 (시퀀스 길이: 29)\n",
      "실험 26 완료 (시퀀스 길이: 29)\n",
      "실험 27 완료 (시퀀스 길이: 25)\n",
      "실험 28 완료 (시퀀스 길이: 25)\n",
      "실험 29 완료 (시퀀스 길이: 29)\n",
      "모든 예측 완료!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type 'range' is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m pred_data = all_predictions[exp_idx][:seq_length, state_idx]\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# 정규화 해제 (denormalize)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m actual_data = actual_data * \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstate_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     69\u001b[39m pred_data = pred_data * \u001b[38;5;28mrange\u001b[39m[state_idx]\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# 시간 단위로 변환 (dt = 0.25 hr)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: type 'range' is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모든 실험에 대한 예측 수행\n",
    "\n",
    "# 전체 데이터셋 크기 확인\n",
    "num_experiments = len(train_dataset)\n",
    "print(f\"총 {num_experiments}개의 실험 데이터가 있습니다.\")\n",
    "\n",
    "# 모든 실험에 대한 예측 수행\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "all_seq_lengths = []\n",
    "\n",
    "print(\"모든 실험에 대한 예측을 수행 중...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for exp_idx in range(num_experiments):\n",
    "        # 각 실험의 데이터 준비\n",
    "        test_init = train_dataset.init[exp_idx:exp_idx+1]\n",
    "        test_states = train_dataset.states[exp_idx:exp_idx+1]\n",
    "        test_CV = train_dataset.CV[exp_idx:exp_idx+1]\n",
    "        test_seq_len = train_dataset.seq_lengths[exp_idx]\n",
    "        \n",
    "        # GPU로 이동\n",
    "        init = test_init.to(device)\n",
    "        CV = test_CV.to(device)\n",
    "        seq_length = test_seq_len.item()\n",
    "        \n",
    "        # 예측 수행\n",
    "        pred_states = model(CV, init, states=None, masks=None, \n",
    "                           max_steps=seq_length, mode='inference')\n",
    "        \n",
    "        # 결과 저장\n",
    "        actual_states = test_states[0].cpu().numpy()\n",
    "        if len(pred_states.shape) == 3:\n",
    "            pred_states_np = pred_states[0].cpu().numpy()\n",
    "        else:\n",
    "            pred_states_np = pred_states.cpu().numpy()\n",
    "        \n",
    "        all_predictions.append(pred_states_np)\n",
    "        all_actuals.append(actual_states)\n",
    "        all_seq_lengths.append(seq_length)\n",
    "        \n",
    "        print(f\"실험 {exp_idx} 완료 (시퀀스 길이: {seq_length})\")\n",
    "\n",
    "print(\"모든 예측 완료!\")\n",
    "\n",
    "# 상태 변수 정보\n",
    "state_names = ['CFLA', 'CALA', 'CFK', 'CBK', 'VF', 'VA', 'VB']\n",
    "state_indices = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# 각 실험별로 모든 상태 변수를 한 번에 시각화\n",
    "for exp_idx in range(num_experiments):\n",
    "    # 3x3 서브플롯 생성 (7개 변수용)\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    seq_length = all_seq_lengths[exp_idx]\n",
    "    \n",
    "    # 각 상태 변수별 플롯\n",
    "    for state_idx, state_name in enumerate(state_names):\n",
    "        ax = axes[state_idx]\n",
    "        \n",
    "        # 해당 실험의 데이터\n",
    "        actual_data = all_actuals[exp_idx][:seq_length, state_idx]\n",
    "        pred_data = all_predictions[exp_idx][:seq_length, state_idx]\n",
    "        time_steps = range(seq_length)\n",
    "        \n",
    "        # 플롯\n",
    "        ax.plot(time_steps, actual_data, 'o-', label='Actual', alpha=0.8,\n",
    "                markersize=5, linewidth=2, color='blue')\n",
    "        ax.plot(time_steps, pred_data, 's--', label='Predicted', alpha=0.8,\n",
    "                markersize=4, linewidth=2, color='red')\n",
    "        \n",
    "        ax.set_title(f'{state_name}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time step', fontsize=10)\n",
    "        ax.set_ylabel('Normalized value', fontsize=10)\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "        \n",
    "        # R² 계산 및 표시\n",
    "        r2 = r2_score(actual_data, pred_data)\n",
    "        ax.text(0.02, 0.98, f'R² = {r2:.4f}', transform=ax.transAxes,\n",
    "                verticalalignment='top', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    # 빈 서브플롯 제거\n",
    "    for i in range(len(state_names), len(axes)):\n",
    "        axes[i].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Experiment {exp_idx} - All State Variables (Sequence Length: {seq_length})', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# 전체 정량적 평가 결과 (R² 기준)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"전체 실험에 대한 R² 평가 결과\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 각 상태 변수별 전체 R² 계산\n",
    "overall_results = []\n",
    "\n",
    "for state_idx, state_name in enumerate(state_names):\n",
    "    all_r2 = []\n",
    "    \n",
    "    for exp_idx in range(num_experiments):\n",
    "        actual_data = all_actuals[exp_idx][:all_seq_lengths[exp_idx], state_idx]\n",
    "        pred_data = all_predictions[exp_idx][:all_seq_lengths[exp_idx], state_idx]\n",
    "        \n",
    "        r2 = r2_score(actual_data, pred_data)\n",
    "        all_r2.append(r2)\n",
    "    \n",
    "    # 통계 계산\n",
    "    mean_r2 = np.mean(all_r2)\n",
    "    std_r2 = np.std(all_r2)\n",
    "    min_r2 = np.min(all_r2)\n",
    "    max_r2 = np.max(all_r2)\n",
    "    \n",
    "    overall_results.append({\n",
    "        'name': state_name,\n",
    "        'r2_mean': mean_r2,\n",
    "        'r2_std': std_r2,\n",
    "        'r2_min': min_r2,\n",
    "        'r2_max': max_r2,\n",
    "        'all_r2': all_r2\n",
    "    })\n",
    "    \n",
    "    print(f\"{state_name:4s}: R² = {mean_r2:.4f}±{std_r2:.4f} \"\n",
    "          f\"(min: {min_r2:.4f}, max: {max_r2:.4f})\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "# 전체 평균 R²\n",
    "total_r2_mean = np.mean([r['r2_mean'] for r in overall_results])\n",
    "print(f\"전체 평균 R²: {total_r2_mean:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 실험별 성능 요약 (전체 상태 변수 평균 R²)\n",
    "print(\"\\n실험별 R² 성능 요약\")\n",
    "print(\"-\"*50)\n",
    "exp_r2_summary = []\n",
    "\n",
    "for exp_idx in range(num_experiments):\n",
    "    exp_r2 = []\n",
    "    \n",
    "    for state_idx in range(len(state_names)):\n",
    "        actual_data = all_actuals[exp_idx][:all_seq_lengths[exp_idx], state_idx]\n",
    "        pred_data = all_predictions[exp_idx][:all_seq_lengths[exp_idx], state_idx]\n",
    "        \n",
    "        r2 = r2_score(actual_data, pred_data)\n",
    "        exp_r2.append(r2)\n",
    "    \n",
    "    mean_exp_r2 = np.mean(exp_r2)\n",
    "    exp_r2_summary.append(mean_exp_r2)\n",
    "    \n",
    "    print(f\"실험 {exp_idx}: 평균 R² = {mean_exp_r2:.4f}, 시퀀스 길이 = {all_seq_lengths[exp_idx]}\")\n",
    "\n",
    "# 상태 변수별 R² 히트맵 (선택사항)\n",
    "print(\"\\n상태 변수별 × 실험별 R² 매트릭스:\")\n",
    "print(\"-\"*50)\n",
    "print(\"실험\\\\변수\", end=\"\")\n",
    "for state_name in state_names:\n",
    "    print(f\"{state_name:>8s}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for exp_idx in range(num_experiments):\n",
    "    print(f\"실험 {exp_idx:2d}\", end=\"\")\n",
    "    for state_idx in range(len(state_names)):\n",
    "        actual_data = all_actuals[exp_idx][:all_seq_lengths[exp_idx], state_idx]\n",
    "        pred_data = all_predictions[exp_idx][:all_seq_lengths[exp_idx], state_idx]\n",
    "        r2 = r2_score(actual_data, pred_data)\n",
    "        print(f\"{r2:8.4f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n모든 실험에 대한 분석 완료!\")\n",
    "print(f\"총 {num_experiments}개 실험, {len(state_names)}개 상태 변수에 대한 R² 분석이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
