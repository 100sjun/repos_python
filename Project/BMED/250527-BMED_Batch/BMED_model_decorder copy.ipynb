{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Using device: {device}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"BMED_DB_augmented.csv\")\n",
    "df = df[df['exp'].isin([0,1,2,3,4])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust min-max scaling including safety margin\n",
    "ranges ={\n",
    "'V' : {'min':0, 'max':50},\n",
    "'E' : {'min':0, 'max':1},\n",
    "'VF' : {'min':0, 'max':2},\n",
    "'VA' : {'min':0, 'max':2},\n",
    "'VB' : {'min':0, 'max':8},\n",
    "'CF_LA' : {'min':-1, 'max':4},\n",
    "'CA_LA' : {'min':-1, 'max':4},\n",
    "'CF_K' : {'min':-1, 'max':7},\n",
    "'CB_K' : {'min':-1, 'max':2},\n",
    "'I' : {'min':0, 'max':5},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "ndf = pd.DataFrame()\n",
    "ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "for col in ['V', 'E', 'VF', 'VA', 'VB', 'CF_LA', 'CA_LA', 'CF_K', 'CB_K', 'I']:\n",
    "    if col in ranges:\n",
    "        ndf[col] = (df[col] - ranges[col]['min'])/(ranges[col]['max'] - ranges[col]['min'])\n",
    "    else:\n",
    "        ndf[col] = df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def prepare_data(ndf):\n",
    "    '''\n",
    "    prepare data list for each experiment\n",
    "\n",
    "    Args:\n",
    "        ndf: normalized dataframe\n",
    "    \n",
    "    Returns:\n",
    "        Vt_list: list of applied voltage\n",
    "        E_list: list of external electrolyte concentration\n",
    "        CFLA_list: list of feed LA concentration\n",
    "        CALA_list: list of acid LA concentration\n",
    "        CFK_list: list of feed K concentration\n",
    "        CBK_list: list of base K concentration\n",
    "        VF_list: list of feed volume\n",
    "        VA_list: list of acid volume\n",
    "        VB_list: list of base volume\n",
    "        I_list: list of current\n",
    "    '''\n",
    "    Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list, I_list = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    for exp_num in ndf['exp'].unique():\n",
    "        exp_data = ndf[ndf['exp'] == exp_num]\n",
    "\n",
    "        # operating conditions\n",
    "        Vt_list.append(exp_data['V'].values)\n",
    "        E_list.append(exp_data['E'].values)\n",
    "\n",
    "        # concentrations\n",
    "        CFLA_list.append(exp_data['CF_LA'].values)\n",
    "        CALA_list.append(exp_data['CA_LA'].values)\n",
    "        CFK_list.append(exp_data['CF_K'].values)\n",
    "        CBK_list.append(exp_data['CB_K'].values)\n",
    "\n",
    "        # volumes\n",
    "        VF_list.append(exp_data['VF'].values)\n",
    "        VA_list.append(exp_data['VA'].values)\n",
    "        VB_list.append(exp_data['VB'].values)\n",
    "\n",
    "        # current\n",
    "        I_list.append(exp_data['I'].values)\n",
    "\n",
    "    return Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list, I_list\n",
    "\n",
    "Vt_list, E_list, CFLA_list, CALA_list, CFK_list, CBK_list, VF_list, VA_list, VB_list, I_list = prepare_data(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "def pad_sequences(data_list, max_length=None, pad_value=-100.0):\n",
    "    '''\n",
    "    Pad variables length sequences to the same length\n",
    "\n",
    "    Args:\n",
    "        data_list: list of tensors with different sequence lengths\n",
    "        max_length: maximum length to pad to (default: longest sequence)\n",
    "        pad_value: value to use for padding\n",
    "\n",
    "    Returns:\n",
    "        padded_tensor: [batch_size, max_length, ...] - padded sequences\n",
    "        seq_lengths: [batch_size] - original sequence lengths\n",
    "    '''\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = max(data.shape[0] for data in data_list) # Auto-calculate the max length\n",
    "    \n",
    "    batch_size = len(data_list) # Batch size\n",
    "    seq_lengths = torch.tensor([data.shape[0] for data in data_list]) # Actual sequential length for each experiments\n",
    "    dimensions = data_list[0].shape[1:] # Get shape of individual elements\n",
    "    padded_tensor = torch.full((batch_size, max_length) + dimensions, pad_value, dtype=torch.float32) # generaste padded tensor filled with pad_value\n",
    "\n",
    "    # Fill with actual data\n",
    "    for i, data in enumerate(data_list):\n",
    "        padded_tensor[i, :data.shape[0]] = torch.tensor(data[:data.shape[0]], dtype=torch.float32)\n",
    "    \n",
    "    return padded_tensor, seq_lengths, max_length\n",
    "\n",
    "Vt, seq_lengths, max_length = pad_sequences(Vt_list)\n",
    "E, _, _ = pad_sequences(E_list,max_length = max_length)\n",
    "CFLA, _, _ = pad_sequences(CFLA_list,max_length = max_length)\n",
    "CALA, _, _ = pad_sequences(CALA_list,max_length = max_length)\n",
    "CFK, _, _ = pad_sequences(CFK_list,max_length = max_length)\n",
    "CBK, _, _ = pad_sequences(CBK_list,max_length = max_length)\n",
    "VF, _, _ = pad_sequences(VF_list,max_length = max_length)\n",
    "VA, _, _ = pad_sequences(VA_list,max_length = max_length)\n",
    "VB, _, _ = pad_sequences(VB_list,max_length = max_length)\n",
    "I, _, _ = pad_sequences(I_list,max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input tensor\n",
    "def prepare_input(Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB, seq_lengths):\n",
    "    '''\n",
    "    prepare input tensor for the model with padding support\n",
    "\n",
    "    Args:\n",
    "        Vt: applied voltage\n",
    "        E: external electrolyte concentration\n",
    "        CFLA: feed LA concentration\n",
    "        CALA: acid LA concentration\n",
    "        CFK: feed K concentration\n",
    "        CBK: base K concentration\n",
    "        VF: feed volume\n",
    "        VA: acid volume\n",
    "        VB: base volume\n",
    "        seq_lengths: actual sequence lengths\n",
    "\n",
    "    Returns:\n",
    "        input_tensor: [batch_size, seq_len, 3, 6] - formatted input for CNN-LSTM\n",
    "        initial_state: [batch_size, 3, 3] - initial concentrations and volumes\n",
    "        mask: [batch_size, seq_len] - padding mask\n",
    "        seq_lengths: [batch_size] - actual sequence lengths\n",
    "    '''\n",
    "\n",
    "    batch_size, seq_len = Vt.shape # Get batch size and sequence length for set the size of input tensor\n",
    "    input = torch.zeros(batch_size, seq_len, 9) # Generate input tensor\n",
    "\n",
    "    # Fill input tensor for each channel\n",
    "    input[:, :, 0] = Vt # Applied voltage\n",
    "    input[:, :, 1] = E # External electrolyte concentration\n",
    "    input[:, :, 2] = CFLA # Feed LA concentration\n",
    "    input[:, :, 3] = CALA # Acid LA concentration\n",
    "    input[:, :, 4] = CFK # Feed K concentration\n",
    "    input[:, :, 5] = CBK # Base K concentration\n",
    "    input[:, :, 6] = VF # Feed volume\n",
    "    input[:, :, 7] = VA # Acid volume\n",
    "    input[:, :, 8] = VB # Base volume\n",
    "\n",
    "    # initial state for each feature\n",
    "    init = torch.zeros(batch_size, 9)\n",
    "    init[:, 0] = Vt[:, 0] # Initial applied voltage\n",
    "    init[:, 1] = E[:, 0] # Initial external electrolyte concentration\n",
    "    init[:, 2] = CFLA[:, 0] # Initial feed LA concentration\n",
    "    init[:, 3] = CALA[:, 0] # Initial acid LA concentration\n",
    "    init[:, 4] = CFK[:, 0] # Initial feed K concentration\n",
    "    init[:, 5] = CBK[:, 0] # Initial base K concentration\n",
    "    init[:, 6] = VF[:, 0] # Initial feed volume\n",
    "    init[:, 7] = VA[:, 0] # Initial acid volume\n",
    "    init[:, 8] = VB[:, 0] # Initial base volume\n",
    "\n",
    "    # Create padding mask\n",
    "    mask = torch.zeros(batch_size, seq_len)\n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        mask[i, :length] = 1.0\n",
    "\n",
    "    return input, init, mask, seq_lengths\n",
    "\n",
    "input_tensor, init, mask, seq_lengths = prepare_input(Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB, seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataset by experiments\n",
    "class BMEDDataset(Dataset):\n",
    "    def __init__(self, inputs, init, masks, seq_lengths, I_exp, CFLA_exp, CALA_exp, CFK_exp, CBK_exp, VF_exp, VA_exp, VB_exp):\n",
    "        self.CV = inputs[:, :, :2] # Extract [Vt, E]\n",
    "        self.init = init[:, 2:] # Extract state variables\n",
    "        self.masks = masks\n",
    "        self.seq_lengths = seq_lengths\n",
    "\n",
    "        self.states = torch.stack([CFLA_exp, CALA_exp, CFK_exp, CBK_exp, VF_exp, VA_exp, VB_exp], dim=2)\n",
    "        self.current = I_exp.unsqueeze(2) # [batch, seq, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.CV)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'CV': self.CV[idx],\n",
    "            'init': self.init[idx],\n",
    "            'masks': self.masks[idx],\n",
    "            'seq_len': self.seq_lengths[idx],\n",
    "            'states': self.states[idx],\n",
    "            'current': self.current[idx]\n",
    "        }\n",
    "train_dataset = BMEDDataset(input_tensor, init, mask, seq_lengths, I, CFLA, CALA, CFK, CBK, VF, VA, VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BMEDModel(\n",
       "  (layer_norm): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "  (rnn_layers): LSTM(9, 64, num_layers=5, batch_first=True, dropout=0.2)\n",
       "  (flux_NN): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=44, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=44, out_features=24, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=24, out_features=4, bias=True)\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (current_NN): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=43, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=43, out_features=22, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=22, out_features=1, bias=True)\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Initialization\n",
    "class BMEDModel(nn.Module):\n",
    "    def __init__(self, hidden_nodes = 64, num_rnn_layers = 2, num_fnn_layers = 2,max_len = 37, dt = 0.25):\n",
    "        super(BMEDModel, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.input_features = 9 # [Vt, E, CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        self.control_features = 2 # [Vt, E]\n",
    "        self.state_features = 7 # [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        self.flux_features = 4 # [dLA, dK, dH2O_A, dH2O_B]\n",
    "        self.current_features = 1 # [I]\n",
    "        self.dt = dt # time step\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.ranges = {\n",
    "            'CFLA': {'min': -1, 'max': 4},\n",
    "            'CALA': {'min': -1, 'max': 4}, \n",
    "            'CFK': {'min': -1, 'max': 7},\n",
    "            'CBK': {'min': -1, 'max': 2},\n",
    "            'VF': {'min': 0, 'max': 2},\n",
    "            'VA': {'min': 0, 'max': 2},\n",
    "            'VB': {'min': 0, 'max': 8}\n",
    "        }\n",
    "\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(self.input_features)\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn_layers = nn.LSTM(\n",
    "            input_size = self.input_features,\n",
    "            hidden_size = hidden_nodes,\n",
    "            num_layers = num_rnn_layers,\n",
    "            batch_first = True,\n",
    "            dropout = 0.2 if num_rnn_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Flux Head\n",
    "        flux_layers = []\n",
    "        flux_sizes = [hidden_nodes]\n",
    "        flux_step = (hidden_nodes - self.flux_features) / (num_fnn_layers)\n",
    "        \n",
    "        for i in range(num_fnn_layers):\n",
    "            next_size = int(hidden_nodes - flux_step * (i + 1))\n",
    "            if i == num_fnn_layers - 1:\n",
    "                next_size = self.flux_features\n",
    "            \n",
    "            flux_layers.append(nn.Linear(flux_sizes[-1], next_size))\n",
    "            flux_layers.append(nn.ELU())\n",
    "            flux_layers.append(nn.Dropout(0.2))\n",
    "            flux_sizes.append(next_size)\n",
    "            \n",
    "        self.flux_NN = nn.Sequential(*flux_layers)\n",
    "        \n",
    "\n",
    "        # Current Head\n",
    "        current_layers = []\n",
    "        current_sizes = [hidden_nodes]\n",
    "        current_step = (hidden_nodes - self.current_features) / (num_fnn_layers)\n",
    "        \n",
    "        for i in range(num_fnn_layers):\n",
    "            next_size = int(hidden_nodes - current_step * (i + 1))\n",
    "            if i == num_fnn_layers - 1:\n",
    "                next_size = self.current_features\n",
    "            \n",
    "            current_layers.append(nn.Linear(current_sizes[-1], next_size))\n",
    "            current_layers.append(nn.ELU())\n",
    "            current_layers.append(nn.Dropout(0.2))\n",
    "            current_sizes.append(next_size)\n",
    "        \n",
    "        self.current_NN = nn.Sequential(*current_layers)\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initialize hidden states for RNN layers\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(self.num_rnn_layers, batch_size, self.hidden_nodes, device=device)\n",
    "        c0 = torch.zeros(self.num_rnn_layers, batch_size, self.hidden_nodes, device=device)\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def denormalize_state(self, norm_state):\n",
    "        '''\n",
    "        Normalized state to real values \n",
    "        '''\n",
    "        batch_size = norm_state.shape[0]\n",
    "        real_state = torch.zeros_like(norm_state)\n",
    "\n",
    "        state_names = ['CFLA', 'CALA', 'CFK', 'CBK', 'VF', 'VA', 'VB']\n",
    "\n",
    "        for i, name in enumerate(state_names):\n",
    "            min_val = self.ranges[name]['min']\n",
    "            max_val = self.ranges[name]['max']\n",
    "            real_state[:, i] = norm_state[:, i] * (max_val - min_val) + min_val\n",
    "        \n",
    "        return real_state\n",
    "    \n",
    "    def normalize_state(self, real_state):\n",
    "        '''\n",
    "        Real state to normalized values\n",
    "        '''\n",
    "        batch_size = real_state.shape[0]\n",
    "        norm_state = torch.zeros_like(real_state)\n",
    "\n",
    "        state_names = ['CFLA', 'CALA', 'CFK', 'CBK', 'VF', 'VA', 'VB']\n",
    "\n",
    "        for i, name in enumerate(state_names):\n",
    "            min_val = self.ranges[name]['min']\n",
    "            max_val = self.ranges[name]['max']\n",
    "            norm_state[:, i] = (real_state[:, i] - min_val) / (max_val - min_val)\n",
    "\n",
    "        return norm_state\n",
    "\n",
    "    def MB_step(self, norm_state, LA_flux, K_flux, VFA_flux, VFB_flux):\n",
    "        '''\n",
    "        Perform one time step of mass balance\n",
    "        state: [batch, 7] - [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        '''\n",
    "        real_state = self.denormalize_state(norm_state)\n",
    "\n",
    "        # extract current values\n",
    "        # channel 0: feed, channel 1: acid, channel 2: base\n",
    "        # property 0: LA_conc, property 1: K_conc, property 2: volume\n",
    "\n",
    "        CFLA = real_state[:, 0]\n",
    "        CALA = real_state[:, 1]\n",
    "        CFK = real_state[:, 2]\n",
    "        CBK = real_state[:, 3]\n",
    "        VF = real_state[:, 4]\n",
    "        VA = real_state[:, 5]\n",
    "        VB = real_state[:, 6]\n",
    "\n",
    "        # volume changes due to water flux\n",
    "        # Assuming positive flux means water moves from feed to acid or base\n",
    "        nVF = VF - (VFA_flux + VFB_flux) * self.dt # Feed Volume\n",
    "        nVA = VA + VFA_flux * self.dt # Acid Volume\n",
    "        nVB = VB + VFB_flux * self.dt # Base Volume\n",
    "\n",
    "        # LA mass balance\n",
    "        nNFLA = CFLA*VF - LA_flux*self.dt\n",
    "        nNALA = CALA*VA + LA_flux*self.dt\n",
    "        nNFK = CFK*VF - K_flux*self.dt\n",
    "        nNBK = CBK*VB + K_flux*self.dt\n",
    "\n",
    "        # update states\n",
    "        new_real_state = torch.zeros_like(real_state)\n",
    "        new_real_state[:, 0] = nNFLA / (nVF + 1e-8) # new Feed LA concentration\n",
    "        new_real_state[:, 1] = nNALA / (nVA + 1e-8) # new Acid LA concentration\n",
    "        new_real_state[:, 2] = nNFK / (nVF + 1e-8) # new Feed K concentration\n",
    "        new_real_state[:, 3] = nNBK / (nVB + 1e-8) # new Base K concentration\n",
    "        new_real_state[:, 4] = nVF # new Feed Volume\n",
    "        new_real_state[:, 5] = nVA # new Acid Volume\n",
    "        new_real_state[:, 6] = nVB # new Base Volume\n",
    "\n",
    "        new_norm_state = self.normalize_state(new_real_state)\n",
    "    \n",
    "        return new_norm_state\n",
    "    \n",
    "    def forward_single_step(self, CV, prev_state, hidden_state):\n",
    "        '''\n",
    "        Predict Single Step of BMED\n",
    "        CV: [batch_size, 2] - [Vt, E]\n",
    "        prev_state: [batch_size, 7] - [CFLA, CALA, CFK, CBK, VF, VA, VB]\n",
    "        hidden_state: [h, c] - hidden state of RNN layers\n",
    "        '''\n",
    "\n",
    "        full_input = torch.cat([CV, prev_state], dim=1)\n",
    "        full_input = full_input.unsqueeze(1) # [batch_size, 1, 9]\n",
    "\n",
    "        # Layer Normalization\n",
    "        rnn_input = self.layer_norm(full_input)\n",
    "        \n",
    "        # RNN forward - pack all information of previous hidden state\n",
    "        rnn_out, new_hidden_state = self.rnn_layers(rnn_input, hidden_state)\n",
    "\n",
    "        # Predict flux and current\n",
    "        flux = self.flux_NN(rnn_out.squeeze(1)) # [batch_size, 4]\n",
    "        current = self.current_NN(rnn_out.squeeze(1)) # [batch_size, 1]\n",
    "\n",
    "        # Physical update\n",
    "        LA_flux = flux[:, 0]\n",
    "        K_flux = flux[:, 1]\n",
    "        VFA_flux = flux[:, 2]\n",
    "        VFB_flux = flux[:, 3]\n",
    "\n",
    "        new_state = self.MB_step(prev_state, LA_flux, K_flux, VFA_flux, VFB_flux)\n",
    "\n",
    "        return current, new_state, new_hidden_state\n",
    "    \n",
    "    def next_seq_pred_train(self, CV, init, states, current, masks):\n",
    "        '''\n",
    "        Autoregressive training\n",
    "        '''\n",
    "        batch_size, max_seq_len, _ = CV.shape\n",
    "        device= CV.device\n",
    "\n",
    "        mask = masks # [batch_size, max_seq_len]\n",
    "\n",
    "        CVf = CV[:, 0, :] # [batch_size, 2]\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden_state = self.init_hidden(batch_size,device)\n",
    "        current_state = init.clone() # [batch_size, 7]\n",
    "\n",
    "        # predicted results\n",
    "        predicted_current = []\n",
    "        predicted_states = []\n",
    "\n",
    "        # use init_state at t=0\n",
    "        predicted_states.append(init)\n",
    "\n",
    "        # use 0 at t=0\n",
    "        first_current = torch.zeros(batch_size, 1, device=device)\n",
    "        predicted_current.append(first_current)\n",
    "\n",
    "        # Calculate for each timestep with the entire batch simultaneously\n",
    "        for t in range(1,max_seq_len):\n",
    "\n",
    "            # Predict current and states of the entire batch\n",
    "            pred_current, pred_state, hidden_state = self.forward_single_step(CVf, current_state, hidden_state)\n",
    "\n",
    "            predicted_current.append(pred_current)\n",
    "            predicted_states.append(pred_state)\n",
    "\n",
    "            # update current state with predicted states\n",
    "            current_state = pred_state\n",
    "\n",
    "        # Convert to tensor\n",
    "        predicted_current = torch.stack(predicted_current, dim=1) # [batch_size, max_seq_len, 1]\n",
    "        predicted_states = torch.stack(predicted_states, dim=1) # [batch_size, max_seq_len, 7]\n",
    "\n",
    "        # Loss calculation: \n",
    "        states_exp = states[:, 1:, :] # [batch_size, seq_len-1, 7]\n",
    "        current_exp = current[:, 1:, :] # [batch_size, seq_len-1, 1]\n",
    "        states_pred = predicted_states[:, 1:, :] # [batch_size, seq_len-1, 7]\n",
    "        current_pred = predicted_current[:, 1:, :] # [batch_size, seq_len-1, 1]\n",
    "        mask_adj = mask[:, 1:] # [batch_size, seq_len-1]\n",
    "\n",
    "        # selected loss features\n",
    "        selected_indices = [0, 2, 5, 6]\n",
    "        states_exp_loss = states_exp[:, :, selected_indices]\n",
    "        states_pred_loss = states_pred[:, :, selected_indices]\n",
    "        \n",
    "        # Calculate loss\n",
    "        states_loss = F.mse_loss(states_pred_loss, states_exp_loss, reduction='none')\n",
    "        #current_loss = F.mse_loss(current_pred, current_exp, reduction='none')\n",
    "\n",
    "        # Apply mask and calculate mean loss\n",
    "        mask_states = mask_adj.unsqueeze(-1).expand(-1, -1, len(selected_indices))\n",
    "        #mask_current = mask_adj.unsqueeze(-1)\n",
    "\n",
    "        masked_states_loss = (states_loss * mask_states).sum() / mask_states.sum()\n",
    "        #masked_current_loss = (current_loss * mask_current).sum() / mask_current.sum()\n",
    "\n",
    "        # total_loss = masked_current_loss + masked_states_loss*7\n",
    "        total_loss = masked_states_loss\n",
    "\n",
    "        return total_loss, predicted_current, predicted_states\n",
    "    \n",
    "    def generate_sequence(self, CV, init, max_steps):\n",
    "        '''\n",
    "        Prediction mode from initial state\n",
    "\n",
    "        Args:\n",
    "            CV: [2] or [batch_size, 2] - [Vt, E]\n",
    "            init_state: initial state ([7] or [batch_size, 7])\n",
    "            max_steps: maximum number of steps to generate\n",
    "        '''\n",
    "\n",
    "        if len(init.shape) == 1:\n",
    "            # if input is single exp, add a batch dimension\n",
    "            init = init.unsqueeze(0)\n",
    "            batch_size = 1\n",
    "            single_exp = True\n",
    "        else:\n",
    "            batch_size = init.shape[0]\n",
    "            single_exp = False\n",
    "        \n",
    "        device = init.device\n",
    "\n",
    "        # check CV batch size and modification\n",
    "        if len(CV.shape) == 1:\n",
    "            CV = CV.unsqueeze(0) # [1, 2]\n",
    "        \n",
    "        CVf = CV[:, 0, :].to(device)\n",
    "        \n",
    "\n",
    "        # initialize hidden state\n",
    "        hidden_state = self.init_hidden(batch_size, device)\n",
    "        current_state = init.clone() # [batch_size, 7]\n",
    "\n",
    "        predicted_current = []\n",
    "        predicted_states = []\n",
    "        \n",
    "        predicted_states.append(init)\n",
    "        first_current = torch.zeros(batch_size, 1, device=device)  # 첫 번째 전류는 0으로 설정\n",
    "        predicted_current.append(first_current)\n",
    "\n",
    "        for t in range(1, max_steps):\n",
    "            pred_current, pred_state, hidden_state = self.forward_single_step(CVf, current_state, hidden_state)\n",
    "\n",
    "            predicted_current.append(pred_current) # [batch_size, 1]\n",
    "            predicted_states.append(pred_state) # [batch_size, 7]\n",
    "\n",
    "            current_state = pred_state\n",
    "\n",
    "        predicted_current = torch.stack(predicted_current, dim=1) # [batch_size, max_steps, 1]\n",
    "        predicted_states = torch.stack(predicted_states, dim=1) # [batch_size, max_steps, 7]\n",
    "\n",
    "        if single_exp:\n",
    "            predicted_current = predicted_current.squeeze(0) # [max_steps, 1]\n",
    "            predicted_states = predicted_states.squeeze(0) # [max_steps, 7]\n",
    "        \n",
    "        return predicted_current, predicted_states\n",
    "\n",
    "    def forward(self, CV, init, states, current, masks, mode='train', max_steps=None):\n",
    "        '''\n",
    "        Integrated forward method\n",
    "        '''\n",
    "\n",
    "        if mode == 'train':\n",
    "            if states is None or current is None or masks is None:\n",
    "                raise ValueError(\"states, current, and masks are required for training\")\n",
    "            return self.next_seq_pred_train(CV, init, states, current, masks)\n",
    "        elif mode == 'inference':\n",
    "            if max_steps is None:\n",
    "                raise ValueError(\"max_steps is required for inference\")\n",
    "            fixed_control = CV[:, 0, :] # [batch_size, 2]\n",
    "            return self.generate_sequence(CV, init, max_steps)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Use 'train' or 'inference'\")\n",
    "dmodel = 64\n",
    "model = BMEDModel(\n",
    "    hidden_nodes=dmodel,\n",
    "    num_rnn_layers=5,\n",
    "    num_fnn_layers=3,\n",
    "    max_len=max_length,\n",
    "    dt=0.25\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noam Scheduler\n",
    "class NoamLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    '''\n",
    "    Pytorch LRScheduler 스타일으 Noam Scheduler\n",
    "    '''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=40, last_epoch=-1):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(NoamLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        if step == 0:\n",
    "            step  = 1\n",
    "\n",
    "        lr_scale = (self.d_model ** -0.5) *min(\n",
    "            step ** -0.5,\n",
    "            step * (self.warmup_steps ** -1.5)\n",
    "        )\n",
    "        return [base_lr * lr_scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "num_epochs = 1000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1)\n",
    "scheduler = NoamLR(optimizer, d_model=dmodel, warmup_steps=0.1*num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "==================================================\n",
      "Epoch 1/1000, Loss: 0.009125, LR: 0.000250, best_loss: 0.009125\n",
      "Epoch 11/1000, Loss: 0.004597, LR: 0.001500, best_loss: 0.004597\n",
      "Epoch 21/1000, Loss: 0.003212, LR: 0.002750, best_loss: 0.001325\n",
      "Epoch 31/1000, Loss: 0.001944, LR: 0.004000, best_loss: 0.001325\n",
      "Epoch 41/1000, Loss: 0.001321, LR: 0.005250, best_loss: 0.001157\n",
      "Epoch 51/1000, Loss: 0.001089, LR: 0.006500, best_loss: 0.001089\n",
      "Epoch 61/1000, Loss: 0.000769, LR: 0.007750, best_loss: 0.000741\n",
      "Epoch 71/1000, Loss: 0.000717, LR: 0.009000, best_loss: 0.000531\n",
      "Epoch 81/1000, Loss: 0.001272, LR: 0.010250, best_loss: 0.000531\n",
      "Epoch 91/1000, Loss: 0.001237, LR: 0.011500, best_loss: 0.000531\n",
      "Epoch 101/1000, Loss: 0.000586, LR: 0.012377, best_loss: 0.000531\n",
      "Epoch 111/1000, Loss: 0.000636, LR: 0.011811, best_loss: 0.000500\n",
      "Epoch 121/1000, Loss: 0.000488, LR: 0.011317, best_loss: 0.000471\n",
      "Epoch 131/1000, Loss: 0.000402, LR: 0.010880, best_loss: 0.000396\n",
      "Epoch 141/1000, Loss: 0.000448, LR: 0.010490, best_loss: 0.000301\n",
      "Epoch 151/1000, Loss: 0.000486, LR: 0.010139, best_loss: 0.000301\n",
      "Epoch 161/1000, Loss: 0.000519, LR: 0.009821, best_loss: 0.000301\n",
      "Epoch 171/1000, Loss: 0.000340, LR: 0.009531, best_loss: 0.000301\n",
      "Epoch 181/1000, Loss: 0.000366, LR: 0.009266, best_loss: 0.000301\n",
      "Epoch 191/1000, Loss: 0.000351, LR: 0.009021, best_loss: 0.000268\n",
      "Epoch 201/1000, Loss: 0.000323, LR: 0.008795, best_loss: 0.000268\n",
      "Epoch 211/1000, Loss: 0.000408, LR: 0.008585, best_loss: 0.000250\n",
      "Epoch 221/1000, Loss: 0.000438, LR: 0.008389, best_loss: 0.000250\n",
      "Epoch 231/1000, Loss: 0.000234, LR: 0.008207, best_loss: 0.000234\n",
      "Epoch 241/1000, Loss: 0.000447, LR: 0.008035, best_loss: 0.000234\n",
      "Epoch 251/1000, Loss: 0.000353, LR: 0.007874, best_loss: 0.000218\n",
      "Epoch 261/1000, Loss: 0.000328, LR: 0.007723, best_loss: 0.000218\n",
      "Epoch 271/1000, Loss: 0.000367, LR: 0.007579, best_loss: 0.000218\n",
      "Epoch 281/1000, Loss: 0.000303, LR: 0.007444, best_loss: 0.000218\n",
      "Epoch 291/1000, Loss: 0.000291, LR: 0.007315, best_loss: 0.000218\n",
      "Epoch 301/1000, Loss: 0.000218, LR: 0.007193, best_loss: 0.000218\n",
      "Epoch 311/1000, Loss: 0.000360, LR: 0.007077, best_loss: 0.000218\n",
      "Epoch 321/1000, Loss: 0.000271, LR: 0.006966, best_loss: 0.000211\n",
      "Epoch 331/1000, Loss: 0.000276, LR: 0.006860, best_loss: 0.000211\n",
      "Epoch 341/1000, Loss: 0.000240, LR: 0.006759, best_loss: 0.000211\n",
      "Epoch 351/1000, Loss: 0.000303, LR: 0.006663, best_loss: 0.000211\n",
      "Epoch 361/1000, Loss: 0.000249, LR: 0.006570, best_loss: 0.000211\n",
      "Epoch 371/1000, Loss: 0.000400, LR: 0.006481, best_loss: 0.000211\n",
      "Epoch 381/1000, Loss: 0.000358, LR: 0.006396, best_loss: 0.000211\n",
      "Epoch 391/1000, Loss: 0.000309, LR: 0.006313, best_loss: 0.000211\n",
      "Epoch 401/1000, Loss: 0.000292, LR: 0.006234, best_loss: 0.000202\n",
      "Epoch 411/1000, Loss: 0.000279, LR: 0.006158, best_loss: 0.000202\n",
      "Epoch 421/1000, Loss: 0.000239, LR: 0.006085, best_loss: 0.000179\n",
      "Epoch 431/1000, Loss: 0.000336, LR: 0.006014, best_loss: 0.000179\n",
      "Epoch 441/1000, Loss: 0.000272, LR: 0.005946, best_loss: 0.000179\n",
      "Epoch 451/1000, Loss: 0.000268, LR: 0.005880, best_loss: 0.000175\n",
      "Epoch 461/1000, Loss: 0.000258, LR: 0.005816, best_loss: 0.000175\n",
      "Epoch 471/1000, Loss: 0.000361, LR: 0.005754, best_loss: 0.000175\n",
      "Epoch 481/1000, Loss: 0.000296, LR: 0.005694, best_loss: 0.000175\n",
      "Epoch 491/1000, Loss: 0.000299, LR: 0.005635, best_loss: 0.000175\n",
      "Epoch 501/1000, Loss: 0.000301, LR: 0.005579, best_loss: 0.000175\n",
      "Epoch 511/1000, Loss: 0.000262, LR: 0.005524, best_loss: 0.000175\n",
      "Epoch 521/1000, Loss: 0.000390, LR: 0.005471, best_loss: 0.000175\n",
      "Epoch 531/1000, Loss: 0.000254, LR: 0.005419, best_loss: 0.000175\n",
      "Epoch 541/1000, Loss: 0.000247, LR: 0.005369, best_loss: 0.000175\n",
      "Epoch 551/1000, Loss: 0.000236, LR: 0.005320, best_loss: 0.000175\n",
      "Epoch 561/1000, Loss: 0.000204, LR: 0.005273, best_loss: 0.000175\n",
      "Epoch 571/1000, Loss: 0.000207, LR: 0.005227, best_loss: 0.000175\n",
      "Epoch 581/1000, Loss: 0.000257, LR: 0.005181, best_loss: 0.000175\n",
      "Epoch 591/1000, Loss: 0.000235, LR: 0.005137, best_loss: 0.000175\n",
      "Epoch 601/1000, Loss: 0.000218, LR: 0.005095, best_loss: 0.000175\n",
      "Epoch 611/1000, Loss: 0.000268, LR: 0.005053, best_loss: 0.000175\n",
      "Epoch 621/1000, Loss: 0.000256, LR: 0.005012, best_loss: 0.000175\n",
      "Epoch 631/1000, Loss: 0.000258, LR: 0.004972, best_loss: 0.000175\n",
      "Epoch 641/1000, Loss: 0.000290, LR: 0.004933, best_loss: 0.000175\n",
      "Epoch 651/1000, Loss: 0.000260, LR: 0.004895, best_loss: 0.000175\n",
      "Epoch 661/1000, Loss: 0.000309, LR: 0.004858, best_loss: 0.000175\n",
      "Epoch 671/1000, Loss: 0.000302, LR: 0.004822, best_loss: 0.000175\n",
      "Epoch 681/1000, Loss: 0.000221, LR: 0.004786, best_loss: 0.000175\n",
      "Epoch 691/1000, Loss: 0.000208, LR: 0.004752, best_loss: 0.000175\n",
      "Epoch 701/1000, Loss: 0.000340, LR: 0.004718, best_loss: 0.000175\n",
      "Epoch 711/1000, Loss: 0.000297, LR: 0.004685, best_loss: 0.000175\n",
      "Epoch 721/1000, Loss: 0.000254, LR: 0.004652, best_loss: 0.000175\n",
      "Epoch 731/1000, Loss: 0.000350, LR: 0.004620, best_loss: 0.000175\n",
      "Epoch 741/1000, Loss: 0.000215, LR: 0.004589, best_loss: 0.000175\n",
      "Epoch 751/1000, Loss: 0.000232, LR: 0.004558, best_loss: 0.000175\n",
      "Epoch 761/1000, Loss: 0.000251, LR: 0.004528, best_loss: 0.000175\n",
      "Epoch 771/1000, Loss: 0.000201, LR: 0.004499, best_loss: 0.000175\n",
      "Epoch 781/1000, Loss: 0.000279, LR: 0.004470, best_loss: 0.000175\n",
      "Epoch 791/1000, Loss: 0.000314, LR: 0.004442, best_loss: 0.000175\n",
      "Epoch 801/1000, Loss: 0.000278, LR: 0.004414, best_loss: 0.000175\n",
      "Epoch 811/1000, Loss: 0.000239, LR: 0.004387, best_loss: 0.000175\n",
      "Epoch 821/1000, Loss: 0.000267, LR: 0.004360, best_loss: 0.000175\n",
      "Epoch 831/1000, Loss: 0.000309, LR: 0.004334, best_loss: 0.000175\n",
      "Epoch 841/1000, Loss: 0.000171, LR: 0.004308, best_loss: 0.000171\n",
      "Epoch 851/1000, Loss: 0.000297, LR: 0.004282, best_loss: 0.000171\n",
      "Epoch 861/1000, Loss: 0.000237, LR: 0.004258, best_loss: 0.000171\n",
      "Epoch 871/1000, Loss: 0.000294, LR: 0.004233, best_loss: 0.000171\n",
      "Epoch 881/1000, Loss: 0.000239, LR: 0.004209, best_loss: 0.000171\n",
      "Epoch 891/1000, Loss: 0.000247, LR: 0.004185, best_loss: 0.000171\n",
      "Epoch 901/1000, Loss: 0.000274, LR: 0.004162, best_loss: 0.000171\n",
      "Epoch 911/1000, Loss: 0.000313, LR: 0.004139, best_loss: 0.000171\n",
      "Epoch 921/1000, Loss: 0.000208, LR: 0.004117, best_loss: 0.000171\n",
      "Epoch 931/1000, Loss: 0.000223, LR: 0.004095, best_loss: 0.000171\n",
      "Epoch 941/1000, Loss: 0.000227, LR: 0.004073, best_loss: 0.000171\n",
      "Epoch 951/1000, Loss: 0.000212, LR: 0.004051, best_loss: 0.000171\n",
      "Epoch 961/1000, Loss: 0.000242, LR: 0.004030, best_loss: 0.000171\n",
      "Epoch 971/1000, Loss: 0.000240, LR: 0.004009, best_loss: 0.000171\n",
      "Epoch 981/1000, Loss: 0.000223, LR: 0.003989, best_loss: 0.000171\n",
      "Epoch 991/1000, Loss: 0.000221, LR: 0.003969, best_loss: 0.000171\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, device):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    best_loss = np.inf\n",
    "    best_states = None\n",
    "    best_current = None\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # move to device\n",
    "        CV = batch['CV'].to(device)\n",
    "        init = batch['init'].to(device)\n",
    "        states = batch['states'].to(device)\n",
    "        current = batch['current'].to(device)\n",
    "        masks = batch['masks'].to(device)\n",
    "\n",
    "        # gradient initialization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss, pred_current, pred_states = model(CV, init, states, current, masks, mode='train')\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 1)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # update scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    return total_loss / num_batches, pred_current, pred_states\n",
    "\n",
    "print(\"Start training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_loss = np.inf\n",
    "best_current = None\n",
    "best_states = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, pred_current, pred_states = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        best_current = pred_current\n",
    "        best_states = pred_states\n",
    "\n",
    "    if epoch % 10 == 0: \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.6f}, LR: {current_lr:.6f}, best_loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 준비\n",
    "test_idx = 3\n",
    "test_states = train_dataset.states[test_idx:test_idx+1]\n",
    "test_seq_len = train_dataset.seq_lengths[test_idx:test_idx+1]\n",
    "\n",
    "# 데이터 유효성 검사\n",
    "if len(test_seq_len) == 0:\n",
    "    raise ValueError(\"테스트 시퀀스 길이가 비어있습니다.\")\n",
    "\n",
    "# 초기 상태 설정\n",
    "init = test_states[:, 0:1, :].to(device)\n",
    "states = test_states[:, 1:, :].to(device)\n",
    "\n",
    "# CV와 masks 텐서 생성 전에 시퀀스 길이 확인\n",
    "seq_length = test_seq_len[0]\n",
    "if seq_length <= 1:\n",
    "    raise ValueError(f\"시퀀스 길이가 너무 짧습니다: {seq_length}\")\n",
    "\n",
    "CV = torch.zeros((1, seq_length-1, 9)).to(device)\n",
    "masks = torch.ones((1, seq_length-1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 예측 수행\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     _, pred_current, pred_states = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 예측 결과를 numpy 배열로 변환\u001b[39;00m\n\u001b[32m     32\u001b[39m pred_states = pred_states.cpu().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\micromamba\\envs\\torchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\micromamba\\envs\\torchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 323\u001b[39m, in \u001b[36mBMEDModel.forward\u001b[39m\u001b[34m(self, CV, init, states, current, masks, mode, max_steps)\u001b[39m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m current \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    322\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mstates, current, and masks are required for training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext_seq_pred_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33minference\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m max_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 219\u001b[39m, in \u001b[36mBMEDModel.next_seq_pred_train\u001b[39m\u001b[34m(self, CV, init, states, current, masks)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Calculate for each timestep with the entire batch simultaneously\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m,max_seq_len):\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# Predict current and states of the entire batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     pred_current, pred_state, hidden_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_single_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCVf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     predicted_current.append(pred_current)\n\u001b[32m    222\u001b[39m     predicted_states.append(pred_state)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mBMEDModel.forward_single_step\u001b[39m\u001b[34m(self, CV, prev_state, hidden_state)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_single_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, CV, prev_state, hidden_state):\n\u001b[32m    159\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m    Predict Single Step of BMED\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03m    CV: [batch_size, 2] - [Vt, E]\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[33;03m    prev_state: [batch_size, 7] - [CFLA, CALA, CFK, CBK, VF, VA, VB]\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    hidden_state: [h, c] - hidden state of RNN layers\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     full_input = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m     full_input = full_input.unsqueeze(\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# [batch_size, 1, 9]\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Layer Normalization\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 준비\n",
    "test_idx = 3\n",
    "test_states = train_dataset.states[test_idx:test_idx+1]\n",
    "test_seq_len = train_dataset.seq_lengths[test_idx:test_idx+1]\n",
    "\n",
    "# 데이터 유효성 검사\n",
    "if len(test_seq_len) == 0:\n",
    "    raise ValueError(\"테스트 시퀀스 길이가 비어있습니다.\")\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 초기 상태 설정\n",
    "init = test_states[:, 0:1, :].to(device)\n",
    "states = test_states[:, 1:, :].to(device)\n",
    "\n",
    "# CV와 masks 텐서 생성 전에 시퀀스 길이 확인\n",
    "seq_length = test_seq_len[0]\n",
    "if seq_length <= 1:\n",
    "    raise ValueError(f\"시퀀스 길이가 너무 짧습니다: {seq_length}\")\n",
    "\n",
    "CV = torch.zeros((1, seq_length-1, 9)).to(device)\n",
    "masks = torch.ones((1, seq_length-1)).to(device)\n",
    "\n",
    "# 예측 수행\n",
    "with torch.no_grad():\n",
    "    _, pred_current, pred_states = model(CV, init, states, current, masks, mode='train')\n",
    "\n",
    "# 예측 결과를 numpy 배열로 변환\n",
    "pred_states = pred_states.cpu().numpy()\n",
    "true_states = states.cpu().numpy()\n",
    "\n",
    "# 시간 축 생성\n",
    "time = np.arange(0, seq_length-1) * 0.25\n",
    "\n",
    "# 플롯 생성\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# 첫 번째 행: CF_LA, CF_K\n",
    "axs[0].plot(time, true_states[0, :, 0], 'b-', label='CF_LA (True)')\n",
    "axs[0].plot(time, pred_states[0, :, 0], 'b--', label='CF_LA (Pred)')\n",
    "axs[0].plot(time, true_states[0, :, 2], 'r-', label='CF_K (True)')\n",
    "axs[0].plot(time, pred_states[0, :, 2], 'r--', label='CF_K (Pred)')\n",
    "axs[0].set_ylabel('Concentration')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# 두 번째 행: VF, VA, VB\n",
    "axs[1].plot(time, true_states[0, :, 4], 'g-', label='VF (True)')\n",
    "axs[1].plot(time, pred_states[0, :, 4], 'g--', label='VF (Pred)')\n",
    "axs[1].plot(time, true_states[0, :, 5], 'm-', label='VA (True)')\n",
    "axs[1].plot(time, pred_states[0, :, 5], 'm--', label='VA (Pred)')\n",
    "axs[1].plot(time, true_states[0, :, 6], 'c-', label='VB (True)')\n",
    "axs[1].plot(time, pred_states[0, :, 6], 'c--', label='VB (Pred)')\n",
    "axs[1].set_xlabel('Time (s)')\n",
    "axs[1].set_ylabel('Volume')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
