{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(data_list, max_length=None, pad_value=-100.0):\n",
    "    \"\"\"\n",
    "    Pad variable length sequences to the same length\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of tensors with different sequence lengths\n",
    "        max_length: Maximum length to pad to (default: longest sequence)\n",
    "        pad_value: Value to use for padding\n",
    "    \n",
    "    Returns:\n",
    "        padded_tensor: [batch_size, max_length, ...] - padded sequences\n",
    "        seq_lengths: [batch_size] - original sequence lengths\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = max(data.shape[0] for data in data_list) # Auto-calculate the max length\n",
    "\n",
    "    batch_size = len(data_list) # Batch size\n",
    "    seq_lengths = torch.tensor([data.shape[0] for data in data_list]) # Actual sequential length for each experiments\n",
    "    dimensions = data_list[0].shape[1:] # Get shape of individual elements\n",
    "    padded_tensor = torch.full((batch_size, max_length) + dimensions, pad_value, dtype=torch.float32) # generaste padded tensor filled with pad_value\n",
    "\n",
    "\n",
    "    # Fill with actual data\n",
    "    for i, data in enumerate(data_list):\n",
    "        padded_tensor[i, :data.shape[0]] = torch.tensor(data[:data.shape[0]], dtype=torch.float32)\n",
    "    \n",
    "    return padded_tensor, seq_lengths, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(Vt, E, C, Vm, seq_lengths=None):\n",
    "    \"\"\"\n",
    "    Prepare input data for the model with padding support\n",
    "    \n",
    "    Args:\n",
    "        voltage: [batch_size, seq_len] - applied voltage\n",
    "        ext_electrolyte: [batch_size, seq_len] - external electrolyte concentration\n",
    "        concentrations: [batch_size, seq_len, 3, 2] - [Feed, Acid, Base] x [LA, K] concentrations\n",
    "        volumes: [batch_size, seq_len, 3] - volumes for each channel\n",
    "        currents: [batch_size, seq_len] - measured currents\n",
    "        seq_lengths: [batch_size] - actual sequence lengths (optional)\n",
    "    \n",
    "    Returns:\n",
    "        input_tensor: [batch_size, seq_len, 3, 6] - formatted input for CNN-LSTM\n",
    "        initial_state: [batch_size, 3, 3] - initial concentrations and volumes\n",
    "        mask: [batch_size, seq_len] - padding mask\n",
    "        seq_lengths: [batch_size] - actual sequence lengths\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = Vt.shape # Get batch size and sequence length for set the size of input tensor\n",
    "    input = torch.zeros(batch_size, seq_len, 3, 5) # Generate input tensor\n",
    "\n",
    "    # Fill input tensor for each channel\n",
    "    for channel in range(3):\n",
    "        input[:, :, channel, 0] = C[:, :, channel, 0] # LA concentration\n",
    "        input[:, :, channel, 1] = C[:, :, channel, 1] # K concentration\n",
    "        input[:, :, channel, 2] = Vm[:, :, channel] # Volume\n",
    "        input[:, :, channel, 3] = Vt # Voltage (same for all)\n",
    "        input[:, :, channel, 4] = E # Ext electrolyte (same for all)\n",
    "\n",
    "    # Initial State for each channel\n",
    "    init = torch.zeros(batch_size,3,3)\n",
    "    init[:, :, 0] = C[:, 0, :, 0] # Initial LA concentrations [batch number, sequence length = 0 = initial state, channel number, feature]\n",
    "    init[:, :, 1] = C[:, 0, :, 1] # Initial K concentrations\n",
    "    init[:, :, 2] = Vm[:, 0, :] # Initial volumes\n",
    "\n",
    "    # Create padding mask\n",
    "    mask = torch.zeros(batch_size, seq_len)\n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        mask[i, :length] = 1.0\n",
    "    \n",
    "    return input, init, mask, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>t</th>\n",
       "      <th>V</th>\n",
       "      <th>E</th>\n",
       "      <th>VF</th>\n",
       "      <th>VA</th>\n",
       "      <th>VB</th>\n",
       "      <th>CF_LA</th>\n",
       "      <th>CA_LA</th>\n",
       "      <th>CB_LA</th>\n",
       "      <th>CF_K</th>\n",
       "      <th>CA_K</th>\n",
       "      <th>CB_K</th>\n",
       "      <th>I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500849</td>\n",
       "      <td>0.499447</td>\n",
       "      <td>0.124897</td>\n",
       "      <td>0.299333</td>\n",
       "      <td>0.200407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325250</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.503098</td>\n",
       "      <td>0.498310</td>\n",
       "      <td>0.124621</td>\n",
       "      <td>0.298671</td>\n",
       "      <td>0.200615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318181</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.506298</td>\n",
       "      <td>0.496768</td>\n",
       "      <td>0.124222</td>\n",
       "      <td>0.298012</td>\n",
       "      <td>0.200692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.312807</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.123750</td>\n",
       "      <td>0.297357</td>\n",
       "      <td>0.200703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309810</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>29</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.141450</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.284139</td>\n",
       "      <td>0.201839</td>\n",
       "      <td>0.615347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756200</td>\n",
       "      <td>0.680995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>29</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.131718</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.286411</td>\n",
       "      <td>0.200989</td>\n",
       "      <td>0.615517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756093</td>\n",
       "      <td>0.587422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>29</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.124207</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.288314</td>\n",
       "      <td>0.200399</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.755581</td>\n",
       "      <td>0.463967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>29</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.119632</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.289580</td>\n",
       "      <td>0.200072</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.754345</td>\n",
       "      <td>0.317689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>29</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.118707</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.289941</td>\n",
       "      <td>0.200010</td>\n",
       "      <td>0.615712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.752065</td>\n",
       "      <td>0.155646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>776 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     exp     t    V     E        VF        VA        VB     CF_LA     CA_LA  \\\n",
       "0      0  0.00  0.4  0.25  0.500000  0.500000  0.125000  0.300000  0.200000   \n",
       "1      0  0.25  0.4  0.25  0.500849  0.499447  0.124897  0.299333  0.200407   \n",
       "2      0  0.50  0.4  0.25  0.503098  0.498310  0.124621  0.298671  0.200615   \n",
       "3      0  0.75  0.4  0.25  0.506298  0.496768  0.124222  0.298012  0.200692   \n",
       "4      0  1.00  0.4  0.25  0.510000  0.495000  0.123750  0.297357  0.200703   \n",
       "..   ...   ...  ...   ...       ...       ...       ...       ...       ...   \n",
       "771   29  6.00  0.7  0.50  0.141450  0.721649  0.284139  0.201839  0.615347   \n",
       "772   29  6.25  0.7  0.50  0.131718  0.721649  0.286411  0.200989  0.615517   \n",
       "773   29  6.50  0.7  0.50  0.124207  0.721649  0.288314  0.200399  0.615635   \n",
       "774   29  6.75  0.7  0.50  0.119632  0.721649  0.289580  0.200072  0.615700   \n",
       "775   29  7.00  0.7  0.50  0.118707  0.721649  0.289941  0.200010  0.615712   \n",
       "\n",
       "     CB_LA      CF_K  CA_K      CB_K         I  \n",
       "0      0.0  0.250000   0.0  0.333333  0.000000  \n",
       "1      0.0  0.252721   0.0  0.325250  0.005000  \n",
       "2      0.0  0.254626   0.0  0.318181  0.010000  \n",
       "3      0.0  0.255745   0.0  0.312807  0.015000  \n",
       "4      0.0  0.256111   0.0  0.309810  0.020000  \n",
       "..     ...       ...   ...       ...       ...  \n",
       "771    0.0  0.176973   0.0  0.756200  0.680995  \n",
       "772    0.0  0.173009   0.0  0.756093  0.587422  \n",
       "773    0.0  0.169989   0.0  0.755581  0.463967  \n",
       "774    0.0  0.168168   0.0  0.754345  0.317689  \n",
       "775    0.0  0.167802   0.0  0.752065  0.155646  \n",
       "\n",
       "[776 rows x 14 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"BMED_DB_augmented.csv\")\n",
    "df['CA_K'] = 0.0\n",
    "df['CB_LA'] = 0.0\n",
    "\n",
    "# Robust min-max scaling, 안전 마진을 포함하는 min-max 정규화\n",
    "ranges ={\n",
    "    'V' : {'min':0, 'max':50},\n",
    "    'E' : {'min':0, 'max':1},\n",
    "    'VF' : {'min':0, 'max':2},\n",
    "    'VA' : {'min':0, 'max':2},\n",
    "    'VB' : {'min':0, 'max':8},\n",
    "    'CF_LA' : {'min':-1, 'max':4},\n",
    "    'CA_LA' : {'min':-1, 'max':4},\n",
    "    'CF_K' : {'min':-1, 'max':7},\n",
    "    'CB_K' : {'min':-1, 'max':2},\n",
    "    'I' : {'min':0, 'max':5},\n",
    "}\n",
    "\n",
    "ndf = pd.DataFrame()\n",
    "ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "ndf['V'] = (df['V'] - ranges['V']['min'])/(ranges['V']['max'] - ranges['V']['min'])\n",
    "ndf['E'] = (df['E'] - ranges['E']['min'])/(ranges['E']['max'] - ranges['E']['min'])\n",
    "ndf['VF'] = (df['VF'] - ranges['VF']['min'])/(ranges['VF']['max'] - ranges['VF']['min'])\n",
    "ndf['VA'] = (df['VA'] - ranges['VA']['min'])/(ranges['VA']['max'] - ranges['VA']['min'])\n",
    "ndf['VB'] = (df['VB'] - ranges['VB']['min'])/(ranges['VB']['max'] - ranges['VB']['min'])\n",
    "ndf['CF_LA'] = (df['CF_LA'] - ranges['CF_LA']['min'])/(ranges['CF_LA']['max'] - ranges['CF_LA']['min'])\n",
    "ndf['CA_LA'] = (df['CA_LA'] - ranges['CA_LA']['min'])/(ranges['CA_LA']['max'] - ranges['CA_LA']['min'])\n",
    "ndf['CB_LA'] = df['CB_LA']\n",
    "ndf['CF_K'] = (df['CF_K'] - ranges['CF_K']['min'])/(ranges['CF_K']['max'] - ranges['CF_K']['min'])\n",
    "ndf['CA_K'] = df['CA_K']\n",
    "ndf['CB_K'] = (df['CB_K'] - ranges['CB_K']['min'])/(ranges['CB_K']['max'] - ranges['CB_K']['min'])\n",
    "ndf['I'] = (df['I'] - ranges['I']['min'])/(ranges['I']['max'] - ranges['I']['min'])\n",
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data Preparation\n",
    "Vt_list = []\n",
    "E_list = []\n",
    "C_list = []\n",
    "Vm_list = []\n",
    "\n",
    "for exp_num in ndf['exp'].unique():\n",
    "    Vt_list.append(ndf[ndf['exp']==exp_num]['V'].values)\n",
    "    E_list.append(ndf[ndf['exp']==exp_num]['E'].values)\n",
    "    \n",
    "    # CF, CA, CB 순으로 묶고 LA, K 순으로 값을 저장\n",
    "    CF_LA = ndf[ndf['exp']==exp_num]['CF_LA'].values\n",
    "    CF_K = ndf[ndf['exp']==exp_num]['CF_K'].values\n",
    "    CA_LA = ndf[ndf['exp']==exp_num]['CA_LA'].values\n",
    "    CA_K = ndf[ndf['exp']==exp_num]['CA_K'].values\n",
    "    CB_LA = ndf[ndf['exp']==exp_num]['CB_LA'].values\n",
    "    CB_K = ndf[ndf['exp']==exp_num]['CB_K'].values\n",
    "    \n",
    "    # 시간순으로 데이터 정렬\n",
    "    C_exp = np.stack([\n",
    "        np.stack([CF_LA, CF_K], axis=1),  # Feed (LA, K)\n",
    "        np.stack([CA_LA, CA_K], axis=1),  # Acid (LA, K)\n",
    "        np.stack([CB_LA, CB_K], axis=1)   # Base (LA, K)\n",
    "    ], axis=1)\n",
    "    \n",
    "    C_list.append(C_exp)\n",
    "    Vm_list.append(df[df['exp']==exp_num][['VF', 'VA', 'VB']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vt, seq_lengths, max_length = pad_sequences(Vt_list)\n",
    "E, _, _ = pad_sequences(E_list)\n",
    "C, _, _ = pad_sequences(C_list)\n",
    "Vm, _, _ = pad_sequences(Vm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs\n",
    "input_tensor, initial_state, mask, seq_lengths = prepare_input_data(Vt, E, C, Vm, seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMEDModel(nn.Module):\n",
    "    def __init__(self, hidden_nodes = 64, num_rnn_layers = 2, cnn_channels = 32, max_seq_len = 37):\n",
    "        super(BMEDModel, self).__init__()\n",
    "\n",
    "        # Input dimensions\n",
    "        # 3 channels (Feed, Acid, Base) x 6 features each\n",
    "        # Features per channel: C_LA, C_K, Vm, Vt, E\n",
    "        self.input_channels = 3\n",
    "        self.input_features = 5\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # CNN Layers for channel-wise feature extraction\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(self.input_features, cnn_channels, kernel_size=3, padding=1), # 입력 특성을 cnn이 32개의 추출 특성을 생성\n",
    "            nn.ReLU(), # 비선형성 부여\n",
    "            nn.Conv1d(cnn_channels, cnn_channels//2, hernel_size=3, padding=1), # 추출 특성 중 중요 특성만 선별\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1) # 특성 추출 후 channel 축 차원 제거를 위한 평균값 산출\n",
    "        )\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(cnn_channels//2)\n",
    "\n",
    "        # RNN layers for temporal dependency\n",
    "        self.rnn_layers = nn.LSTM(\n",
    "            input_size = cnn_channels//2,\n",
    "            hidden_size = hidden_nodes,\n",
    "            num_layers = num_rnn_layers,\n",
    "            batch_first = True,\n",
    "            dropout = 0.2 if num_rnn_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Flux Head\n",
    "        self.flux_NN = nn.Sequential(\n",
    "            nn.Linear(hidden_nodes, hidden_nodes//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_nodes//2, 4) # [LA ion migration, K ion migration, Water Migration Feed to Acid, Water Migration Feed to Base]\n",
    "        )\n",
    "        \n",
    "        # Current Head\n",
    "        self.current_NN = nn.Sequential(\n",
    "            nn.Linear(hidden_nodes, hidden_nodes//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_nodes//2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, initial_state, seq_lengths, mask):\n",
    "        '''\n",
    "        x: [batch_size, sequence_length, channels, features]\n",
    "        initial_state: [batch_size, channels, features] - initial concentration and volumes\n",
    "        seq_lengths: [batch_size] - actual sequence lengths for each sample\n",
    "        mask: [batch_size, sequence_length] - mask for padded positions\n",
    "        '''\n",
    "        batch_size, seq_len, channels, features = x.shape\n",
    "\n",
    "        # reshape for CNN processing\n",
    "        x_cnn = x.view(batch_size*seq_len, channels, features)\n",
    "\n",
    "        # CNN feature extraction\n",
    "        cnn_features = self.cnn_layers(x_cnn)   # [batch*seq, chaneels//2, 1]\n",
    "        cnn_features = cnn_features.squeeze(-1) # [batch*seq, channels//2]\n",
    "\n",
    "        # Reshape back for RNN\n",
    "        rnn_input = cnn_features.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # Layer Normalization\n",
    "        rnn_input = self.layer_norm(rnn_input)\n",
    "\n",
    "        # Pack padded sequence for RNN\n",
    "        rnn_input = nn.utils.rnn.pack_padded_sequence(\n",
    "            rnn_input, seq_lengths.cpu(), batch_first = True, enforce_sorted=False\n",
    "        )\n",
    "        rnn_out, _ = self.rnn(rnn_input)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True) # pad 무시 후 다시 복원하여 layer 처리가 용이하도록 변경\n",
    "\n",
    "        # Predict Fluxes for each time step\n",
    "        fluxes = self.flux_NN(rnn_out) # [bath, seq_len, 4]\n",
    "\n",
    "        # Predict Current for each time step\n",
    "        current = self.current_NN(rnn_out) # [bath, seq_len, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalLayer(nn.Module):\n",
    "    def __init__(self, dt = 0.25): # 0.25 hour time step\n",
    "        super(PhysicalLayer, self).__init__()\n",
    "        self.dt = dt\n",
    "    \n",
    "    def forward(self, fluxes, initial_state, seq_lengths, mask):\n",
    "        '''\n",
    "        fluxes: [batch_size, seq_len, 4] - [LA migration, K migration, H2O Feed to Acid, H2O Feed to Base]\n",
    "        initial_state: [batch_size, 3, 3] - [Feed, Acid, Base] x [LA_conc, K_conc, Volume]\n",
    "        seq_lengths: [batch_size] - actual sequence lengths for each sample\n",
    "        mask: [batch_size, sequence_length] - mask for padded positions\n",
    "\n",
    "        returns: [batch_size, seq_len, 3, 3] - time series of [LA_conc, K_conc, Volume] for each channel\n",
    "        '''\n",
    "\n",
    "        batch_size, seq_len, _ = fluxes.shape\n",
    "\n",
    "        # initialize output tensor\n",
    "        outputs = torch.zeros(batch_size, seq_len, 3, 3)  # [batch, time, channel, property]\n",
    "\n",
    "        # Set initial conditions\n",
    "        cur_state = initial_state.clone() # [batch, channel, property]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            # Extract fluxes for current time step\n",
    "            LA_flux = fluxes[:, t, 0] # LA migration (Feed -> Acid)\n",
    "            K_flux = fluxes[:, t, 1] # K migration (Feed -> Base)\n",
    "            VFA_flux = fluxes[:, t, 2] # H2O migration from Feed to Acid (Feed -> Acid)\n",
    "            VFB_flux = fluxes[:, t, 3] # H2O migration from Feed to Base (Feed -> Base)\n",
    "\n",
    "\n",
    "            # Only update if within actual sequence lengths\n",
    "            time_mask = (t < seq_lengths).float()\n",
    "            LA_flux = LA_flux * time_mask\n",
    "            K_flux = K_flux * time_mask\n",
    "            VFA_flux = VFA_flux * time_mask\n",
    "            VFB_flux = VFB_flux * time_mask\n",
    "\n",
    "            # Mass balance calculations\n",
    "            cur_state = self.MB_step(cur_state, LA_flux, K_flux, VFA_flux, VFB_flux)\n",
    "\n",
    "            # Store results\n",
    "            outputs[:,t,:,:] = cur_state\n",
    "    \n",
    "    def MB_step(self, state, LA_flux, K_flux, VFA_flux, VFB_flux):\n",
    "        '''\n",
    "        Perfrom one time step of mass balance\n",
    "        state: [batch, 3, 3] - [Feed, Acid, Base] x [LA_conc, K_conc, Volume]\n",
    "        '''\n",
    "        batch_size = state.shape[0]\n",
    "        new_state = state.clone()\n",
    "\n",
    "        # Extract current values\n",
    "        # channel 0: feed, channel 1: acid, channel 2: base\n",
    "        # property 0: LA_conc, property 1: K_conc, property 2: volume\n",
    "\n",
    "        Feed_LA_Conc = state[:, 0, 0]\n",
    "        Feed_K_Conc = state[:, 0, 1]\n",
    "        Feed_Vol = state[:, 0, 2]\n",
    "\n",
    "        Acid_LA_Conc = state[:, 1, 0]\n",
    "        Acid_K_Conc = state[:, 1, 1]\n",
    "        Acid_Vol = state[:, 1, 2]\n",
    "\n",
    "        Base_LA_Conc = state[:, 2, 0]\n",
    "        Base_K_Conc = state[:, 2, 1]\n",
    "        Base_Vol = state[:, 2, 2]\n",
    "\n",
    "        # Volume Changes due to water flux\n",
    "        # Assuming positive flux means water moves from feed to acid or base\n",
    "        new_state[:, 0, 2] = Feed_Vol - (VFA_flux + VFB_flux) * self.dt # Feed Volume\n",
    "        new_state[:, 1, 2] = Acid_Vol + VFA_flux * self.dt # Acid Volume\n",
    "        new_state[:, 2, 2] = Base_Vol + VFB_flux * self.dt # Base Volume\n",
    "\n",
    "        # LA mass balance (Feed -> Acid)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fixed Length Example ===\n",
      "Input shape: torch.Size([16, 28, 3, 6])\n",
      "Initial state shape: torch.Size([16, 3, 3])\n",
      "Output shape: torch.Size([16, 28, 3, 3])\n",
      "Fluxes shape: torch.Size([16, 28, 4])\n",
      "Fixed length model created successfully!\n",
      "\n",
      "=== Variable Length (Padded) Example with Different RNN Types ===\n",
      "Creating models with different RNN types...\n",
      "\n",
      "=== Testing LSTM Model ===\n",
      "Input shape: torch.Size([4, 28, 3, 6])\n",
      "Sequence lengths: tensor([28, 20, 25, 15])\n",
      "Output shape: torch.Size([4, 28, 3, 3])\n",
      "Fluxes shape: torch.Size([4, 28, 4])\n",
      "Total parameters: 58,644\n",
      "\n",
      "=== Testing RNN Model ===\n",
      "Input shape: torch.Size([4, 28, 3, 6])\n",
      "Sequence lengths: tensor([28, 20, 25, 15])\n",
      "Output shape: torch.Size([4, 28, 3, 3])\n",
      "Fluxes shape: torch.Size([4, 28, 4])\n",
      "Total parameters: 17,940\n",
      "\n",
      "=== Testing GRU Model ===\n",
      "Input shape: torch.Size([4, 28, 3, 6])\n",
      "Sequence lengths: tensor([28, 20, 25, 15])\n",
      "Output shape: torch.Size([4, 28, 3, 3])\n",
      "Fluxes shape: torch.Size([4, 28, 4])\n",
      "Total parameters: 45,076\n",
      "\n",
      "=== Parameter Comparison ===\n",
      "LSTM: 58,644 parameters\n",
      "RNN: 17,940 parameters\n",
      "GRU: 45,076 parameters\n",
      "All models created successfully!\n",
      "\n",
      "=== Loss Calculation with Masking ===\n",
      "LSTM Output loss (masked): inf\n",
      "LSTM Flux loss (masked): 4.5807\n",
      "LSTM Total loss: inf\n",
      "\n",
      "=== Speed and Complexity Comparison ===\n",
      "RNN Type | Parameters | Relative Speed | Best For\n",
      "-------------------------------------------------------\n",
      "RNN      | Least      | Fastest        | Simple patterns, short sequences\n",
      "GRU      | Medium     | Medium         | Balance of performance & speed\n",
      "LSTM     | Most       | Slowest        | Complex patterns, long sequences\n",
      "\n",
      "For your 28-step BMED system, LSTM is recommended for best performance.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class BMEDModel(nn.Module):\n",
    "    \n",
    "    def forward(self, x, initial_state, seq_lengths=None, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, sequence_length, channels, features]\n",
    "        initial_state: [batch_size, channels, features] - initial concentrations and volumes\n",
    "        seq_lengths: [batch_size] - actual sequence lengths for each sample\n",
    "        mask: [batch_size, sequence_length] - mask for padded positions\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, channels, features = x.shape\n",
    "        \n",
    "        # Reshape for CNN processing\n",
    "        x_cnn = x.view(batch_size * seq_len, channels, features)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        cnn_features = self.cnn_layers(x_cnn)  # [batch*seq, channels//2, 1]\n",
    "        cnn_features = cnn_features.squeeze(-1)  # [batch*seq, channels//2]\n",
    "        \n",
    "        # Reshape back for RNN\n",
    "        rnn_input = cnn_features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Pack padded sequence for RNN if sequence lengths are provided\n",
    "        if seq_lengths is not None:\n",
    "            rnn_input = nn.utils.rnn.pack_padded_sequence(\n",
    "                rnn_input, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            rnn_out, _ = self.rnn(rnn_input)\n",
    "            rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        else:\n",
    "            rnn_out, _ = self.rnn(rnn_input)\n",
    "        \n",
    "        # Predict fluxes for each time step\n",
    "        fluxes = self.flux_predictor(rnn_out)  # [batch, seq_len, 4]\n",
    "        \n",
    "        # Apply mask to fluxes if provided\n",
    "        if mask is not None:\n",
    "            fluxes = fluxes * mask.unsqueeze(-1)\n",
    "        \n",
    "        # Apply physical layer to calculate concentrations and volumes\n",
    "        outputs = self.physical_layer(fluxes, initial_state, seq_lengths, mask)\n",
    "        \n",
    "        return outputs, fluxes\n",
    "\n",
    "class PhysicalLayer(nn.Module):\n",
    "    def __init__(self, dt=0.5):  # 0.5 hour time step\n",
    "        super(PhysicalLayer, self).__init__()\n",
    "        self.dt = dt\n",
    "    \n",
    "    def forward(self, fluxes, initial_state, seq_lengths=None, mask=None):\n",
    "        \"\"\"\n",
    "        fluxes: [batch_size, seq_len, 4] - [Water_flux, LA_flux, K_flux, Current]\n",
    "        initial_state: [batch_size, 3, 3] - [Feed, Acid, Base] x [LA_conc, K_conc, Volume]\n",
    "        seq_lengths: [batch_size] - actual sequence lengths for each sample\n",
    "        mask: [batch_size, seq_len] - mask for padded positions\n",
    "        \n",
    "        Returns: [batch_size, seq_len, 3, 3] - time series of [LA_conc, K_conc, Volume] for each channel\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = fluxes.shape\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        outputs = torch.zeros(batch_size, seq_len, 3, 3)  # [batch, time, channel, property]\n",
    "        \n",
    "        # Set initial conditions\n",
    "        current_state = initial_state.clone()  # [batch, channel, property]\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Extract fluxes for current time step\n",
    "            water_flux = fluxes[:, t, 0]  # Water movement\n",
    "            la_flux = fluxes[:, t, 1]     # LA movement (Feed → Acid)\n",
    "            k_flux = fluxes[:, t, 2]      # K movement (Feed → Base)\n",
    "            \n",
    "            # Only update if within actual sequence length\n",
    "            if seq_lengths is not None:\n",
    "                # Create mask for this time step\n",
    "                time_mask = (t < seq_lengths).float()\n",
    "                water_flux = water_flux * time_mask\n",
    "                la_flux = la_flux * time_mask\n",
    "                k_flux = k_flux * time_mask\n",
    "            elif mask is not None:\n",
    "                time_mask = mask[:, t]\n",
    "                water_flux = water_flux * time_mask\n",
    "                la_flux = la_flux * time_mask\n",
    "                k_flux = k_flux * time_mask\n",
    "            \n",
    "            # Mass balance calculations\n",
    "            current_state = self.mass_balance_step(current_state, water_flux, la_flux, k_flux)\n",
    "            \n",
    "            # Store results\n",
    "            outputs[:, t, :, :] = current_state\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def mass_balance_step(self, state, water_flux, la_flux, k_flux):\n",
    "        \"\"\"\n",
    "        Perform one time step of mass balance\n",
    "        state: [batch, 3, 3] - [Feed, Acid, Base] x [LA_conc, K_conc, Volume]\n",
    "        \"\"\"\n",
    "        batch_size = state.shape[0]\n",
    "        new_state = state.clone()\n",
    "        \n",
    "        # Extract current values\n",
    "        # Channel 0: Feed, Channel 1: Acid, Channel 2: Base\n",
    "        # Property 0: LA_conc, Property 1: K_conc, Property 2: Volume\n",
    "        \n",
    "        feed_la_conc = state[:, 0, 0]\n",
    "        feed_k_conc = state[:, 0, 1]\n",
    "        feed_vol = state[:, 0, 2]\n",
    "        \n",
    "        acid_la_conc = state[:, 1, 0]\n",
    "        acid_vol = state[:, 1, 2]\n",
    "        \n",
    "        base_k_conc = state[:, 2, 1]\n",
    "        base_vol = state[:, 2, 2]\n",
    "        \n",
    "        # Volume changes due to water flux\n",
    "        # Assuming positive flux means water moves from feed\n",
    "        new_state[:, 0, 2] = feed_vol - water_flux * self.dt  # Feed volume decreases\n",
    "        new_state[:, 1, 2] = acid_vol + water_flux * self.dt * 0.5  # Acid volume increases\n",
    "        new_state[:, 2, 2] = base_vol + water_flux * self.dt * 0.5  # Base volume increases\n",
    "        \n",
    "        # LA mass balance (Feed → Acid)\n",
    "        la_moles_transferred = la_flux * self.dt\n",
    "        \n",
    "        # Feed LA decreases\n",
    "        feed_la_moles = feed_la_conc * feed_vol\n",
    "        new_feed_la_moles = torch.clamp(feed_la_moles - la_moles_transferred, min=0)\n",
    "        new_state[:, 0, 0] = new_feed_la_moles / torch.clamp(new_state[:, 0, 2], min=1e-6)\n",
    "        \n",
    "        # Acid LA increases\n",
    "        acid_la_moles = acid_la_conc * acid_vol\n",
    "        new_acid_la_moles = acid_la_moles + la_moles_transferred\n",
    "        new_state[:, 1, 0] = new_acid_la_moles / torch.clamp(new_state[:, 1, 2], min=1e-6)\n",
    "        \n",
    "        # K mass balance (Feed → Base)\n",
    "        k_moles_transferred = k_flux * self.dt\n",
    "        \n",
    "        # Feed K decreases\n",
    "        feed_k_moles = feed_k_conc * feed_vol\n",
    "        new_feed_k_moles = torch.clamp(feed_k_moles - k_moles_transferred, min=0)\n",
    "        new_state[:, 0, 1] = new_feed_k_moles / torch.clamp(new_state[:, 0, 2], min=1e-6)\n",
    "        \n",
    "        # Base K increases\n",
    "        base_k_moles = base_k_conc * base_vol\n",
    "        new_base_k_moles = base_k_moles + k_moles_transferred\n",
    "        new_state[:, 2, 1] = new_base_k_moles / torch.clamp(new_state[:, 2, 2], min=1e-6)\n",
    "        \n",
    "        # Acid and Base don't have K and LA respectively (constraint)\n",
    "        new_state[:, 1, 1] = 0  # No K in acid\n",
    "        new_state[:, 2, 0] = 0  # No LA in base\n",
    "        \n",
    "        return new_state\n",
    "\n",
    "def prepare_input_data(voltage, ext_electrolyte, concentrations, volumes, currents, seq_lengths=None):\n",
    "    \"\"\"\n",
    "    Prepare input data for the model with padding support\n",
    "    \n",
    "    Args:\n",
    "        voltage: [batch_size, seq_len] - applied voltage\n",
    "        ext_electrolyte: [batch_size, seq_len] - external electrolyte concentration\n",
    "        concentrations: [batch_size, seq_len, 3, 2] - [Feed, Acid, Base] x [LA, K] concentrations\n",
    "        volumes: [batch_size, seq_len, 3] - volumes for each channel\n",
    "        currents: [batch_size, seq_len] - measured currents\n",
    "        seq_lengths: [batch_size] - actual sequence lengths (optional)\n",
    "    \n",
    "    Returns:\n",
    "        input_tensor: [batch_size, seq_len, 3, 6] - formatted input for CNN-LSTM\n",
    "        initial_state: [batch_size, 3, 3] - initial concentrations and volumes\n",
    "        mask: [batch_size, seq_len] - padding mask\n",
    "        seq_lengths: [batch_size] - actual sequence lengths\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = voltage.shape\n",
    "    \n",
    "    # Create input tensor\n",
    "    input_tensor = torch.zeros(batch_size, seq_len, 3, 6)\n",
    "    \n",
    "    # Fill input tensor for each channel\n",
    "    for channel in range(3):\n",
    "        input_tensor[:, :, channel, 0] = concentrations[:, :, channel, 0]  # LA concentration\n",
    "        input_tensor[:, :, channel, 1] = concentrations[:, :, channel, 1]  # K concentration\n",
    "        input_tensor[:, :, channel, 2] = volumes[:, :, channel]            # Volume\n",
    "        input_tensor[:, :, channel, 3] = voltage                          # Voltage (same for all)\n",
    "        input_tensor[:, :, channel, 4] = ext_electrolyte                  # Ext electrolyte (same for all)\n",
    "        input_tensor[:, :, channel, 5] = currents                        # Current (same for all)\n",
    "    \n",
    "    # Initial state: [LA_conc, K_conc, Volume] for each channel at t=0\n",
    "    initial_state = torch.zeros(batch_size, 3, 3)\n",
    "    initial_state[:, :, 0] = concentrations[:, 0, :, 0]  # Initial LA concentrations\n",
    "    initial_state[:, :, 1] = concentrations[:, 0, :, 1]  # Initial K concentrations\n",
    "    initial_state[:, :, 2] = volumes[:, 0, :]            # Initial volumes\n",
    "    \n",
    "    # Create padding mask\n",
    "    if seq_lengths is None:\n",
    "        seq_lengths = torch.full((batch_size,), seq_len, dtype=torch.long)\n",
    "    \n",
    "    mask = torch.zeros(batch_size, seq_len)\n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        mask[i, :length] = 1.0\n",
    "    \n",
    "    return input_tensor, initial_state, mask, seq_lengths\n",
    "\n",
    "def pad_sequences(data_list, max_length=None, pad_value=0.0):\n",
    "    \"\"\"\n",
    "    Pad variable length sequences to the same length\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of tensors with different sequence lengths\n",
    "        max_length: Maximum length to pad to (default: longest sequence)\n",
    "        pad_value: Value to use for padding\n",
    "    \n",
    "    Returns:\n",
    "        padded_tensor: [batch_size, max_length, ...] - padded sequences\n",
    "        seq_lengths: [batch_size] - original sequence lengths\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = max(data.shape[0] for data in data_list)\n",
    "    \n",
    "    batch_size = len(data_list)\n",
    "    seq_lengths = torch.tensor([data.shape[0] for data in data_list])\n",
    "    \n",
    "    # Get shape of individual elements\n",
    "    example_shape = data_list[0].shape[1:]  # Remove time dimension\n",
    "    \n",
    "    # Create padded tensor\n",
    "    padded_tensor = torch.full(\n",
    "        (batch_size, max_length) + example_shape, \n",
    "        pad_value, \n",
    "        dtype=data_list[0].dtype\n",
    "    )\n",
    "    \n",
    "    # Fill with actual data\n",
    "    for i, data in enumerate(data_list):\n",
    "        actual_length = min(data.shape[0], max_length)\n",
    "        padded_tensor[i, :actual_length] = data[:actual_length]\n",
    "    \n",
    "    return padded_tensor, seq_lengths\n",
    "\n",
    "# Example usage with variable length sequences\n",
    "def create_model_with_padding_example():\n",
    "    # Model instantiation with different RNN types\n",
    "    print(\"Creating models with different RNN types...\")\n",
    "    \n",
    "    models = {}\n",
    "    models['LSTM'] = BMEDModel(hidden_size=64, rnn_layers=2, cnn_channels=32, max_seq_len=28, rnn_type='LSTM')\n",
    "    models['RNN'] = BMEDModel(hidden_size=64, rnn_layers=2, cnn_channels=32, max_seq_len=28, rnn_type='RNN')\n",
    "    models['GRU'] = BMEDModel(hidden_size=64, rnn_layers=2, cnn_channels=32, max_seq_len=28, rnn_type='GRU')\n",
    "    \n",
    "    # Example: Create variable length sequences\n",
    "    batch_size = 4\n",
    "    max_seq_len = 28  # Updated to 28 for 14 hours\n",
    "    \n",
    "    # Simulate different experiment durations\n",
    "    actual_lengths = [28, 20, 25, 15]  # Different experiment durations\n",
    "    \n",
    "    # Create variable length data\n",
    "    voltage_list = []\n",
    "    ext_electrolyte_list = []\n",
    "    concentrations_list = []\n",
    "    volumes_list = []\n",
    "    currents_list = []\n",
    "    \n",
    "    for length in actual_lengths:\n",
    "        voltage_list.append(torch.randn(length) * 5 + 10)\n",
    "        ext_electrolyte_list.append(torch.randn(length) * 0.1 + 0.5)\n",
    "        concentrations_list.append(torch.randn(length, 3, 2) * 0.1 + 0.5)\n",
    "        volumes_list.append(torch.randn(length, 3) * 0.05 + 1.0)\n",
    "        currents_list.append(torch.randn(length) * 2 + 5)\n",
    "    \n",
    "    # Pad sequences\n",
    "    voltage_padded, seq_lengths = pad_sequences(voltage_list, max_seq_len)\n",
    "    ext_electrolyte_padded, _ = pad_sequences(ext_electrolyte_list, max_seq_len)\n",
    "    concentrations_padded, _ = pad_sequences(concentrations_list, max_seq_len)\n",
    "    volumes_padded, _ = pad_sequences(volumes_list, max_seq_len)\n",
    "    currents_padded, _ = pad_sequences(currents_list, max_seq_len)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    input_tensor, initial_state, mask, seq_lengths = prepare_input_data(\n",
    "        voltage_padded, ext_electrolyte_padded, concentrations_padded, \n",
    "        volumes_padded, currents_padded, seq_lengths\n",
    "    )\n",
    "    \n",
    "    # Test all models\n",
    "    results = {}\n",
    "    for rnn_type, model in models.items():\n",
    "        print(f\"\\n=== Testing {rnn_type} Model ===\")\n",
    "        \n",
    "        # Forward pass with padding support\n",
    "        outputs, fluxes = model(input_tensor, initial_state, seq_lengths, mask)\n",
    "        \n",
    "        print(f\"Input shape: {input_tensor.shape}\")\n",
    "        print(f\"Sequence lengths: {seq_lengths}\")\n",
    "        print(f\"Output shape: {outputs.shape}\")\n",
    "        print(f\"Fluxes shape: {fluxes.shape}\")\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        \n",
    "        results[rnn_type] = {\n",
    "            'model': model,\n",
    "            'outputs': outputs,\n",
    "            'fluxes': fluxes,\n",
    "            'params': total_params\n",
    "        }\n",
    "    \n",
    "    # Show parameter comparison\n",
    "    print(f\"\\n=== Parameter Comparison ===\")\n",
    "    for rnn_type, result in results.items():\n",
    "        print(f\"{rnn_type}: {result['params']:,} parameters\")\n",
    "    \n",
    "    return results, input_tensor, initial_state, mask, seq_lengths\n",
    "\n",
    "# Example usage and training setup\n",
    "def create_model_and_example():\n",
    "    # Model instantiation with LSTM (default)\n",
    "    model = BMEDModel(hidden_size=64, rnn_layers=2, cnn_channels=32, rnn_type='LSTM')\n",
    "    \n",
    "    # Example data shapes\n",
    "    batch_size = 16\n",
    "    seq_len = 28  # 14 hours / 0.5 hour steps\n",
    "    \n",
    "    # Example input preparation\n",
    "    voltage = torch.randn(batch_size, seq_len) * 5 + 10  # 10±5V\n",
    "    ext_electrolyte = torch.randn(batch_size, seq_len) * 0.1 + 0.5  # 0.5±0.1 M\n",
    "    \n",
    "    # Concentrations [batch, time, channel, component]\n",
    "    concentrations = torch.randn(batch_size, seq_len, 3, 2) * 0.1 + 0.5\n",
    "    volumes = torch.randn(batch_size, seq_len, 3) * 0.05 + 1.0  # 1±0.05 L\n",
    "    currents = torch.randn(batch_size, seq_len) * 2 + 5  # 5±2 A\n",
    "    \n",
    "    # Prepare inputs\n",
    "    input_tensor, initial_state, mask, seq_lengths = prepare_input_data(\n",
    "        voltage, ext_electrolyte, concentrations, volumes, currents\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs, fluxes = model(input_tensor, initial_state)\n",
    "    \n",
    "    print(f\"Input shape: {input_tensor.shape}\")\n",
    "    print(f\"Initial state shape: {initial_state.shape}\")\n",
    "    print(f\"Output shape: {outputs.shape}\")  # [batch, seq_len, 3, 3]\n",
    "    print(f\"Fluxes shape: {fluxes.shape}\")   # [batch, seq_len, 4]\n",
    "    \n",
    "    return model, input_tensor, initial_state, outputs, fluxes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example with fixed length sequences\n",
    "    print(\"=== Fixed Length Example ===\")\n",
    "    model, inputs, initial_state, outputs, fluxes = create_model_and_example()\n",
    "    print(\"Fixed length model created successfully!\\n\")\n",
    "    \n",
    "    # Example with variable length sequences and different RNN types\n",
    "    print(\"=== Variable Length (Padded) Example with Different RNN Types ===\")\n",
    "    results, inputs_padded, initial_state_padded, mask, seq_lengths = create_model_with_padding_example()\n",
    "    print(\"All models created successfully!\")\n",
    "    \n",
    "    # Example of loss calculation with masking\n",
    "    print(\"\\n=== Loss Calculation with Masking ===\")\n",
    "    \n",
    "    # Use LSTM model for loss calculation example\n",
    "    lstm_outputs = results['LSTM']['outputs']\n",
    "    lstm_fluxes = results['LSTM']['fluxes']\n",
    "    \n",
    "    # Dummy target data\n",
    "    target_outputs = torch.randn_like(lstm_outputs)\n",
    "    target_fluxes = torch.randn_like(lstm_fluxes)\n",
    "    \n",
    "    # Masked loss calculation\n",
    "    def masked_mse_loss(predictions, targets, mask):\n",
    "        \"\"\"Calculate MSE loss only for non-padded positions\"\"\"\n",
    "        loss = ((predictions - targets) ** 2) * mask.unsqueeze(-1)\n",
    "        return loss.sum() / mask.sum()\n",
    "    \n",
    "    # Calculate losses for LSTM\n",
    "    output_loss = masked_mse_loss(lstm_outputs.view(lstm_outputs.shape[0], lstm_outputs.shape[1], -1), \n",
    "                                 target_outputs.view(target_outputs.shape[0], target_outputs.shape[1], -1), \n",
    "                                 mask)\n",
    "    flux_loss = masked_mse_loss(lstm_fluxes, target_fluxes, mask)\n",
    "    \n",
    "    print(f\"LSTM Output loss (masked): {output_loss.item():.4f}\")\n",
    "    print(f\"LSTM Flux loss (masked): {flux_loss.item():.4f}\")\n",
    "    print(f\"LSTM Total loss: {(output_loss + flux_loss).item():.4f}\")\n",
    "    \n",
    "    # Speed comparison (rough estimate)\n",
    "    print(f\"\\n=== Speed and Complexity Comparison ===\")\n",
    "    print(\"RNN Type | Parameters | Relative Speed | Best For\")\n",
    "    print(\"-\" * 55)\n",
    "    print(\"RNN      | Least      | Fastest        | Simple patterns, short sequences\")\n",
    "    print(\"GRU      | Medium     | Medium         | Balance of performance & speed\")\n",
    "    print(\"LSTM     | Most       | Slowest        | Complex patterns, long sequences\")\n",
    "    print(\"\\nFor your 28-step BMED system, LSTM is recommended for best performance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
