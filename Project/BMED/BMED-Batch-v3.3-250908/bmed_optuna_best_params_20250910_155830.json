{
  "lstm_hidden_size": 32,
  "lstm_n_layers": 2,
  "lstm_dropout": 0.5,
  "decoder_hidden_size": 64,
  "decoder_n_layers": 2,
  "decoder_dropout": 0.2,
  "current_hidden_size": 72,
  "current_n_layers": 2,
  "current_dropout": 0.30000000000000004,
  "noam_factor": 1.2000000000000002,
  "warmup_ratio": 0.25,
  "batch_size": 16
}