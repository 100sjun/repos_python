{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dbd6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ed6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수들\n",
    "def df_treat(name):\n",
    "    \"\"\"데이터 정규화 및 전처리\"\"\"\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "\n",
    "    exp_num_list = sorted(ndf['exp'].unique())\n",
    "    return df, ndf, range_mm, exp_num_list\n",
    "\n",
    "def seq_data(ndf, exp_num_list):\n",
    "    \"\"\"시퀀스 데이터 생성\"\"\"\n",
    "    seq = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "    \n",
    "    for exp in exp_num_list:\n",
    "        exp_df = ndf[ndf['exp'] == exp]\n",
    "        seq.append(exp_df[feature_cols].values)\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def pad_seq(seq):\n",
    "    \"\"\"시퀀스 패딩\"\"\"\n",
    "    max_len = max([len(s) for s in seq])\n",
    "    seq_len = [len(s) for s in seq]\n",
    "    pad_seq = pad_sequence([torch.tensor(s) for s in seq], batch_first=True, padding_value=-1)\n",
    "    return pad_seq, seq_len, max_len\n",
    "\n",
    "def gen_dataset(pad_seq, seq_len):\n",
    "    \"\"\"데이터셋 생성\"\"\"\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "221f4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 정의 (기존 학습된 모델과 동일한 구조)\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    \"\"\"LSTM layer with layer normalization applied to gates\"\"\"\n",
    "    def __init__(self, input_node, hidden_node):\n",
    "        super().__init__()\n",
    "        self.input_node = input_node\n",
    "        self.hidden_node = hidden_node\n",
    "\n",
    "        self.w_i = nn.Linear(input_node, 4 * hidden_node, bias=False)\n",
    "        self.w_h = nn.Linear(hidden_node, 4 * hidden_node, bias=False)\n",
    "\n",
    "        self.ln_i = nn.LayerNorm(hidden_node)\n",
    "        self.ln_h = nn.LayerNorm(hidden_node)\n",
    "        self.ln_g = nn.LayerNorm(hidden_node)\n",
    "        self.ln_o = nn.LayerNorm(hidden_node)\n",
    "\n",
    "        self.ln_c = nn.LayerNorm(hidden_node)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        gi = self.w_i(input)\n",
    "        gh = self.w_h(h_prev)\n",
    "        i_i, i_f, i_g, i_o = gi.chunk(4, dim=-1)\n",
    "        h_i, h_f, h_g, h_o = gh.chunk(4, dim=-1)\n",
    "\n",
    "        i_g = torch.sigmoid(self.ln_i(i_i + h_i))\n",
    "        f_g = torch.sigmoid(self.ln_h(i_f + h_f))\n",
    "        g_g = torch.tanh(self.ln_g(i_g + h_g))\n",
    "        o_g = torch.sigmoid(self.ln_o(i_o + h_o))\n",
    "\n",
    "        c_new = f_g * c_prev + i_g * g_g\n",
    "        c_new = self.ln_c(c_new)\n",
    "\n",
    "        h_new = o_g * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new\n",
    "class StateExtr(nn.Module):\n",
    "    \"\"\"State Extractor using LayerNorm LSTM\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        for _ in range(self.n_layer):\n",
    "            h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "            c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            layer_input = x_t\n",
    "            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "                h_new, c_new = lstm_cell(layer_input, (h_states[layer_idx], c_states[layer_idx]))\n",
    "\n",
    "                h_states[layer_idx] = h_new\n",
    "                c_states[layer_idx] = c_new\n",
    "\n",
    "                if layer_idx < len(self.lstm_cells) - 1:\n",
    "                    layer_input = self.dropout(h_new)\n",
    "                else:\n",
    "                    layer_input = h_new\n",
    "\n",
    "            outputs.append(layer_input)\n",
    "        \n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        normalized = self.final_layer_norm(masked_output)\n",
    "        return self.final_dropout(normalized)\n",
    "\n",
    "# Stateful State Extractor for Free Running Model\n",
    "class StatefulStateExtr(nn.Module):\n",
    "    \"\"\"Hidden state를 유지하며 sequential 처리하는 State Extractor\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Hidden states를 클래스 변수로 관리\n",
    "        self.h_states = None\n",
    "        self.c_states = None\n",
    "\n",
    "    def reset_states(self, batch_size=None, device=None):\n",
    "        \"\"\"새로운 시퀀스 시작 시 hidden state 초기화\"\"\"\n",
    "        if batch_size is not None and device is not None:\n",
    "            self.h_states = []\n",
    "            self.c_states = []\n",
    "            for _ in range(self.n_layer):\n",
    "                self.h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "                self.c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        else:\n",
    "            self.h_states = None\n",
    "            self.c_states = None\n",
    "\n",
    "    def forward_single_timestep(self, x_t):\n",
    "        \"\"\"단일 시점 처리 - hidden state 유지\"\"\"\n",
    "        batch_size = x_t.size(0)\n",
    "        device = x_t.device\n",
    "        \n",
    "        # 첫 번째 호출 시 hidden state 초기화\n",
    "        if self.h_states is None:\n",
    "            self.reset_states(batch_size, device)\n",
    "\n",
    "        layer_input = x_t\n",
    "        for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "            h_new, c_new = lstm_cell(layer_input, (self.h_states[layer_idx], self.c_states[layer_idx]))\n",
    "\n",
    "            # Hidden state 업데이트\n",
    "            self.h_states[layer_idx] = h_new\n",
    "            self.c_states[layer_idx] = c_new\n",
    "\n",
    "            if layer_idx < len(self.lstm_cells) - 1:\n",
    "                layer_input = self.dropout(h_new)\n",
    "            else:\n",
    "                layer_input = h_new\n",
    "\n",
    "        # 정규화 및 드롭아웃 적용\n",
    "        normalized = self.final_layer_norm(layer_input)\n",
    "        return self.final_dropout(normalized)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"기존 teacher forcing 방식과 호환성 유지\"\"\"\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # Teacher forcing 모드에서는 매번 hidden state 초기화\n",
    "        self.reset_states(batch_size, device)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "            output = self.forward_single_timestep(x_t)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        \n",
    "        # 마스킹 적용\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        return masked_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a72689bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"Physical Change Decoder\"\"\"\n",
    "    def __init__(self, input_node, output_node, n_layer, hidden_node, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_node, output_node))\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CurrentPredictor(nn.Module):\n",
    "    \"\"\"Current Predictor\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_node, 1))\n",
    "    \n",
    "    def forward(self, new_state):\n",
    "        x = new_state\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db6a567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"Physics Constraint Layer with Current Prediction\"\"\"\n",
    "    def __init__(self, range_mm, current_predictor, eps=1e-2):\n",
    "        super().__init__()\n",
    "        self.sps = eps\n",
    "        self.current_predictor = current_predictor\n",
    "        self.register_buffer('range_mm_tensor', self._convert_range_to_tensor(range_mm))\n",
    "\n",
    "    def _convert_range_to_tensor(self, range_mm):\n",
    "        feature_names = ['V','E','VF','VA','VB','CFLA','CALA','CFK','CBK','I']\n",
    "        ranges = torch.zeros(len(feature_names),2)\n",
    "\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if name in range_mm:\n",
    "                ranges[i, 0] = range_mm[name]['min']\n",
    "                ranges[i, 1] = range_mm[name]['max']\n",
    "        \n",
    "        return ranges\n",
    "    \n",
    "    def normalize(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "    def denormalize(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return data * (max_val - min_val) + min_val\n",
    "\n",
    "    def forward(self, physical_changes, current_state):\n",
    "        V_idx, E_idx, VF_idx, VA_idx, VB_idx = 0, 1, 2, 3, 4\n",
    "        CFLA_idx, CALA_idx, CFK_idx, CBK_idx, I_idx = 5, 6, 7, 8, 9\n",
    "\n",
    "        VF = self.denormalize(current_state[..., 2:3], VF_idx)\n",
    "        VA = self.denormalize(current_state[..., 3:4], VA_idx)\n",
    "        VB = self.denormalize(current_state[..., 4:5], VB_idx)\n",
    "        CFLA = self.denormalize(current_state[..., 5:6], CFLA_idx)\n",
    "        CALA = self.denormalize(current_state[..., 6:7], CALA_idx)\n",
    "        CFK = self.denormalize(current_state[..., 7:8], CFK_idx)\n",
    "        CBK = self.denormalize(current_state[..., 8:9], CBK_idx)\n",
    "\n",
    "        dVA = physical_changes[..., 0:1]\n",
    "        dVB = physical_changes[..., 1:2]\n",
    "        rratio = physical_changes[..., 2:3]\n",
    "        dNBK = physical_changes[..., 3:4]\n",
    "\n",
    "        ratio = torch.sigmoid(rratio)\n",
    "        dNALA = ratio * dNBK\n",
    "\n",
    "        NFLA = CFLA * VF\n",
    "        NALA = CALA * VA\n",
    "        NFK = CFK * VF\n",
    "        NBK = CBK * VB\n",
    "\n",
    "        nVF = VF - dVA - dVB\n",
    "        nVA = VA + dVA\n",
    "        nVB = VB + dVB\n",
    "\n",
    "        nVF = torch.clamp(nVF, min=self.sps)\n",
    "        nVA = torch.clamp(nVA, min=self.sps)\n",
    "        nVB = torch.clamp(nVB, min=self.sps)\n",
    "        \n",
    "        nNFLA = NFLA - torch.clamp(dNALA, min=0.0)\n",
    "        nNALA = NALA + torch.clamp(dNALA, min=0.0)\n",
    "        nNFK = NFK - torch.clamp(dNBK, min=0.0)\n",
    "        nNBK = NBK + torch.clamp(dNBK, min=0.0)\n",
    "\n",
    "        nNFLA = torch.clamp(nNFLA, min=0.0)\n",
    "        nNALA = torch.clamp(nNALA, min=0.0)\n",
    "        nNFK = torch.clamp(nNFK, min=0.0)\n",
    "        nNBK = torch.clamp(nNBK, min=0.0)\n",
    "\n",
    "        nCFLA = nNFLA / nVF\n",
    "        nCALA = nNALA / nVA\n",
    "        nCFK = nNFK / nVF\n",
    "        nCBK = nNBK / nVB\n",
    "\n",
    "        V = current_state[..., 0:1]\n",
    "        E = current_state[..., 1:2]\n",
    "        nVF_norm = self.normalize(nVF, VF_idx)\n",
    "        nVA_norm = self.normalize(nVA, VA_idx)\n",
    "        nVB_norm = self.normalize(nVB, VB_idx)\n",
    "        nCFLA_norm = self.normalize(nCFLA, CFLA_idx)\n",
    "        nCALA_norm = self.normalize(nCALA, CALA_idx)\n",
    "        nCFK_norm = self.normalize(nCFK, CFK_idx)\n",
    "        nCBK_norm = self.normalize(nCBK, CBK_idx)\n",
    "\n",
    "        temp_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        nI_pred_norm = self.current_predictor(temp_state)\n",
    "        \n",
    "        nI_real = self.denormalize(nI_pred_norm, I_idx)\n",
    "        nI_real = torch.clamp(nI_real, min=0.0)\n",
    "        nI_norm = self.normalize(nI_real, I_idx)\n",
    "\n",
    "        next_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm, nI_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b90b3815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 통합된 BMEDAutoregressiveModel 구현 완료!\n"
     ]
    }
   ],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"통합 BMED Autoregressive Model - Teacher Forcing & Free Running 모드 지원\"\"\"\n",
    "    def __init__(self, state_extr_params, decoder_params, current_predictor_params, range_mm):\n",
    "        super().__init__()\n",
    "        self.state_extr = StateExtr(**state_extr_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.current_predictor = CurrentPredictor(**current_predictor_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer(range_mm, self.current_predictor)\n",
    "        \n",
    "        # Free running을 위한 hidden state 관리\n",
    "        self._hidden_states = None\n",
    "        self._cell_states = None\n",
    "\n",
    "    def _reset_hidden_states(self, batch_size, device):\n",
    "        \"\"\"Free running 시작 시 hidden state 초기화\"\"\"\n",
    "        self._hidden_states = []\n",
    "        self._cell_states = []\n",
    "        for _ in range(self.state_extr.n_layer):\n",
    "            self._hidden_states.append(torch.zeros(batch_size, self.state_extr.hidden_node, device=device))\n",
    "            self._cell_states.append(torch.zeros(batch_size, self.state_extr.hidden_node, device=device))\n",
    "\n",
    "    def teacher_forcing_forward(self, x, seq_len):\n",
    "        \"\"\"Teacher Forcing 모드: 기존 방식\"\"\"\n",
    "        hidden_states = self.state_extr(x, seq_len)\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        new_x = self.physics_constraint(physical_changes, x)\n",
    "        return new_x\n",
    "\n",
    "    def free_running_forward(self, initial_state, target_length):\n",
    "        \"\"\"\n",
    "        Free Running 모드: 초기 상태만으로 전체 시퀀스 생성\n",
    "        Args:\n",
    "            initial_state: [batch, features] - 초기 상태 (모든 10개 특성 포함)\n",
    "            target_length: int - 예측할 시퀀스 길이\n",
    "        Returns:\n",
    "            predictions: [batch, target_length, features] - 예측된 전체 시퀀스\n",
    "        \"\"\"\n",
    "        batch_size = initial_state.size(0)\n",
    "        feature_size = initial_state.size(1)\n",
    "        device = initial_state.device\n",
    "        \n",
    "        # Hidden state 초기화\n",
    "        self._reset_hidden_states(batch_size, device)\n",
    "        \n",
    "        # 예측 결과 저장\n",
    "        predictions = torch.zeros(batch_size, target_length, feature_size, device=device)\n",
    "        current_state = initial_state.clone()\n",
    "        \n",
    "        for t in range(target_length):\n",
    "            predictions[:, t, :] = current_state\n",
    "            \n",
    "            if t < target_length - 1:\n",
    "                # 현재 상태에서 I(전류) 제거하여 LSTM 입력 생성 (9개 특성)\n",
    "                lstm_input = current_state[:, :-1]  # [batch, 9]\n",
    "                \n",
    "                # LSTM forward with hidden state maintenance\n",
    "                hidden_output = self._forward_lstm_single_step(lstm_input)\n",
    "                \n",
    "                # Physical change 예측\n",
    "                physical_changes = self.physical_decoder(hidden_output.unsqueeze(1))  # [batch, 1, output]\n",
    "                \n",
    "                # Physics constraint로 다음 상태 계산\n",
    "                current_state_expanded = current_state.unsqueeze(1)  # [batch, 1, features]\n",
    "                next_state = self.physics_constraint(physical_changes, current_state_expanded)\n",
    "                current_state = next_state.squeeze(1)  # [batch, features]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def _forward_lstm_single_step(self, x_t):\n",
    "        \"\"\"단일 시점 LSTM forward (hidden state 유지)\"\"\"\n",
    "        layer_input = x_t\n",
    "        \n",
    "        for layer_idx, lstm_cell in enumerate(self.state_extr.lstm_cells):\n",
    "            h_new, c_new = lstm_cell(layer_input, \n",
    "                                   (self._hidden_states[layer_idx], self._cell_states[layer_idx]))\n",
    "            \n",
    "            # Hidden state 업데이트\n",
    "            self._hidden_states[layer_idx] = h_new\n",
    "            self._cell_states[layer_idx] = c_new\n",
    "            \n",
    "            # Dropout 적용 (마지막 layer 제외)\n",
    "            if layer_idx < len(self.state_extr.lstm_cells) - 1:\n",
    "                layer_input = self.state_extr.dropout(h_new)\n",
    "            else:\n",
    "                layer_input = h_new\n",
    "        \n",
    "        # 최종 정규화 및 드롭아웃\n",
    "        normalized = self.state_extr.final_layer_norm(layer_input)\n",
    "        return self.state_extr.final_dropout(normalized)\n",
    "\n",
    "    def forward(self, x, seq_len=None, mode='teacher_forcing', target_length=None):\n",
    "        \"\"\"\n",
    "        통합 forward 메소드\n",
    "        Args:\n",
    "            x: 입력 데이터\n",
    "            seq_len: 시퀀스 길이 (teacher forcing 모드에서 사용)\n",
    "            mode: 'teacher_forcing' 또는 'free_running'\n",
    "            target_length: free running에서 예측할 길이\n",
    "        \"\"\"\n",
    "        if mode == 'teacher_forcing':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len은 teacher_forcing 모드에서 필수입니다\")\n",
    "            return self.teacher_forcing_forward(x, seq_len)\n",
    "        \n",
    "        elif mode == 'free_running':\n",
    "            if target_length is None:\n",
    "                raise ValueError(\"target_length는 free_running 모드에서 필수입니다\")\n",
    "            return self.free_running_forward(x, target_length)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 모드: {mode}. 'teacher_forcing' 또는 'free_running'을 사용하세요.\")\n",
    "\n",
    "print(\"✅ 통합된 BMEDAutoregressiveModel 구현 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17822324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 준비 함수들 구현 완료!\n",
      "   - tf_data: Teacher Forcing용 입력/타겟 준비\n",
      "   - fr_data: Free Running용 초기상태/타겟 준비\n",
      "   - masked_mse_loss: 마스킹된 MSE 손실 함수\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비 함수들 - Teacher Forcing & Free Running 지원\n",
    "\n",
    "def tf_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Teacher Forcing용 데이터 준비 (원본 bmed_tf_learning.ipynb와 동일)\n",
    "    \n",
    "    Args:\n",
    "        input_seq: [batch, seq_len, features] - 전체 시퀀스 (10개 특성)\n",
    "        seq_len: [batch] - 각 시퀀스의 실제 길이\n",
    "    \n",
    "    Returns:\n",
    "        inputs: [batch, seq_len-1, 9] - LSTM 입력 (I 제외한 9개 특성)\n",
    "        targets: [batch, seq_len-1, 10] - 예측 타겟 (전체 10개 특성)\n",
    "        target_seq_len: [batch] - 타겟 시퀀스 길이 (seq_len - 1)\n",
    "    \"\"\"\n",
    "    # 입력: 시퀀스의 마지막 시점 제외, I(전류) 특성 제외\n",
    "    inputs = input_seq[:, :-1, :-1]  # [batch, seq_len-1, 9]\n",
    "    \n",
    "    # 타겟: 시퀀스의 첫 번째 시점 제외, 모든 특성 포함\n",
    "    targets = input_seq[:, 1:, :]    # [batch, seq_len-1, 10]\n",
    "    \n",
    "    # 타겟 시퀀스 길이\n",
    "    target_seq_len = seq_len - 1\n",
    "    \n",
    "    return inputs, targets, target_seq_len\n",
    "\n",
    "def fr_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Free Running용 데이터 준비\n",
    "    \n",
    "    Args:\n",
    "        input_seq: [batch, seq_len, features] - 전체 시퀀스 (10개 특성)\n",
    "        seq_len: [batch] - 각 시퀀스의 실제 길이\n",
    "    \n",
    "    Returns:\n",
    "        initial_states: [batch, 10] - 초기 상태 (전체 10개 특성)\n",
    "        targets: [batch, seq_len, 10] - 예측 타겟 (전체 시퀀스)\n",
    "        target_lengths: [batch] - 예측할 시퀀스 길이\n",
    "    \"\"\"\n",
    "    # 초기 상태: 첫 번째 시점\n",
    "    initial_states = input_seq[:, 0, :]  # [batch, 10]\n",
    "    \n",
    "    # 타겟: 전체 시퀀스\n",
    "    targets = input_seq  # [batch, seq_len, 10]\n",
    "    \n",
    "    # 예측할 길이\n",
    "    target_lengths = seq_len  # [batch]\n",
    "    \n",
    "    return initial_states, targets, target_lengths\n",
    "\n",
    "def masked_mse_loss(pred, target, seq_len):\n",
    "    \"\"\"Masked MSE Loss function\"\"\"\n",
    "    batch_size, max_len, features = pred.shape\n",
    "    seq_len_cpu = seq_len.detach().cpu().long()\n",
    "\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "    mask = mask.float().to(pred.device)\n",
    "\n",
    "    loss = F.mse_loss(pred, target, reduction='none')\n",
    "    masked_loss = loss * mask.unsqueeze(-1)\n",
    "\n",
    "    total_loss = masked_loss.sum()\n",
    "    total_elements = mask.sum()\n",
    "\n",
    "    masked_loss = total_loss / total_elements\n",
    "    return masked_loss\n",
    "\n",
    "print(\"✅ 데이터 준비 함수들 구현 완료!\")\n",
    "print(\"   - tf_data: Teacher Forcing용 입력/타겟 준비\")\n",
    "print(\"   - fr_data: Free Running용 초기상태/타겟 준비\") \n",
    "print(\"   - masked_mse_loss: 마스킹된 MSE 손실 함수\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19fb6c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 데이터 로드 중: BMED_DATA_AG.csv\n",
      "   - 실험 수: 15\n",
      "   - 최대 시퀀스 길이: 37\n",
      "🔧 사용 중인 장치: cuda\n",
      "📥 모델 파일 로드 중: BMED_TF_250909.pth\n",
      "✅ 모델 설정 정보 발견:\n",
      "   - State Extractor: input=9, hidden=64, layers=5, dropout=0.1\n",
      "   - Decoder: input=64, hidden=16, layers=2, dropout=0.2\n",
      "   - Current Predictor: input=9, hidden=48, layers=2, dropout=0.3\n",
      "🔧 Range_mm 사용: 현재 데이터 기준 (TF 노트북과 동일 스케일)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 전처리\n",
    "data_path = \"BMED_DATA_AG.csv\"\n",
    "print(f\"📊 데이터 로드 중: {data_path}\")\n",
    "df, ndf, current_range_mm, exp_num_list = df_treat(data_path)\n",
    "\n",
    "# 시퀀스 데이터 생성\n",
    "seq = seq_data(ndf, exp_num_list)\n",
    "pad_sequences, seq_lengths, max_length = pad_seq(seq)\n",
    "dataset = gen_dataset(pad_sequences, seq_lengths)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "print(f\"   - 실험 수: {len(exp_num_list)}\")\n",
    "print(f\"   - 최대 시퀀스 길이: {max_length}\")\n",
    "\n",
    "# 저장된 모델 로드\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 사용 중인 장치: {device}\")\n",
    "\n",
    "# 모델 파일 경로\n",
    "model_path = \"BMED_TF_250909.pth\"\n",
    "\n",
    "# 저장된 모델 체크포인트 로드\n",
    "print(f\"📥 모델 파일 로드 중: {model_path}\")\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# 모델 설정 정보 확인\n",
    "model_config = checkpoint['model_config']\n",
    "state_extr_params = model_config['state_extr_params']\n",
    "decoder_params = model_config['decoder_params']\n",
    "current_predictor_params = model_config['current_predictor_params']\n",
    "checkpoint_range_mm = model_config['range_mm']  # 체크포인트의 range_mm 별도 저장\n",
    "\n",
    "# ✅ 현재 데이터의 range_mm 사용 (체크포인트 range_mm으로 덮어쓰지 않음)\n",
    "range_mm = current_range_mm\n",
    "print(\"✅ 모델 설정 정보 발견:\")\n",
    "print(f\"   - State Extractor: input={state_extr_params['input_node']}, hidden={state_extr_params['hidden_node']}, layers={state_extr_params['n_layer']}, dropout={state_extr_params['dropout']}\")\n",
    "print(f\"   - Decoder: input={decoder_params['input_node']}, hidden={decoder_params['hidden_node']}, layers={decoder_params['n_layer']}, dropout={decoder_params['dropout']}\")\n",
    "print(f\"   - Current Predictor: input={current_predictor_params['input_node']}, hidden={current_predictor_params['hidden_node']}, layers={current_predictor_params['n_layer']}, dropout={current_predictor_params['dropout']}\")\n",
    "print(f\"🔧 Range_mm 사용: 현재 데이터 기준 (TF 노트북과 동일 스케일)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ca6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Pure Free Running Transfer Learning 시작\n",
      "============================================================\n",
      "📦 통합 모델 생성 중...\n",
      "⚡ Pretrained TF 가중치를 Initial Connection Weight으로 로드 중...\n",
      "✅ Pretrained TF 가중치 로드 완료!\n",
      "\n",
      "🔧 Pure Free Running Transfer Learning 설정:\n",
      "   - Transfer Learning 에포크: 1,000\n",
      "   - 학습 방식: Pure Free Running Only (TF 계산 완전 배제)\n",
      "   - Warmup 에포크: 300 (10%)\n",
      "   - Peak Learning Rate: 5.77e-03\n",
      "   - Optimizer: AdamW\n",
      "   - Scheduler: Noam (factor=0.8)\n",
      "   - 배치 크기: 5\n",
      "   - 전체 데이터 사용 (Data split 없음)\n",
      "   - 총 배치 수: 3\n",
      "============================================================\n",
      "🚀 Pure Free Running Transfer Learning 시작...\n",
      "📊 Progress: [Epoch | FR Loss | LR | Status]\n",
      "Epoch     1: FR Loss=0.001907, LR=4.09e-05 ★ BEST FR [WARMUP]\n",
      "          ✓ Best FR Loss: 0.001907 → Saved as BMED_FR_250909.pth\n",
      "Epoch     2: FR Loss=0.001823, LR=8.18e-05 ★ BEST FR [WARMUP]\n",
      "          ✓ Best FR Loss: 0.001823 → Saved as BMED_FR_250909.pth\n",
      "Epoch     3: FR Loss=0.001655, LR=1.23e-04 ★ BEST FR [WARMUP]\n",
      "          ✓ Best FR Loss: 0.001655 → Saved as BMED_FR_250909.pth\n",
      "Epoch     4: FR Loss=0.001506, LR=1.64e-04 ★ BEST FR [WARMUP]\n",
      "          ✓ Best FR Loss: 0.001506 → Saved as BMED_FR_250909.pth\n",
      "Epoch     5: FR Loss=0.001703, LR=2.04e-04 [WARMUP]\n",
      "Epoch     6: FR Loss=0.001400, LR=2.45e-04 ★ BEST FR [WARMUP]\n",
      "          ✓ Best FR Loss: 0.001400 → Saved as BMED_FR_250909.pth\n",
      "Epoch     7: FR Loss=0.001249, LR=2.86e-04 ★ BEST FR [WARMUP]\n",
      "          ✓ Best FR Loss: 0.001249 → Saved as BMED_FR_250909.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    117\u001b[39m fr_loss = pure_free_running_loss(unified_model, fr_initial_states, fr_targets, fr_lengths)\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Gradient 계산 및 업데이트\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mfr_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m torch.nn.utils.clip_grad_norm_(unified_model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    122\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Pure Free Running Transfer Learning - TF 가중치를 초기값으로 활용한 FR 전용 학습\n",
    "\n",
    "print(\"🎯 Pure Free Running Transfer Learning 시작\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 통합 모델 생성 및 pretrained 가중치 로드\n",
    "print(\"📦 통합 모델 생성 중...\")\n",
    "unified_model = BMEDAutoregressiveModel(\n",
    "    state_extr_params=state_extr_params,\n",
    "    decoder_params=decoder_params, \n",
    "    current_predictor_params=current_predictor_params,\n",
    "    range_mm=range_mm  # ✅ 현재 데이터의 range_mm 사용\n",
    ").to(device)\n",
    "\n",
    "# Pretrained 가중치를 Initial Connection Weight으로 로드\n",
    "print(\"⚡ Pretrained TF 가중치를 Initial Connection Weight으로 로드 중...\")\n",
    "unified_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"✅ Pretrained TF 가중치 로드 완료!\")\n",
    "\n",
    "# Pure Free Running Loss 함수 정의 (Teacher Forcing 계산 완전 배제)\n",
    "def pure_free_running_loss(model, initial_states, targets, target_lengths):\n",
    "    \"\"\"Pure Free Running 모드 전용 손실 함수 (TF 계산 없음)\"\"\"\n",
    "    batch_size = initial_states.size(0)\n",
    "    total_loss = 0.0\n",
    "    valid_predictions = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # 개별 시퀀스에 대해 Free Running 예측\n",
    "        single_initial = initial_states[i:i+1, :]  # [1, 10]\n",
    "        single_target = targets[i, :target_lengths[i], :]  # [seq_len, 10]\n",
    "        length = target_lengths[i].item()\n",
    "        \n",
    "        # Free Running 예측만 수행\n",
    "        prediction = model(single_initial, mode='free_running', target_length=length)\n",
    "        prediction = prediction.squeeze(0)  # [seq_len, 10]\n",
    "        \n",
    "        # 손실 계산 (전체 시퀀스)\n",
    "        seq_loss = F.mse_loss(prediction, single_target)\n",
    "        total_loss += seq_loss\n",
    "        valid_predictions += 1\n",
    "    \n",
    "    return total_loss / valid_predictions if valid_predictions > 0 else torch.tensor(0.0)\n",
    "\n",
    "# Noam Scheduler 클래스 정의 (bmed_tf_learning.ipynb와 동일)\n",
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, model_size, warmup_epochs, factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_size = model_size\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.factor = factor\n",
    "        self.epoch_num = 0\n",
    "\n",
    "    def step_epoch(self):\n",
    "        self.epoch_num += 1\n",
    "        lr = self.factor * (\n",
    "            self.model_size ** (-0.5) *\n",
    "            min(self.epoch_num ** (-0.5), self.epoch_num * self.warmup_epochs ** (-1.5))\n",
    "        )\n",
    "    \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "# Pure Free Running Transfer Learning 설정\n",
    "print(\"\\n🔧 Pure Free Running Transfer Learning 설정:\")\n",
    "num_epochs = 1000  # Transfer Learning용 에포크 수\n",
    "print(f\"   - Transfer Learning 에포크: {num_epochs:,}\")\n",
    "print(\"   - 학습 방식: Pure Free Running Only (TF 계산 완전 배제)\")\n",
    "\n",
    "# Teacher Forcing 학습보다 낮은 학습률로 Fine-tuning\n",
    "optimizer = torch.optim.AdamW(unified_model.parameters(), lr=1.0)  \n",
    "warmup_epochs = int(num_epochs * 0.3)  # 10% warmup\n",
    "scheduler = NoamScheduler(optimizer, model_size=state_extr_params['hidden_node'], warmup_epochs=warmup_epochs, factor=1.7)  # factor를 낮춰 학습률 조정\n",
    "\n",
    "peak_lr = 0.8 * (state_extr_params['hidden_node'] ** (-0.5)) * (warmup_epochs ** (-0.5))\n",
    "print(f\"   - Warmup 에포크: {warmup_epochs:,} (10%)\")\n",
    "print(f\"   - Peak Learning Rate: {peak_lr:.2e}\")\n",
    "print(f\"   - Optimizer: AdamW\")\n",
    "print(f\"   - Scheduler: Noam (factor=0.8)\")\n",
    "\n",
    "# 전체 데이터 사용 (data split 없음 - bmed_tf_learning.ipynb와 동일)\n",
    "train_dataloader = DataLoader(dataset, batch_size=5, shuffle=True)  # Shuffle 추가로 학습 효과 증대\n",
    "print(f\"   - 배치 크기: {train_dataloader.batch_size}\")\n",
    "print(f\"   - 전체 데이터 사용 (Data split 없음)\")\n",
    "print(f\"   - 총 배치 수: {len(train_dataloader)}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pure Free Running Transfer Learning 실행\n",
    "best_fr_loss = float('inf')\n",
    "best_epoch = 0\n",
    "train_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "print(\"🚀 Pure Free Running Transfer Learning 시작...\")\n",
    "print(\"📊 Progress: [Epoch | FR Loss | LR | Status]\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_lr = scheduler.step_epoch()\n",
    "    \n",
    "    unified_model.train()\n",
    "    epoch_fr_loss = 0.0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    for batch_idx, (input_seq, seq_len) in enumerate(train_dataloader):\n",
    "        try:\n",
    "            input_seq = input_seq.to(device)\n",
    "            seq_len = seq_len.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Free Running 데이터 준비\n",
    "            fr_initial_states, fr_targets, fr_lengths = fr_data(input_seq, seq_len)\n",
    "            \n",
    "            # Pure Free Running Loss 계산 (TF 계산 완전 배제)\n",
    "            fr_loss = pure_free_running_loss(unified_model, fr_initial_states, fr_targets, fr_lengths)\n",
    "            \n",
    "            # Gradient 계산 및 업데이트\n",
    "            fr_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(unified_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 손실 누적\n",
    "            epoch_fr_loss += fr_loss.item()\n",
    "            valid_batches += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Batch {batch_idx} 오류: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if valid_batches == 0:\n",
    "        print(\"❌ 유효한 배치가 없습니다.\")\n",
    "        break\n",
    "    \n",
    "    # 평균 손실 계산\n",
    "    avg_fr_loss = epoch_fr_loss / valid_batches\n",
    "    \n",
    "    # 기록 저장\n",
    "    train_losses.append(avg_fr_loss)\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Best 모델 확인 (Free Running Loss 기준)\n",
    "    if avg_fr_loss < best_fr_loss:\n",
    "        best_fr_loss = avg_fr_loss\n",
    "        best_epoch = epoch + 1\n",
    "        best_status = \" ★ BEST FR\"\n",
    "        \n",
    "        # Best model 저장 - BMED_FR_{date}.pth 형식\n",
    "        from datetime import datetime\n",
    "        today = datetime.now().strftime(\"%y%m%d\")\n",
    "        model_filename = f\"BMED_FR_{today}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': unified_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': best_epoch,\n",
    "            'best_fr_loss': best_fr_loss,\n",
    "            'model_config': {\n",
    "                'state_extr_params': state_extr_params,\n",
    "                'decoder_params': decoder_params,\n",
    "                'current_predictor_params': current_predictor_params,\n",
    "                'range_mm': range_mm  # ✅ 현재 데이터의 range_mm 저장\n",
    "            },\n",
    "            'pure_fr_transfer_config': {\n",
    "                'base_model': model_path,\n",
    "                'training_mode': 'pure_free_running_only',\n",
    "                'tf_excluded': True,\n",
    "                'num_epochs': num_epochs,\n",
    "                'warmup_epochs': warmup_epochs,\n",
    "                'factor': 0.8\n",
    "            }\n",
    "        }, model_filename)\n",
    "    else:\n",
    "        best_status = \"\"\n",
    "    \n",
    "    # Warmup 상태 표시\n",
    "    warmup_status = \" [WARMUP]\" if epoch + 1 <= warmup_epochs else \"\"\n",
    "    \n",
    "    # 진행 상황 출력 (매 100 에포크 또는 처음 10 에포크)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1:5d}: FR Loss={avg_fr_loss:.6f}, LR={current_lr:.2e}{best_status}{warmup_status}\")\n",
    "        if best_status:\n",
    "            print(f\"          ✓ Best FR Loss: {best_fr_loss:.6f} → Saved as BMED_FR_{today}.pth\")\n",
    "\n",
    "print(f\"\\n🎉 Pure Free Running Transfer Learning 완료!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 최종 결과:\")\n",
    "print(f\"   - Best Epoch: {best_epoch}\")\n",
    "print(f\"   - Best Free Running Loss: {best_fr_loss:.6f}\")\n",
    "print(f\"   - Final FR Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"   - Total Training Batches: {len(train_dataloader) * num_epochs:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 모델 파일 저장 확인\n",
    "today = datetime.now().strftime(\"%y%m%d\")\n",
    "model_filename = f\"BMED_FR_{today}.pth\"\n",
    "print(f\"✅ Pure Free Running 모델 저장: {model_filename}\")\n",
    "print(f\"   - 베이스 모델: {model_path} (TF 가중치)\")\n",
    "print(f\"   - 학습 방식: Pure Free Running Only\")\n",
    "print(f\"   - Teacher Forcing: 완전 배제\")\n",
    "print(f\"   - 초기 가중치: Pretrained TF weights\")\n",
    "print(f\"   - Range_mm: 현재 데이터 기준 (TF 노트북과 동일)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ec3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher Forcing vs Free Running 전체 실험 비교 시각화 - Feature별 Subplot\n",
    "\n",
    "print(\"🎯 Teacher Forcing vs Free Running 전체 실험 비교 시각화\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "unified_model.eval()\n",
    "\n",
    "# ✅ 현재 데이터의 range_mm을 사용하여 비정규화 함수 정의\n",
    "def denormalize_data(normalized_data, feature_name):\n",
    "    \"\"\"현재 데이터 기준 정규화된 데이터를 원래 단위로 복원\"\"\"\n",
    "    if feature_name in range_mm:\n",
    "        min_val = range_mm[feature_name]['min']\n",
    "        max_val = range_mm[feature_name]['max']\n",
    "        return normalized_data * (max_val - min_val) + min_val\n",
    "    return normalized_data\n",
    "\n",
    "# 예측 결과와 실제 값을 저장할 딕셔너리\n",
    "tf_predictions_dict = {}\n",
    "fr_predictions_dict = {}\n",
    "actual_dict = {}\n",
    "\n",
    "# V와 E를 제외한 feature들의 인덱스와 이름\n",
    "feature_names = ['VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "feature_indices = [2, 3, 4, 5, 6, 7, 8, 9]  # V(0), E(1)를 제외한 인덱스\n",
    "\n",
    "# 전체 실험에 대해 예측 수행\n",
    "with torch.no_grad():\n",
    "    for exp_num in exp_num_list:\n",
    "        # 실험 데이터 추출\n",
    "        exp_data = ndf[ndf['exp'] == exp_num].copy()\n",
    "        if len(exp_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 특성 컬럼들\n",
    "        feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "        \n",
    "        # 실험 데이터를 tensor로 변환\n",
    "        exp_tensor = torch.tensor(exp_data[feature_cols].values).float().unsqueeze(0).to(device)  # [1, seq_len, 10]\n",
    "        seq_length = len(exp_data)\n",
    "        seq_len_tensor = torch.tensor([seq_length]).to(device)\n",
    "        \n",
    "        # Teacher Forcing 데이터 준비 및 예측\n",
    "        tf_inputs, tf_targets, tf_seq_len = tf_data(exp_tensor, seq_len_tensor)\n",
    "        tf_pred = unified_model(tf_inputs, tf_seq_len, mode='teacher_forcing')\n",
    "        \n",
    "        # Free Running 데이터 준비 및 예측\n",
    "        fr_initial_states, fr_targets, fr_lengths = fr_data(exp_tensor, seq_len_tensor)\n",
    "        initial_state = fr_initial_states  # [1, 10]\n",
    "        target_length = int(fr_lengths[0].item())\n",
    "        fr_pred = unified_model(initial_state, mode='free_running', target_length=target_length)\n",
    "        \n",
    "        # CPU로 이동하고 numpy 변환\n",
    "        tf_pred_np = tf_pred[0].cpu().numpy()  # [seq_len-1, 10]\n",
    "        fr_pred_np = fr_pred[0].cpu().numpy()  # [seq_len, 10]\n",
    "        actual_tf_np = tf_targets[0].cpu().numpy()  # [seq_len-1, 10] (TF targets)\n",
    "        actual_fr_np = fr_targets[0, :target_length].cpu().numpy()  # [seq_len, 10] (FR targets)\n",
    "        \n",
    "        # 딕셔너리에 저장\n",
    "        tf_predictions_dict[exp_num] = tf_pred_np\n",
    "        fr_predictions_dict[exp_num] = fr_pred_np\n",
    "        actual_dict[exp_num] = {\n",
    "            'tf': actual_tf_np,  # Teacher Forcing 타겟 (시점 1부터)\n",
    "            'fr': actual_fr_np,  # Free Running 타겟 (전체 시퀀스)\n",
    "            'time_tf': exp_data['t'].values[1:len(tf_pred_np)+1],  # TF 시간축\n",
    "            'time_fr': exp_data['t'].values[:len(fr_pred_np)]      # FR 시간축\n",
    "        }\n",
    "\n",
    "print(f\"✅ {len(tf_predictions_dict)}개 실험에 대한 예측 완료\")\n",
    "\n",
    "# V와 E를 제외한 각 feature별로 그래프 생성\n",
    "for feat_idx, feat_name in zip(feature_indices, feature_names):\n",
    "    # subplot 개수 계산 (행과 열 최적화)\n",
    "    n_experiments = len(exp_num_list)\n",
    "    n_cols = min(6, n_experiments)  # 최대 6열\n",
    "    n_rows = (n_experiments + n_cols - 1) // n_cols  # 필요한 행 수\n",
    "    \n",
    "    # 그래프 크기 설정\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*3))\n",
    "    fig.suptitle(f'Feature: {feat_name} - Teacher Forcing vs Free Running vs Actual (Current Data Scale)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # subplot이 1개일 경우 리스트로 변환\n",
    "    if n_experiments == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # 각 실험에 대해 subplot 생성\n",
    "    for i, exp_num in enumerate(exp_num_list):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        \n",
    "        if n_rows > 1:\n",
    "            ax = axes[row, col]\n",
    "        else:\n",
    "            ax = axes[col] if n_cols > 1 else axes[0]\n",
    "        \n",
    "        # 예측값과 실제값 가져오기 (현재 데이터 range_mm으로 비정규화)\n",
    "        if exp_num in tf_predictions_dict:\n",
    "            tf_pred_values = denormalize_data(tf_predictions_dict[exp_num][:, feat_idx], feat_name)\n",
    "            fr_pred_values = denormalize_data(fr_predictions_dict[exp_num][:, feat_idx], feat_name)\n",
    "            tf_actual_values = denormalize_data(actual_dict[exp_num]['tf'][:, feat_idx], feat_name)\n",
    "            fr_actual_values = denormalize_data(actual_dict[exp_num]['fr'][:, feat_idx], feat_name)\n",
    "            tf_time = actual_dict[exp_num]['time_tf'][:len(tf_pred_values)]\n",
    "            fr_time = actual_dict[exp_num]['time_fr'][:len(fr_pred_values)]\n",
    "            \n",
    "            # 그래프 그리기\n",
    "            # 실제값 (Free Running은 전체, Teacher Forcing은 시점 1부터)\n",
    "            ax.plot(fr_time, fr_actual_values, 'k-', linewidth=2, label='Actual', alpha=0.8)\n",
    "            \n",
    "            # Teacher Forcing 예측 (시점 1부터)\n",
    "            ax.plot(tf_time, tf_pred_values, 'b--', linewidth=1.5, label='TF Predicted', alpha=0.7)\n",
    "            \n",
    "            # Free Running 예측 (전체 시퀀스)\n",
    "            ax.plot(fr_time, fr_pred_values, 'r:', linewidth=2, label='FR Predicted', alpha=0.8)\n",
    "            \n",
    "            # 초기값 표시\n",
    "            if len(fr_actual_values) > 0:\n",
    "                ax.plot(fr_time[0], fr_actual_values[0], 'go', markersize=6, label='Initial')\n",
    "            \n",
    "            # 예측 시작점 표시\n",
    "            if len(fr_time) > 1:\n",
    "                ax.axvline(x=fr_time[1], color='gray', linestyle=':', alpha=0.5)\n",
    "            \n",
    "            # 그래프 설정\n",
    "            ax.set_title(f'Exp {exp_num}', fontsize=11)\n",
    "            ax.set_xlabel('Time', fontsize=9)\n",
    "            ax.set_ylabel(feat_name, fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(fontsize=8)\n",
    "            \n",
    "            # MSE 계산 및 표시 (비정규화된 값으로)\n",
    "            tf_mse = np.mean((tf_actual_values - tf_pred_values)**2) if len(tf_pred_values) > 0 else 0\n",
    "            fr_mse = np.mean((fr_actual_values - fr_pred_values)**2) if len(fr_pred_values) > 0 else 0\n",
    "            \n",
    "            # 텍스트 박스로 MSE 표시\n",
    "            textstr = f'TF: {tf_mse:.4f}\\nFR: {fr_mse:.4f}'\n",
    "            ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=8,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "            \n",
    "            # y축 범위 설정\n",
    "            all_values = np.concatenate([tf_actual_values, fr_actual_values, tf_pred_values, fr_pred_values])\n",
    "            y_min, y_max = all_values.min(), all_values.max()\n",
    "            y_range = y_max - y_min\n",
    "            if y_range > 0:\n",
    "                ax.set_ylim(y_min - 0.1*y_range, y_max + 0.1*y_range)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'No data\\nExp {exp_num}', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    # 빈 subplot 제거\n",
    "    for i in range(n_experiments, n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        if n_rows > 1:\n",
    "            axes[row, col].remove()\n",
    "        else:\n",
    "            if n_cols > 1:\n",
    "                axes[col].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"🎉 모든 feature에 대한 Teacher Forcing vs Free Running 비교 그래프 생성 완료!\")\n",
    "print(\"📊 그래프 설명:\")\n",
    "print(\"   - 검은색 실선: 실제값 (Actual)\")\n",
    "print(\"   - 파란색 점선: Teacher Forcing 예측값\")\n",
    "print(\"   - 빨간색 점선: Free Running 예측값\")\n",
    "print(\"   - 초록색 점: 초기값\")\n",
    "print(\"   - 회색 수직선: 예측 시작점\")\n",
    "print(\"   - 각 subplot: 개별 실험 결과\")\n",
    "print(\"   - V, E 제외한 8개 feature 모두 표시\")\n",
    "print(\"✅ 수정 완료: 현재 데이터 기준 range_mm 사용으로 TF 노트북과 동일한 y축 스케일\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
