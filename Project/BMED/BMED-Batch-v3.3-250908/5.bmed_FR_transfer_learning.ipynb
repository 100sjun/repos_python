{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dbd6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ed6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "def df_treat(name):\n",
    "    \"\"\"ë°ì´í„° ì •ê·œí™” ë° ì „ì²˜ë¦¬\"\"\"\n",
    "    df = pd.read_csv(name)\n",
    "    ndf = pd.DataFrame()\n",
    "    range_mm={\n",
    "        'V': {'min':df['V'].min()*0.8, 'max': df['V'].max()*1.2},\n",
    "        'E': {'min':df['E'].min()*0.8, 'max': df['E'].max()*1.2},\n",
    "        'VF': {'min':df['VF'].min()*0.8, 'max': df['VF'].max()*1.2},\n",
    "        'VA': {'min':df['VA'].min()*0.8, 'max': df['VA'].max()*1.2},\n",
    "        'VB': {'min':df['VB'].min()*0.8, 'max': df['VB'].max()*1.2},\n",
    "        'CFLA': {'min':0, 'max': df['CFLA'].max()*1.2},\n",
    "        'CALA': {'min':0, 'max': df['CALA'].max()*1.2},\n",
    "        'CFK': {'min':0, 'max': df['CFK'].max()*1.2},\n",
    "        'CBK': {'min':0, 'max': df['CBK'].max()*1.2},\n",
    "        'I': {'min':0, 'max': df['I'].max()*1.2},\n",
    "    }\n",
    "    ndf['exp'] = df['exp']; ndf['t'] = df['t']\n",
    "\n",
    "    for col in ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']:\n",
    "        if col in range_mm:\n",
    "            ndf[col] = (df[col] - range_mm[col]['min'])/(range_mm[col]['max'] - range_mm[col]['min'])\n",
    "        else:\n",
    "            ndf[col] = df[col]\n",
    "\n",
    "    exp_num_list = sorted(ndf['exp'].unique())\n",
    "    return df, ndf, range_mm, exp_num_list\n",
    "\n",
    "def seq_data(ndf, exp_num_list):\n",
    "    \"\"\"ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\"\"\"\n",
    "    seq = []\n",
    "    feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "    \n",
    "    for exp in exp_num_list:\n",
    "        exp_df = ndf[ndf['exp'] == exp]\n",
    "        seq.append(exp_df[feature_cols].values)\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def pad_seq(seq):\n",
    "    \"\"\"ì‹œí€€ìŠ¤ íŒ¨ë”©\"\"\"\n",
    "    max_len = max([len(s) for s in seq])\n",
    "    seq_len = [len(s) for s in seq]\n",
    "    pad_seq = pad_sequence([torch.tensor(s) for s in seq], batch_first=True, padding_value=-1)\n",
    "    return pad_seq, seq_len, max_len\n",
    "\n",
    "def gen_dataset(pad_seq, seq_len):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    input_tensor = pad_seq.float()\n",
    "    seq_len_tensor = torch.tensor(seq_len)\n",
    "    dataset = TensorDataset(input_tensor, seq_len_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "221f4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ê³¼ ë™ì¼í•œ êµ¬ì¡°)\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    \"\"\"LSTM layer with layer normalization applied to gates\"\"\"\n",
    "    def __init__(self, input_node, hidden_node):\n",
    "        super().__init__()\n",
    "        self.input_node = input_node\n",
    "        self.hidden_node = hidden_node\n",
    "\n",
    "        self.w_i = nn.Linear(input_node, 4 * hidden_node, bias=False)\n",
    "        self.w_h = nn.Linear(hidden_node, 4 * hidden_node, bias=False)\n",
    "\n",
    "        self.ln_i = nn.LayerNorm(hidden_node)\n",
    "        self.ln_h = nn.LayerNorm(hidden_node)\n",
    "        self.ln_g = nn.LayerNorm(hidden_node)\n",
    "        self.ln_o = nn.LayerNorm(hidden_node)\n",
    "\n",
    "        self.ln_c = nn.LayerNorm(hidden_node)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        gi = self.w_i(input)\n",
    "        gh = self.w_h(h_prev)\n",
    "        i_i, i_f, i_g, i_o = gi.chunk(4, dim=-1)\n",
    "        h_i, h_f, h_g, h_o = gh.chunk(4, dim=-1)\n",
    "\n",
    "        i_g = torch.sigmoid(self.ln_i(i_i + h_i))\n",
    "        f_g = torch.sigmoid(self.ln_h(i_f + h_f))\n",
    "        g_g = torch.tanh(self.ln_g(i_g + h_g))\n",
    "        o_g = torch.sigmoid(self.ln_o(i_o + h_o))\n",
    "\n",
    "        c_new = f_g * c_prev + i_g * g_g\n",
    "        c_new = self.ln_c(c_new)\n",
    "\n",
    "        h_new = o_g * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new\n",
    "class StateExtr(nn.Module):\n",
    "    \"\"\"State Extractor using LayerNorm LSTM\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        for _ in range(self.n_layer):\n",
    "            h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "            c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            layer_input = x_t\n",
    "            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "                h_new, c_new = lstm_cell(layer_input, (h_states[layer_idx], c_states[layer_idx]))\n",
    "\n",
    "                h_states[layer_idx] = h_new\n",
    "                c_states[layer_idx] = c_new\n",
    "\n",
    "                if layer_idx < len(self.lstm_cells) - 1:\n",
    "                    layer_input = self.dropout(h_new)\n",
    "                else:\n",
    "                    layer_input = h_new\n",
    "\n",
    "            outputs.append(layer_input)\n",
    "        \n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        normalized = self.final_layer_norm(masked_output)\n",
    "        return self.final_dropout(normalized)\n",
    "\n",
    "# Stateful State Extractor for Free Running Model\n",
    "class StatefulStateExtr(nn.Module):\n",
    "    \"\"\"Hidden stateë¥¼ ìœ ì§€í•˜ë©° sequential ì²˜ë¦¬í•˜ëŠ” State Extractor\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for _ in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(hidden_node)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Hidden statesë¥¼ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ê´€ë¦¬\n",
    "        self.h_states = None\n",
    "        self.c_states = None\n",
    "\n",
    "    def reset_states(self, batch_size=None, device=None):\n",
    "        \"\"\"ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ ì‹œì‘ ì‹œ hidden state ì´ˆê¸°í™”\"\"\"\n",
    "        if batch_size is not None and device is not None:\n",
    "            self.h_states = []\n",
    "            self.c_states = []\n",
    "            for _ in range(self.n_layer):\n",
    "                self.h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "                self.c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "        else:\n",
    "            self.h_states = None\n",
    "            self.c_states = None\n",
    "\n",
    "    def forward_single_timestep(self, x_t):\n",
    "        \"\"\"ë‹¨ì¼ ì‹œì  ì²˜ë¦¬ - hidden state ìœ ì§€\"\"\"\n",
    "        batch_size = x_t.size(0)\n",
    "        device = x_t.device\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œ hidden state ì´ˆê¸°í™”\n",
    "        if self.h_states is None:\n",
    "            self.reset_states(batch_size, device)\n",
    "\n",
    "        layer_input = x_t\n",
    "        for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "            h_new, c_new = lstm_cell(layer_input, (self.h_states[layer_idx], self.c_states[layer_idx]))\n",
    "\n",
    "            # Hidden state ì—…ë°ì´íŠ¸\n",
    "            self.h_states[layer_idx] = h_new\n",
    "            self.c_states[layer_idx] = c_new\n",
    "\n",
    "            if layer_idx < len(self.lstm_cells) - 1:\n",
    "                layer_input = self.dropout(h_new)\n",
    "            else:\n",
    "                layer_input = h_new\n",
    "\n",
    "        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ ì ìš©\n",
    "        normalized = self.final_layer_norm(layer_input)\n",
    "        return self.final_dropout(normalized)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"ê¸°ì¡´ teacher forcing ë°©ì‹ê³¼ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # Teacher forcing ëª¨ë“œì—ì„œëŠ” ë§¤ë²ˆ hidden state ì´ˆê¸°í™”\n",
    "        self.reset_states(batch_size, device)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "            output = self.forward_single_timestep(x_t)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        \n",
    "        # ë§ˆìŠ¤í‚¹ ì ìš©\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        return masked_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a72689bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    \"\"\"Physical Change Decoder\"\"\"\n",
    "    def __init__(self, input_node, output_node, n_layer, hidden_node, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_node, output_node))\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        x = hidden_states\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CurrentPredictor(nn.Module):\n",
    "    \"\"\"Current Predictor\"\"\"\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_node, 1))\n",
    "    \n",
    "    def forward(self, new_state):\n",
    "        x = new_state\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db6a567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraintLayer(nn.Module):\n",
    "    \"\"\"Physics Constraint Layer with Current Prediction\"\"\"\n",
    "    def __init__(self, range_mm, current_predictor, eps=1e-2):\n",
    "        super().__init__()\n",
    "        self.sps = eps\n",
    "        self.current_predictor = current_predictor\n",
    "        self.register_buffer('range_mm_tensor', self._convert_range_to_tensor(range_mm))\n",
    "\n",
    "    def _convert_range_to_tensor(self, range_mm):\n",
    "        feature_names = ['V','E','VF','VA','VB','CFLA','CALA','CFK','CBK','I']\n",
    "        ranges = torch.zeros(len(feature_names),2)\n",
    "\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if name in range_mm:\n",
    "                ranges[i, 0] = range_mm[name]['min']\n",
    "                ranges[i, 1] = range_mm[name]['max']\n",
    "        \n",
    "        return ranges\n",
    "    \n",
    "    def normalize(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "    def denormalize(self, data, feature_idx):\n",
    "        min_val = self.range_mm_tensor[feature_idx, 0]\n",
    "        max_val = self.range_mm_tensor[feature_idx, 1]\n",
    "        return data * (max_val - min_val) + min_val\n",
    "\n",
    "    def forward(self, physical_changes, current_state):\n",
    "        V_idx, E_idx, VF_idx, VA_idx, VB_idx = 0, 1, 2, 3, 4\n",
    "        CFLA_idx, CALA_idx, CFK_idx, CBK_idx, I_idx = 5, 6, 7, 8, 9\n",
    "\n",
    "        VF = self.denormalize(current_state[..., 2:3], VF_idx)\n",
    "        VA = self.denormalize(current_state[..., 3:4], VA_idx)\n",
    "        VB = self.denormalize(current_state[..., 4:5], VB_idx)\n",
    "        CFLA = self.denormalize(current_state[..., 5:6], CFLA_idx)\n",
    "        CALA = self.denormalize(current_state[..., 6:7], CALA_idx)\n",
    "        CFK = self.denormalize(current_state[..., 7:8], CFK_idx)\n",
    "        CBK = self.denormalize(current_state[..., 8:9], CBK_idx)\n",
    "\n",
    "        dVA = physical_changes[..., 0:1]\n",
    "        dVB = physical_changes[..., 1:2]\n",
    "        rratio = physical_changes[..., 2:3]\n",
    "        dNBK = physical_changes[..., 3:4]\n",
    "\n",
    "        ratio = torch.sigmoid(rratio)\n",
    "        dNALA = ratio * dNBK\n",
    "\n",
    "        NFLA = CFLA * VF\n",
    "        NALA = CALA * VA\n",
    "        NFK = CFK * VF\n",
    "        NBK = CBK * VB\n",
    "\n",
    "        nVF = VF - dVA - dVB\n",
    "        nVA = VA + dVA\n",
    "        nVB = VB + dVB\n",
    "\n",
    "        nVF = torch.clamp(nVF, min=self.sps)\n",
    "        nVA = torch.clamp(nVA, min=self.sps)\n",
    "        nVB = torch.clamp(nVB, min=self.sps)\n",
    "        \n",
    "        nNFLA = NFLA - torch.clamp(dNALA, min=0.0)\n",
    "        nNALA = NALA + torch.clamp(dNALA, min=0.0)\n",
    "        nNFK = NFK - torch.clamp(dNBK, min=0.0)\n",
    "        nNBK = NBK + torch.clamp(dNBK, min=0.0)\n",
    "\n",
    "        nNFLA = torch.clamp(nNFLA, min=0.0)\n",
    "        nNALA = torch.clamp(nNALA, min=0.0)\n",
    "        nNFK = torch.clamp(nNFK, min=0.0)\n",
    "        nNBK = torch.clamp(nNBK, min=0.0)\n",
    "\n",
    "        nCFLA = nNFLA / nVF\n",
    "        nCALA = nNALA / nVA\n",
    "        nCFK = nNFK / nVF\n",
    "        nCBK = nNBK / nVB\n",
    "\n",
    "        V = current_state[..., 0:1]\n",
    "        E = current_state[..., 1:2]\n",
    "        nVF_norm = self.normalize(nVF, VF_idx)\n",
    "        nVA_norm = self.normalize(nVA, VA_idx)\n",
    "        nVB_norm = self.normalize(nVB, VB_idx)\n",
    "        nCFLA_norm = self.normalize(nCFLA, CFLA_idx)\n",
    "        nCALA_norm = self.normalize(nCALA, CALA_idx)\n",
    "        nCFK_norm = self.normalize(nCFK, CFK_idx)\n",
    "        nCBK_norm = self.normalize(nCBK, CBK_idx)\n",
    "\n",
    "        temp_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        nI_pred_norm = self.current_predictor(temp_state)\n",
    "        \n",
    "        nI_real = self.denormalize(nI_pred_norm, I_idx)\n",
    "        nI_real = torch.clamp(nI_real, min=0.0)\n",
    "        nI_norm = self.normalize(nI_real, I_idx)\n",
    "\n",
    "        next_state = torch.cat([\n",
    "            V, E, nVF_norm, nVA_norm, nVB_norm, nCFLA_norm, nCALA_norm, nCFK_norm, nCBK_norm, nI_norm\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b90b3815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í†µí•©ëœ BMEDAutoregressiveModel êµ¬í˜„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "class BMEDAutoregressiveModel(nn.Module):\n",
    "    \"\"\"í†µí•© BMED Autoregressive Model - Teacher Forcing & Free Running ëª¨ë“œ ì§€ì›\"\"\"\n",
    "    def __init__(self, state_extr_params, decoder_params, current_predictor_params, range_mm):\n",
    "        super().__init__()\n",
    "        self.state_extr = StateExtr(**state_extr_params)\n",
    "        self.physical_decoder = PhysicalChangeDecoder(**decoder_params)\n",
    "        self.current_predictor = CurrentPredictor(**current_predictor_params)\n",
    "        self.physics_constraint = PhysicsConstraintLayer(range_mm, self.current_predictor)\n",
    "        \n",
    "        # Free runningì„ ìœ„í•œ hidden state ê´€ë¦¬\n",
    "        self._hidden_states = None\n",
    "        self._cell_states = None\n",
    "\n",
    "    def _reset_hidden_states(self, batch_size, device):\n",
    "        \"\"\"Free running ì‹œì‘ ì‹œ hidden state ì´ˆê¸°í™”\"\"\"\n",
    "        self._hidden_states = []\n",
    "        self._cell_states = []\n",
    "        for _ in range(self.state_extr.n_layer):\n",
    "            self._hidden_states.append(torch.zeros(batch_size, self.state_extr.hidden_node, device=device))\n",
    "            self._cell_states.append(torch.zeros(batch_size, self.state_extr.hidden_node, device=device))\n",
    "\n",
    "    def teacher_forcing_forward(self, x, seq_len):\n",
    "        \"\"\"Teacher Forcing ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹\"\"\"\n",
    "        hidden_states = self.state_extr(x, seq_len)\n",
    "        physical_changes = self.physical_decoder(hidden_states)\n",
    "        new_x = self.physics_constraint(physical_changes, x)\n",
    "        return new_x\n",
    "\n",
    "    def free_running_forward(self, initial_state, target_length):\n",
    "        \"\"\"\n",
    "        Free Running ëª¨ë“œ: ì´ˆê¸° ìƒíƒœë§Œìœ¼ë¡œ ì „ì²´ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "        Args:\n",
    "            initial_state: [batch, features] - ì´ˆê¸° ìƒíƒœ (ëª¨ë“  10ê°œ íŠ¹ì„± í¬í•¨)\n",
    "            target_length: int - ì˜ˆì¸¡í•  ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "        Returns:\n",
    "            predictions: [batch, target_length, features] - ì˜ˆì¸¡ëœ ì „ì²´ ì‹œí€€ìŠ¤\n",
    "        \"\"\"\n",
    "        batch_size = initial_state.size(0)\n",
    "        feature_size = initial_state.size(1)\n",
    "        device = initial_state.device\n",
    "        \n",
    "        # Hidden state ì´ˆê¸°í™”\n",
    "        self._reset_hidden_states(batch_size, device)\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "        predictions = torch.zeros(batch_size, target_length, feature_size, device=device)\n",
    "        current_state = initial_state.clone()\n",
    "        \n",
    "        for t in range(target_length):\n",
    "            predictions[:, t, :] = current_state\n",
    "            \n",
    "            if t < target_length - 1:\n",
    "                # í˜„ì¬ ìƒíƒœì—ì„œ I(ì „ë¥˜) ì œê±°í•˜ì—¬ LSTM ì…ë ¥ ìƒì„± (9ê°œ íŠ¹ì„±)\n",
    "                lstm_input = current_state[:, :-1]  # [batch, 9]\n",
    "                \n",
    "                # LSTM forward with hidden state maintenance\n",
    "                hidden_output = self._forward_lstm_single_step(lstm_input)\n",
    "                \n",
    "                # Physical change ì˜ˆì¸¡\n",
    "                physical_changes = self.physical_decoder(hidden_output.unsqueeze(1))  # [batch, 1, output]\n",
    "                \n",
    "                # Physics constraintë¡œ ë‹¤ìŒ ìƒíƒœ ê³„ì‚°\n",
    "                current_state_expanded = current_state.unsqueeze(1)  # [batch, 1, features]\n",
    "                next_state = self.physics_constraint(physical_changes, current_state_expanded)\n",
    "                current_state = next_state.squeeze(1)  # [batch, features]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def _forward_lstm_single_step(self, x_t):\n",
    "        \"\"\"ë‹¨ì¼ ì‹œì  LSTM forward (hidden state ìœ ì§€)\"\"\"\n",
    "        layer_input = x_t\n",
    "        \n",
    "        for layer_idx, lstm_cell in enumerate(self.state_extr.lstm_cells):\n",
    "            h_new, c_new = lstm_cell(layer_input, \n",
    "                                   (self._hidden_states[layer_idx], self._cell_states[layer_idx]))\n",
    "            \n",
    "            # Hidden state ì—…ë°ì´íŠ¸\n",
    "            self._hidden_states[layer_idx] = h_new\n",
    "            self._cell_states[layer_idx] = c_new\n",
    "            \n",
    "            # Dropout ì ìš© (ë§ˆì§€ë§‰ layer ì œì™¸)\n",
    "            if layer_idx < len(self.state_extr.lstm_cells) - 1:\n",
    "                layer_input = self.state_extr.dropout(h_new)\n",
    "            else:\n",
    "                layer_input = h_new\n",
    "        \n",
    "        # ìµœì¢… ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ\n",
    "        normalized = self.state_extr.final_layer_norm(layer_input)\n",
    "        return self.state_extr.final_dropout(normalized)\n",
    "\n",
    "    def forward(self, x, seq_len=None, mode='teacher_forcing', target_length=None):\n",
    "        \"\"\"\n",
    "        í†µí•© forward ë©”ì†Œë“œ\n",
    "        Args:\n",
    "            x: ì…ë ¥ ë°ì´í„°\n",
    "            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´ (teacher forcing ëª¨ë“œì—ì„œ ì‚¬ìš©)\n",
    "            mode: 'teacher_forcing' ë˜ëŠ” 'free_running'\n",
    "            target_length: free runningì—ì„œ ì˜ˆì¸¡í•  ê¸¸ì´\n",
    "        \"\"\"\n",
    "        if mode == 'teacher_forcing':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_lenì€ teacher_forcing ëª¨ë“œì—ì„œ í•„ìˆ˜ì…ë‹ˆë‹¤\")\n",
    "            return self.teacher_forcing_forward(x, seq_len)\n",
    "        \n",
    "        elif mode == 'free_running':\n",
    "            if target_length is None:\n",
    "                raise ValueError(\"target_lengthëŠ” free_running ëª¨ë“œì—ì„œ í•„ìˆ˜ì…ë‹ˆë‹¤\")\n",
    "            return self.free_running_forward(x, target_length)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {mode}. 'teacher_forcing' ë˜ëŠ” 'free_running'ì„ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "\n",
    "print(\"âœ… í†µí•©ëœ BMEDAutoregressiveModel êµ¬í˜„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17822324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ë“¤ êµ¬í˜„ ì™„ë£Œ!\n",
      "   - tf_data: Teacher Forcingìš© ì…ë ¥/íƒ€ê²Ÿ ì¤€ë¹„\n",
      "   - fr_data: Free Runningìš© ì´ˆê¸°ìƒíƒœ/íƒ€ê²Ÿ ì¤€ë¹„\n",
      "   - masked_mse_loss: ë§ˆìŠ¤í‚¹ëœ MSE ì†ì‹¤ í•¨ìˆ˜\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ë“¤ - Teacher Forcing & Free Running ì§€ì›\n",
    "\n",
    "def tf_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Teacher Forcingìš© ë°ì´í„° ì¤€ë¹„ (ì›ë³¸ bmed_tf_learning.ipynbì™€ ë™ì¼)\n",
    "    \n",
    "    Args:\n",
    "        input_seq: [batch, seq_len, features] - ì „ì²´ ì‹œí€€ìŠ¤ (10ê°œ íŠ¹ì„±)\n",
    "        seq_len: [batch] - ê° ì‹œí€€ìŠ¤ì˜ ì‹¤ì œ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        inputs: [batch, seq_len-1, 9] - LSTM ì…ë ¥ (I ì œì™¸í•œ 9ê°œ íŠ¹ì„±)\n",
    "        targets: [batch, seq_len-1, 10] - ì˜ˆì¸¡ íƒ€ê²Ÿ (ì „ì²´ 10ê°œ íŠ¹ì„±)\n",
    "        target_seq_len: [batch] - íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ê¸¸ì´ (seq_len - 1)\n",
    "    \"\"\"\n",
    "    # ì…ë ¥: ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹œì  ì œì™¸, I(ì „ë¥˜) íŠ¹ì„± ì œì™¸\n",
    "    inputs = input_seq[:, :-1, :-1]  # [batch, seq_len-1, 9]\n",
    "    \n",
    "    # íƒ€ê²Ÿ: ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ ì‹œì  ì œì™¸, ëª¨ë“  íŠ¹ì„± í¬í•¨\n",
    "    targets = input_seq[:, 1:, :]    # [batch, seq_len-1, 10]\n",
    "    \n",
    "    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    target_seq_len = seq_len - 1\n",
    "    \n",
    "    return inputs, targets, target_seq_len\n",
    "\n",
    "def fr_data(input_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Free Runningìš© ë°ì´í„° ì¤€ë¹„\n",
    "    \n",
    "    Args:\n",
    "        input_seq: [batch, seq_len, features] - ì „ì²´ ì‹œí€€ìŠ¤ (10ê°œ íŠ¹ì„±)\n",
    "        seq_len: [batch] - ê° ì‹œí€€ìŠ¤ì˜ ì‹¤ì œ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        initial_states: [batch, 10] - ì´ˆê¸° ìƒíƒœ (ì „ì²´ 10ê°œ íŠ¹ì„±)\n",
    "        targets: [batch, seq_len, 10] - ì˜ˆì¸¡ íƒ€ê²Ÿ (ì „ì²´ ì‹œí€€ìŠ¤)\n",
    "        target_lengths: [batch] - ì˜ˆì¸¡í•  ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \"\"\"\n",
    "    # ì´ˆê¸° ìƒíƒœ: ì²« ë²ˆì§¸ ì‹œì \n",
    "    initial_states = input_seq[:, 0, :]  # [batch, 10]\n",
    "    \n",
    "    # íƒ€ê²Ÿ: ì „ì²´ ì‹œí€€ìŠ¤\n",
    "    targets = input_seq  # [batch, seq_len, 10]\n",
    "    \n",
    "    # ì˜ˆì¸¡í•  ê¸¸ì´\n",
    "    target_lengths = seq_len  # [batch]\n",
    "    \n",
    "    return initial_states, targets, target_lengths\n",
    "\n",
    "def masked_mse_loss(pred, target, seq_len):\n",
    "    \"\"\"Masked MSE Loss function\"\"\"\n",
    "    batch_size, max_len, features = pred.shape\n",
    "    seq_len_cpu = seq_len.detach().cpu().long()\n",
    "\n",
    "    mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "    mask = mask.float().to(pred.device)\n",
    "\n",
    "    loss = F.mse_loss(pred, target, reduction='none')\n",
    "    masked_loss = loss * mask.unsqueeze(-1)\n",
    "\n",
    "    total_loss = masked_loss.sum()\n",
    "    total_elements = mask.sum()\n",
    "\n",
    "    masked_loss = total_loss / total_elements\n",
    "    return masked_loss\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ë“¤ êµ¬í˜„ ì™„ë£Œ!\")\n",
    "print(\"   - tf_data: Teacher Forcingìš© ì…ë ¥/íƒ€ê²Ÿ ì¤€ë¹„\")\n",
    "print(\"   - fr_data: Free Runningìš© ì´ˆê¸°ìƒíƒœ/íƒ€ê²Ÿ ì¤€ë¹„\") \n",
    "print(\"   - masked_mse_loss: ë§ˆìŠ¤í‚¹ëœ MSE ì†ì‹¤ í•¨ìˆ˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19fb6c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘: BMED_DATA_AG.csv\n",
      "   - ì‹¤í—˜ ìˆ˜: 15\n",
      "   - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: 37\n",
      "ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: cuda\n",
      "ğŸ“¥ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘: BMED_TF_250909.pth\n",
      "âœ… ëª¨ë¸ ì„¤ì • ì •ë³´ ë°œê²¬:\n",
      "   - State Extractor: input=9, hidden=64, layers=5, dropout=0.1\n",
      "   - Decoder: input=64, hidden=16, layers=2, dropout=0.2\n",
      "   - Current Predictor: input=9, hidden=48, layers=2, dropout=0.3\n",
      "ğŸ”§ Range_mm ì‚¬ìš©: í˜„ì¬ ë°ì´í„° ê¸°ì¤€ (TF ë…¸íŠ¸ë¶ê³¼ ë™ì¼ ìŠ¤ì¼€ì¼)\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "data_path = \"BMED_DATA_AG.csv\"\n",
    "print(f\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}\")\n",
    "df, ndf, current_range_mm, exp_num_list = df_treat(data_path)\n",
    "\n",
    "# ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\n",
    "seq = seq_data(ndf, exp_num_list)\n",
    "pad_sequences, seq_lengths, max_length = pad_seq(seq)\n",
    "dataset = gen_dataset(pad_sequences, seq_lengths)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "print(f\"   - ì‹¤í—˜ ìˆ˜: {len(exp_num_list)}\")\n",
    "print(f\"   - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_length}\")\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}\")\n",
    "\n",
    "# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ\n",
    "model_path = \"BMED_TF_250909.pth\"\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n",
    "print(f\"ğŸ“¥ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘: {model_path}\")\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì • ì •ë³´ í™•ì¸\n",
    "model_config = checkpoint['model_config']\n",
    "state_extr_params = model_config['state_extr_params']\n",
    "decoder_params = model_config['decoder_params']\n",
    "current_predictor_params = model_config['current_predictor_params']\n",
    "checkpoint_range_mm = model_config['range_mm']  # ì²´í¬í¬ì¸íŠ¸ì˜ range_mm ë³„ë„ ì €ì¥\n",
    "\n",
    "# âœ… í˜„ì¬ ë°ì´í„°ì˜ range_mm ì‚¬ìš© (ì²´í¬í¬ì¸íŠ¸ range_mmìœ¼ë¡œ ë®ì–´ì“°ì§€ ì•ŠìŒ)\n",
    "range_mm = current_range_mm\n",
    "print(\"âœ… ëª¨ë¸ ì„¤ì • ì •ë³´ ë°œê²¬:\")\n",
    "print(f\"   - State Extractor: input={state_extr_params['input_node']}, hidden={state_extr_params['hidden_node']}, layers={state_extr_params['n_layer']}, dropout={state_extr_params['dropout']}\")\n",
    "print(f\"   - Decoder: input={decoder_params['input_node']}, hidden={decoder_params['hidden_node']}, layers={decoder_params['n_layer']}, dropout={decoder_params['dropout']}\")\n",
    "print(f\"   - Current Predictor: input={current_predictor_params['input_node']}, hidden={current_predictor_params['hidden_node']}, layers={current_predictor_params['n_layer']}, dropout={current_predictor_params['dropout']}\")\n",
    "print(f\"ğŸ”§ Range_mm ì‚¬ìš©: í˜„ì¬ ë°ì´í„° ê¸°ì¤€ (TF ë…¸íŠ¸ë¶ê³¼ ë™ì¼ ìŠ¤ì¼€ì¼)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ca6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Pure Free Running Transfer Learning ì‹œì‘\n",
      "============================================================\n",
      "ğŸ“¦ í†µí•© ëª¨ë¸ ìƒì„± ì¤‘...\n",
      "âš¡ Pretrained TF ê°€ì¤‘ì¹˜ë¥¼ Initial Connection Weightìœ¼ë¡œ ë¡œë“œ ì¤‘...\n",
      "âœ… Pretrained TF ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ!\n",
      "\n",
      "ğŸ”§ Pure Free Running Transfer Learning ì„¤ì •:\n",
      "   - Transfer Learning ì—í¬í¬: 1,000\n",
      "   - í•™ìŠµ ë°©ì‹: Pure Free Running Only (TF ê³„ì‚° ì™„ì „ ë°°ì œ)\n",
      "   - Warmup ì—í¬í¬: 300 (10%)\n",
      "   - Peak Learning Rate: 5.77e-03\n",
      "   - Optimizer: AdamW\n",
      "   - Scheduler: Noam (factor=0.8)\n",
      "   - ë°°ì¹˜ í¬ê¸°: 5\n",
      "   - ì „ì²´ ë°ì´í„° ì‚¬ìš© (Data split ì—†ìŒ)\n",
      "   - ì´ ë°°ì¹˜ ìˆ˜: 3\n",
      "============================================================\n",
      "ğŸš€ Pure Free Running Transfer Learning ì‹œì‘...\n",
      "ğŸ“Š Progress: [Epoch | FR Loss | LR | Status]\n",
      "Epoch     1: FR Loss=0.001907, LR=4.09e-05 â˜… BEST FR [WARMUP]\n",
      "          âœ“ Best FR Loss: 0.001907 â†’ Saved as BMED_FR_250909.pth\n",
      "Epoch     2: FR Loss=0.001823, LR=8.18e-05 â˜… BEST FR [WARMUP]\n",
      "          âœ“ Best FR Loss: 0.001823 â†’ Saved as BMED_FR_250909.pth\n",
      "Epoch     3: FR Loss=0.001655, LR=1.23e-04 â˜… BEST FR [WARMUP]\n",
      "          âœ“ Best FR Loss: 0.001655 â†’ Saved as BMED_FR_250909.pth\n",
      "Epoch     4: FR Loss=0.001506, LR=1.64e-04 â˜… BEST FR [WARMUP]\n",
      "          âœ“ Best FR Loss: 0.001506 â†’ Saved as BMED_FR_250909.pth\n",
      "Epoch     5: FR Loss=0.001703, LR=2.04e-04 [WARMUP]\n",
      "Epoch     6: FR Loss=0.001400, LR=2.45e-04 â˜… BEST FR [WARMUP]\n",
      "          âœ“ Best FR Loss: 0.001400 â†’ Saved as BMED_FR_250909.pth\n",
      "Epoch     7: FR Loss=0.001249, LR=2.86e-04 â˜… BEST FR [WARMUP]\n",
      "          âœ“ Best FR Loss: 0.001249 â†’ Saved as BMED_FR_250909.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    117\u001b[39m fr_loss = pure_free_running_loss(unified_model, fr_initial_states, fr_targets, fr_lengths)\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Gradient ê³„ì‚° ë° ì—…ë°ì´íŠ¸\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mfr_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m torch.nn.utils.clip_grad_norm_(unified_model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    122\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Pure Free Running Transfer Learning - TF ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°ê°’ìœ¼ë¡œ í™œìš©í•œ FR ì „ìš© í•™ìŠµ\n",
    "\n",
    "print(\"ğŸ¯ Pure Free Running Transfer Learning ì‹œì‘\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# í†µí•© ëª¨ë¸ ìƒì„± ë° pretrained ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "print(\"ğŸ“¦ í†µí•© ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "unified_model = BMEDAutoregressiveModel(\n",
    "    state_extr_params=state_extr_params,\n",
    "    decoder_params=decoder_params, \n",
    "    current_predictor_params=current_predictor_params,\n",
    "    range_mm=range_mm  # âœ… í˜„ì¬ ë°ì´í„°ì˜ range_mm ì‚¬ìš©\n",
    ").to(device)\n",
    "\n",
    "# Pretrained ê°€ì¤‘ì¹˜ë¥¼ Initial Connection Weightìœ¼ë¡œ ë¡œë“œ\n",
    "print(\"âš¡ Pretrained TF ê°€ì¤‘ì¹˜ë¥¼ Initial Connection Weightìœ¼ë¡œ ë¡œë“œ ì¤‘...\")\n",
    "unified_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"âœ… Pretrained TF ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# Pure Free Running Loss í•¨ìˆ˜ ì •ì˜ (Teacher Forcing ê³„ì‚° ì™„ì „ ë°°ì œ)\n",
    "def pure_free_running_loss(model, initial_states, targets, target_lengths):\n",
    "    \"\"\"Pure Free Running ëª¨ë“œ ì „ìš© ì†ì‹¤ í•¨ìˆ˜ (TF ê³„ì‚° ì—†ìŒ)\"\"\"\n",
    "    batch_size = initial_states.size(0)\n",
    "    total_loss = 0.0\n",
    "    valid_predictions = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # ê°œë³„ ì‹œí€€ìŠ¤ì— ëŒ€í•´ Free Running ì˜ˆì¸¡\n",
    "        single_initial = initial_states[i:i+1, :]  # [1, 10]\n",
    "        single_target = targets[i, :target_lengths[i], :]  # [seq_len, 10]\n",
    "        length = target_lengths[i].item()\n",
    "        \n",
    "        # Free Running ì˜ˆì¸¡ë§Œ ìˆ˜í–‰\n",
    "        prediction = model(single_initial, mode='free_running', target_length=length)\n",
    "        prediction = prediction.squeeze(0)  # [seq_len, 10]\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚° (ì „ì²´ ì‹œí€€ìŠ¤)\n",
    "        seq_loss = F.mse_loss(prediction, single_target)\n",
    "        total_loss += seq_loss\n",
    "        valid_predictions += 1\n",
    "    \n",
    "    return total_loss / valid_predictions if valid_predictions > 0 else torch.tensor(0.0)\n",
    "\n",
    "# Noam Scheduler í´ë˜ìŠ¤ ì •ì˜ (bmed_tf_learning.ipynbì™€ ë™ì¼)\n",
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, model_size, warmup_epochs, factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_size = model_size\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.factor = factor\n",
    "        self.epoch_num = 0\n",
    "\n",
    "    def step_epoch(self):\n",
    "        self.epoch_num += 1\n",
    "        lr = self.factor * (\n",
    "            self.model_size ** (-0.5) *\n",
    "            min(self.epoch_num ** (-0.5), self.epoch_num * self.warmup_epochs ** (-1.5))\n",
    "        )\n",
    "    \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "# Pure Free Running Transfer Learning ì„¤ì •\n",
    "print(\"\\nğŸ”§ Pure Free Running Transfer Learning ì„¤ì •:\")\n",
    "num_epochs = 1000  # Transfer Learningìš© ì—í¬í¬ ìˆ˜\n",
    "print(f\"   - Transfer Learning ì—í¬í¬: {num_epochs:,}\")\n",
    "print(\"   - í•™ìŠµ ë°©ì‹: Pure Free Running Only (TF ê³„ì‚° ì™„ì „ ë°°ì œ)\")\n",
    "\n",
    "# Teacher Forcing í•™ìŠµë³´ë‹¤ ë‚®ì€ í•™ìŠµë¥ ë¡œ Fine-tuning\n",
    "optimizer = torch.optim.AdamW(unified_model.parameters(), lr=1.0)  \n",
    "warmup_epochs = int(num_epochs * 0.3)  # 10% warmup\n",
    "scheduler = NoamScheduler(optimizer, model_size=state_extr_params['hidden_node'], warmup_epochs=warmup_epochs, factor=1.7)  # factorë¥¼ ë‚®ì¶° í•™ìŠµë¥  ì¡°ì •\n",
    "\n",
    "peak_lr = 0.8 * (state_extr_params['hidden_node'] ** (-0.5)) * (warmup_epochs ** (-0.5))\n",
    "print(f\"   - Warmup ì—í¬í¬: {warmup_epochs:,} (10%)\")\n",
    "print(f\"   - Peak Learning Rate: {peak_lr:.2e}\")\n",
    "print(f\"   - Optimizer: AdamW\")\n",
    "print(f\"   - Scheduler: Noam (factor=0.8)\")\n",
    "\n",
    "# ì „ì²´ ë°ì´í„° ì‚¬ìš© (data split ì—†ìŒ - bmed_tf_learning.ipynbì™€ ë™ì¼)\n",
    "train_dataloader = DataLoader(dataset, batch_size=5, shuffle=True)  # Shuffle ì¶”ê°€ë¡œ í•™ìŠµ íš¨ê³¼ ì¦ëŒ€\n",
    "print(f\"   - ë°°ì¹˜ í¬ê¸°: {train_dataloader.batch_size}\")\n",
    "print(f\"   - ì „ì²´ ë°ì´í„° ì‚¬ìš© (Data split ì—†ìŒ)\")\n",
    "print(f\"   - ì´ ë°°ì¹˜ ìˆ˜: {len(train_dataloader)}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pure Free Running Transfer Learning ì‹¤í–‰\n",
    "best_fr_loss = float('inf')\n",
    "best_epoch = 0\n",
    "train_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "print(\"ğŸš€ Pure Free Running Transfer Learning ì‹œì‘...\")\n",
    "print(\"ğŸ“Š Progress: [Epoch | FR Loss | LR | Status]\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_lr = scheduler.step_epoch()\n",
    "    \n",
    "    unified_model.train()\n",
    "    epoch_fr_loss = 0.0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    for batch_idx, (input_seq, seq_len) in enumerate(train_dataloader):\n",
    "        try:\n",
    "            input_seq = input_seq.to(device)\n",
    "            seq_len = seq_len.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Free Running ë°ì´í„° ì¤€ë¹„\n",
    "            fr_initial_states, fr_targets, fr_lengths = fr_data(input_seq, seq_len)\n",
    "            \n",
    "            # Pure Free Running Loss ê³„ì‚° (TF ê³„ì‚° ì™„ì „ ë°°ì œ)\n",
    "            fr_loss = pure_free_running_loss(unified_model, fr_initial_states, fr_targets, fr_lengths)\n",
    "            \n",
    "            # Gradient ê³„ì‚° ë° ì—…ë°ì´íŠ¸\n",
    "            fr_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(unified_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # ì†ì‹¤ ëˆ„ì \n",
    "            epoch_fr_loss += fr_loss.item()\n",
    "            valid_batches += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Batch {batch_idx} ì˜¤ë¥˜: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if valid_batches == 0:\n",
    "        print(\"âŒ ìœ íš¨í•œ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        break\n",
    "    \n",
    "    # í‰ê·  ì†ì‹¤ ê³„ì‚°\n",
    "    avg_fr_loss = epoch_fr_loss / valid_batches\n",
    "    \n",
    "    # ê¸°ë¡ ì €ì¥\n",
    "    train_losses.append(avg_fr_loss)\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Best ëª¨ë¸ í™•ì¸ (Free Running Loss ê¸°ì¤€)\n",
    "    if avg_fr_loss < best_fr_loss:\n",
    "        best_fr_loss = avg_fr_loss\n",
    "        best_epoch = epoch + 1\n",
    "        best_status = \" â˜… BEST FR\"\n",
    "        \n",
    "        # Best model ì €ì¥ - BMED_FR_{date}.pth í˜•ì‹\n",
    "        from datetime import datetime\n",
    "        today = datetime.now().strftime(\"%y%m%d\")\n",
    "        model_filename = f\"BMED_FR_{today}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': unified_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': best_epoch,\n",
    "            'best_fr_loss': best_fr_loss,\n",
    "            'model_config': {\n",
    "                'state_extr_params': state_extr_params,\n",
    "                'decoder_params': decoder_params,\n",
    "                'current_predictor_params': current_predictor_params,\n",
    "                'range_mm': range_mm  # âœ… í˜„ì¬ ë°ì´í„°ì˜ range_mm ì €ì¥\n",
    "            },\n",
    "            'pure_fr_transfer_config': {\n",
    "                'base_model': model_path,\n",
    "                'training_mode': 'pure_free_running_only',\n",
    "                'tf_excluded': True,\n",
    "                'num_epochs': num_epochs,\n",
    "                'warmup_epochs': warmup_epochs,\n",
    "                'factor': 0.8\n",
    "            }\n",
    "        }, model_filename)\n",
    "    else:\n",
    "        best_status = \"\"\n",
    "    \n",
    "    # Warmup ìƒíƒœ í‘œì‹œ\n",
    "    warmup_status = \" [WARMUP]\" if epoch + 1 <= warmup_epochs else \"\"\n",
    "    \n",
    "    # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 100 ì—í¬í¬ ë˜ëŠ” ì²˜ìŒ 10 ì—í¬í¬)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1:5d}: FR Loss={avg_fr_loss:.6f}, LR={current_lr:.2e}{best_status}{warmup_status}\")\n",
    "        if best_status:\n",
    "            print(f\"          âœ“ Best FR Loss: {best_fr_loss:.6f} â†’ Saved as BMED_FR_{today}.pth\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Pure Free Running Transfer Learning ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“Š ìµœì¢… ê²°ê³¼:\")\n",
    "print(f\"   - Best Epoch: {best_epoch}\")\n",
    "print(f\"   - Best Free Running Loss: {best_fr_loss:.6f}\")\n",
    "print(f\"   - Final FR Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"   - Total Training Batches: {len(train_dataloader) * num_epochs:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ëª¨ë¸ íŒŒì¼ ì €ì¥ í™•ì¸\n",
    "today = datetime.now().strftime(\"%y%m%d\")\n",
    "model_filename = f\"BMED_FR_{today}.pth\"\n",
    "print(f\"âœ… Pure Free Running ëª¨ë¸ ì €ì¥: {model_filename}\")\n",
    "print(f\"   - ë² ì´ìŠ¤ ëª¨ë¸: {model_path} (TF ê°€ì¤‘ì¹˜)\")\n",
    "print(f\"   - í•™ìŠµ ë°©ì‹: Pure Free Running Only\")\n",
    "print(f\"   - Teacher Forcing: ì™„ì „ ë°°ì œ\")\n",
    "print(f\"   - ì´ˆê¸° ê°€ì¤‘ì¹˜: Pretrained TF weights\")\n",
    "print(f\"   - Range_mm: í˜„ì¬ ë°ì´í„° ê¸°ì¤€ (TF ë…¸íŠ¸ë¶ê³¼ ë™ì¼)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ec3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher Forcing vs Free Running ì „ì²´ ì‹¤í—˜ ë¹„êµ ì‹œê°í™” - Featureë³„ Subplot\n",
    "\n",
    "print(\"ğŸ¯ Teacher Forcing vs Free Running ì „ì²´ ì‹¤í—˜ ë¹„êµ ì‹œê°í™”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "unified_model.eval()\n",
    "\n",
    "# âœ… í˜„ì¬ ë°ì´í„°ì˜ range_mmì„ ì‚¬ìš©í•˜ì—¬ ë¹„ì •ê·œí™” í•¨ìˆ˜ ì •ì˜\n",
    "def denormalize_data(normalized_data, feature_name):\n",
    "    \"\"\"í˜„ì¬ ë°ì´í„° ê¸°ì¤€ ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì›ë˜ ë‹¨ìœ„ë¡œ ë³µì›\"\"\"\n",
    "    if feature_name in range_mm:\n",
    "        min_val = range_mm[feature_name]['min']\n",
    "        max_val = range_mm[feature_name]['max']\n",
    "        return normalized_data * (max_val - min_val) + min_val\n",
    "    return normalized_data\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "tf_predictions_dict = {}\n",
    "fr_predictions_dict = {}\n",
    "actual_dict = {}\n",
    "\n",
    "# Vì™€ Eë¥¼ ì œì™¸í•œ featureë“¤ì˜ ì¸ë±ìŠ¤ì™€ ì´ë¦„\n",
    "feature_names = ['VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "feature_indices = [2, 3, 4, 5, 6, 7, 8, 9]  # V(0), E(1)ë¥¼ ì œì™¸í•œ ì¸ë±ìŠ¤\n",
    "\n",
    "# ì „ì²´ ì‹¤í—˜ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "with torch.no_grad():\n",
    "    for exp_num in exp_num_list:\n",
    "        # ì‹¤í—˜ ë°ì´í„° ì¶”ì¶œ\n",
    "        exp_data = ndf[ndf['exp'] == exp_num].copy()\n",
    "        if len(exp_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # íŠ¹ì„± ì»¬ëŸ¼ë“¤\n",
    "        feature_cols = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK', 'I']\n",
    "        \n",
    "        # ì‹¤í—˜ ë°ì´í„°ë¥¼ tensorë¡œ ë³€í™˜\n",
    "        exp_tensor = torch.tensor(exp_data[feature_cols].values).float().unsqueeze(0).to(device)  # [1, seq_len, 10]\n",
    "        seq_length = len(exp_data)\n",
    "        seq_len_tensor = torch.tensor([seq_length]).to(device)\n",
    "        \n",
    "        # Teacher Forcing ë°ì´í„° ì¤€ë¹„ ë° ì˜ˆì¸¡\n",
    "        tf_inputs, tf_targets, tf_seq_len = tf_data(exp_tensor, seq_len_tensor)\n",
    "        tf_pred = unified_model(tf_inputs, tf_seq_len, mode='teacher_forcing')\n",
    "        \n",
    "        # Free Running ë°ì´í„° ì¤€ë¹„ ë° ì˜ˆì¸¡\n",
    "        fr_initial_states, fr_targets, fr_lengths = fr_data(exp_tensor, seq_len_tensor)\n",
    "        initial_state = fr_initial_states  # [1, 10]\n",
    "        target_length = int(fr_lengths[0].item())\n",
    "        fr_pred = unified_model(initial_state, mode='free_running', target_length=target_length)\n",
    "        \n",
    "        # CPUë¡œ ì´ë™í•˜ê³  numpy ë³€í™˜\n",
    "        tf_pred_np = tf_pred[0].cpu().numpy()  # [seq_len-1, 10]\n",
    "        fr_pred_np = fr_pred[0].cpu().numpy()  # [seq_len, 10]\n",
    "        actual_tf_np = tf_targets[0].cpu().numpy()  # [seq_len-1, 10] (TF targets)\n",
    "        actual_fr_np = fr_targets[0, :target_length].cpu().numpy()  # [seq_len, 10] (FR targets)\n",
    "        \n",
    "        # ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "        tf_predictions_dict[exp_num] = tf_pred_np\n",
    "        fr_predictions_dict[exp_num] = fr_pred_np\n",
    "        actual_dict[exp_num] = {\n",
    "            'tf': actual_tf_np,  # Teacher Forcing íƒ€ê²Ÿ (ì‹œì  1ë¶€í„°)\n",
    "            'fr': actual_fr_np,  # Free Running íƒ€ê²Ÿ (ì „ì²´ ì‹œí€€ìŠ¤)\n",
    "            'time_tf': exp_data['t'].values[1:len(tf_pred_np)+1],  # TF ì‹œê°„ì¶•\n",
    "            'time_fr': exp_data['t'].values[:len(fr_pred_np)]      # FR ì‹œê°„ì¶•\n",
    "        }\n",
    "\n",
    "print(f\"âœ… {len(tf_predictions_dict)}ê°œ ì‹¤í—˜ì— ëŒ€í•œ ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "\n",
    "# Vì™€ Eë¥¼ ì œì™¸í•œ ê° featureë³„ë¡œ ê·¸ë˜í”„ ìƒì„±\n",
    "for feat_idx, feat_name in zip(feature_indices, feature_names):\n",
    "    # subplot ê°œìˆ˜ ê³„ì‚° (í–‰ê³¼ ì—´ ìµœì í™”)\n",
    "    n_experiments = len(exp_num_list)\n",
    "    n_cols = min(6, n_experiments)  # ìµœëŒ€ 6ì—´\n",
    "    n_rows = (n_experiments + n_cols - 1) // n_cols  # í•„ìš”í•œ í–‰ ìˆ˜\n",
    "    \n",
    "    # ê·¸ë˜í”„ í¬ê¸° ì„¤ì •\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*3))\n",
    "    fig.suptitle(f'Feature: {feat_name} - Teacher Forcing vs Free Running vs Actual (Current Data Scale)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # subplotì´ 1ê°œì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    if n_experiments == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # ê° ì‹¤í—˜ì— ëŒ€í•´ subplot ìƒì„±\n",
    "    for i, exp_num in enumerate(exp_num_list):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        \n",
    "        if n_rows > 1:\n",
    "            ax = axes[row, col]\n",
    "        else:\n",
    "            ax = axes[col] if n_cols > 1 else axes[0]\n",
    "        \n",
    "        # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°€ì ¸ì˜¤ê¸° (í˜„ì¬ ë°ì´í„° range_mmìœ¼ë¡œ ë¹„ì •ê·œí™”)\n",
    "        if exp_num in tf_predictions_dict:\n",
    "            tf_pred_values = denormalize_data(tf_predictions_dict[exp_num][:, feat_idx], feat_name)\n",
    "            fr_pred_values = denormalize_data(fr_predictions_dict[exp_num][:, feat_idx], feat_name)\n",
    "            tf_actual_values = denormalize_data(actual_dict[exp_num]['tf'][:, feat_idx], feat_name)\n",
    "            fr_actual_values = denormalize_data(actual_dict[exp_num]['fr'][:, feat_idx], feat_name)\n",
    "            tf_time = actual_dict[exp_num]['time_tf'][:len(tf_pred_values)]\n",
    "            fr_time = actual_dict[exp_num]['time_fr'][:len(fr_pred_values)]\n",
    "            \n",
    "            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "            # ì‹¤ì œê°’ (Free Runningì€ ì „ì²´, Teacher Forcingì€ ì‹œì  1ë¶€í„°)\n",
    "            ax.plot(fr_time, fr_actual_values, 'k-', linewidth=2, label='Actual', alpha=0.8)\n",
    "            \n",
    "            # Teacher Forcing ì˜ˆì¸¡ (ì‹œì  1ë¶€í„°)\n",
    "            ax.plot(tf_time, tf_pred_values, 'b--', linewidth=1.5, label='TF Predicted', alpha=0.7)\n",
    "            \n",
    "            # Free Running ì˜ˆì¸¡ (ì „ì²´ ì‹œí€€ìŠ¤)\n",
    "            ax.plot(fr_time, fr_pred_values, 'r:', linewidth=2, label='FR Predicted', alpha=0.8)\n",
    "            \n",
    "            # ì´ˆê¸°ê°’ í‘œì‹œ\n",
    "            if len(fr_actual_values) > 0:\n",
    "                ax.plot(fr_time[0], fr_actual_values[0], 'go', markersize=6, label='Initial')\n",
    "            \n",
    "            # ì˜ˆì¸¡ ì‹œì‘ì  í‘œì‹œ\n",
    "            if len(fr_time) > 1:\n",
    "                ax.axvline(x=fr_time[1], color='gray', linestyle=':', alpha=0.5)\n",
    "            \n",
    "            # ê·¸ë˜í”„ ì„¤ì •\n",
    "            ax.set_title(f'Exp {exp_num}', fontsize=11)\n",
    "            ax.set_xlabel('Time', fontsize=9)\n",
    "            ax.set_ylabel(feat_name, fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(fontsize=8)\n",
    "            \n",
    "            # MSE ê³„ì‚° ë° í‘œì‹œ (ë¹„ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ)\n",
    "            tf_mse = np.mean((tf_actual_values - tf_pred_values)**2) if len(tf_pred_values) > 0 else 0\n",
    "            fr_mse = np.mean((fr_actual_values - fr_pred_values)**2) if len(fr_pred_values) > 0 else 0\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ë°•ìŠ¤ë¡œ MSE í‘œì‹œ\n",
    "            textstr = f'TF: {tf_mse:.4f}\\nFR: {fr_mse:.4f}'\n",
    "            ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=8,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "            \n",
    "            # yì¶• ë²”ìœ„ ì„¤ì •\n",
    "            all_values = np.concatenate([tf_actual_values, fr_actual_values, tf_pred_values, fr_pred_values])\n",
    "            y_min, y_max = all_values.min(), all_values.max()\n",
    "            y_range = y_max - y_min\n",
    "            if y_range > 0:\n",
    "                ax.set_ylim(y_min - 0.1*y_range, y_max + 0.1*y_range)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'No data\\nExp {exp_num}', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    # ë¹ˆ subplot ì œê±°\n",
    "    for i in range(n_experiments, n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        if n_rows > 1:\n",
    "            axes[row, col].remove()\n",
    "        else:\n",
    "            if n_cols > 1:\n",
    "                axes[col].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸ‰ ëª¨ë“  featureì— ëŒ€í•œ Teacher Forcing vs Free Running ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"ğŸ“Š ê·¸ë˜í”„ ì„¤ëª…:\")\n",
    "print(\"   - ê²€ì€ìƒ‰ ì‹¤ì„ : ì‹¤ì œê°’ (Actual)\")\n",
    "print(\"   - íŒŒë€ìƒ‰ ì ì„ : Teacher Forcing ì˜ˆì¸¡ê°’\")\n",
    "print(\"   - ë¹¨ê°„ìƒ‰ ì ì„ : Free Running ì˜ˆì¸¡ê°’\")\n",
    "print(\"   - ì´ˆë¡ìƒ‰ ì : ì´ˆê¸°ê°’\")\n",
    "print(\"   - íšŒìƒ‰ ìˆ˜ì§ì„ : ì˜ˆì¸¡ ì‹œì‘ì \")\n",
    "print(\"   - ê° subplot: ê°œë³„ ì‹¤í—˜ ê²°ê³¼\")\n",
    "print(\"   - V, E ì œì™¸í•œ 8ê°œ feature ëª¨ë‘ í‘œì‹œ\")\n",
    "print(\"âœ… ìˆ˜ì • ì™„ë£Œ: í˜„ì¬ ë°ì´í„° ê¸°ì¤€ range_mm ì‚¬ìš©ìœ¼ë¡œ TF ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ yì¶• ìŠ¤ì¼€ì¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
