{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78f38071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module libraries\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc2a8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with layer normalization\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    def __init__(self, input_node, hidden_node):\n",
    "        super().__init__()\n",
    "        self.input_node = input_node\n",
    "        self.hidden_node = hidden_node\n",
    "\n",
    "        self.w_i = nn.Linear(input_node, 4*hidden_node, bias=False)\n",
    "        self.w_h = nn.Linear(hidden_node, 4*hidden_node, bias=False)\n",
    "\n",
    "        self.ln_i = nn.LayerNorm(hidden_node)\n",
    "        self.ln_f = nn.LayerNorm(hidden_node)\n",
    "        self.ln_w = nn.LayerNorm(hidden_node)\n",
    "        self.ln_o = nn.LayerNorm(hidden_node)\n",
    "        self.ln_c = nn.LayerNorm(hidden_node)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        gi = self.w_i(input)\n",
    "        gh = self.w_h(h_prev)\n",
    "        i_i, i_f, i_w, i_o = gi.chunk(4, dim=-1)\n",
    "        h_i, h_f, h_w, h_o = gh.chunk(4, dim=-1)\n",
    "\n",
    "        i_g = torch.sigmoid(self.ln_i(i_i + h_i))\n",
    "        f_g = torch.sigmoid(self.ln_f(i_f + h_f))\n",
    "        w_g = torch.tanh(self.ln_w(i_w + h_w))\n",
    "        o_g = torch.sigmoid(self.ln_o(i_o + h_o))\n",
    "        \n",
    "\n",
    "        c_new = f_g * c_prev + i_g * w_g\n",
    "        c_new = self.ln_c(c_new)\n",
    "\n",
    "        h_new = o_g * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0921e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State feature extractor using LayerNorm LSTM\n",
    "class StateExtr(nn.Module):\n",
    "    def __init__(self, input_node, hidden_node, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_node = hidden_node\n",
    "        self.n_layer = n_layer\n",
    "        self.input_node = input_node\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        self.lstm_cells.append(LayerNormLSTM(input_node, hidden_node))\n",
    "\n",
    "        for i in range(n_layer - 1):\n",
    "            self.lstm_cells.append(LayerNormLSTM(hidden_node, hidden_node))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.last_norm = nn.LayerNorm(hidden_node)\n",
    "        self.last_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        batch_size, max_len, input_node = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        \n",
    "        for i in range(self.n_layer):\n",
    "            h_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "            c_states.append(torch.zeros(batch_size, self.hidden_node, device=device))\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            layer_input = x_t # initialize layer input with input tensor\n",
    "            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n",
    "                h_new, c_new = lstm_cell(layer_input, (h_states[layer_idx], c_states[layer_idx]))\n",
    "                \n",
    "                h_states[layer_idx] = h_new\n",
    "                c_states[layer_idx] = c_new\n",
    "\n",
    "                if layer_idx < len(self.lstm_cells) - 1:\n",
    "                    layer_input = self.dropout(h_new)\n",
    "                else:\n",
    "                    layer_input = h_new\n",
    "\n",
    "                outputs.append(layer_input)\n",
    "\n",
    "        output_tensor = torch.stack(outputs, dim=1)\n",
    "        seq_len_cpu = seq_len.detach().cpu().long()\n",
    "        mask = torch.arange(max_len, device='cpu')[None, :] < seq_len_cpu[:, None]\n",
    "        mask = mask.float().to(device).unsqueeze(-1)\n",
    "\n",
    "        masked_output = output_tensor * mask\n",
    "        normed_output = self.last_norm(masked_output)\n",
    "        return self.last_dropout(normed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "843575d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical change decoder\n",
    "class PhysicalChangeDecoder(nn.Module):\n",
    "    def __init__(self, input_node, output_node, n_layer, hidden_node, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(input_node, hidden_node))\n",
    "        self.layers.append(nn.LayerNorm(hidden_node))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        for i in range(n_layer - 1):\n",
    "            self.layers.append(nn.Linear(hidden_node, hidden_node))\n",
    "            self.layers.append(nn.LayerNorm(hidden_node))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_node, output_node))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        x = hidden_states # x initialization\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) # update x with each layer\n",
    "        return x         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30255df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMED model\n",
    "class BMEDModel(nn.Module):\n",
    "    def __init__(self, state_extr_params, decoder_params, current_predictor_params, range_mm):\n",
    "        super().__init__()\n",
    "        self.state_extr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d406e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "## Normalize input data with the min-max normalization range of pre-trained model\n",
    "def normalize(inputs, range_mm):\n",
    "    features = ['V', 'E', 'VF', 'VA', 'VB', 'CFLA', 'CALA', 'CFK', 'CBK']\n",
    "    norm = []\n",
    "\n",
    "    for _, (name, value) in enumerate(zip(features, inputs)):\n",
    "        min_val = range_mm[name]['min']\n",
    "        max_val = range_mm[name]['max']\n",
    "        norm_val = (value - min_val) / (max_val - min_val)\n",
    "        norm.append(norm_val)\n",
    "    \n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8242d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model_path = 'BMED_FR_250910.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Model: {model_path}')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "simulator = BMEDModel(\n",
    "    state_extr_params = model['model_config']['state_extr_params'],\n",
    "    decoder_params = model['model_config']['decoder_params'],\n",
    "    current_predictor_params = model['model_config']['current_predictor_params'],\n",
    "    range_mm = model['model_config']['range_mm']\n",
    ").to(device)\n",
    "\n",
    "simulator.load_state_dict(model['model_state_dict'])\n",
    "simulator.eval()\n",
    "\n",
    "print('Load model parameters')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Simulation Conditions\n",
    "\n",
    "## cond_init = [V, E, VF, VA, VB, CFLA, CALA, CFK, CBK]\n",
    "### units = [V, mol/L, L, L, L, mol/L, mol/L, mol/L, mol/L]\n",
    "cond_init = [20, 0.25, 1, 1, 2, 1, 0, 2, 0]\n",
    "\n",
    "## cond_flow = [QF, QA, QB, CFLA, CFK]\n",
    "### units = [L/step, L/step, L/step, mol/L, mol/L]\n",
    "### 1 time step  = 0.25 hr\n",
    "QF, QA, QB = 10, 10, 10 # mL/min\n",
    "cond_flow = [QF*60/1000*0.25, QA*60/1000*0.25, QB*60/1000*0.25, 1, 2]\n",
    "\n",
    "## Overflooding volumes\n",
    "VOF = [1, 1, 2] # L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bbd90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_mm_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Continuous Simulation Running\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m## Normalize inputs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m norm_inputs = normalize(cond_init, \u001b[43mmodel_mm_norm\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_mm_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# Continuous Simulation Running\n",
    "\n",
    "## Normalize inputs\n",
    "norm_inputs = normalize(cond_init, model_mm_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4685f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15, 0.15, 0.15, 1, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
