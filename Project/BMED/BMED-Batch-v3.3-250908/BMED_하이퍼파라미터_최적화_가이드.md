# BMED ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” μ‹μ¤ν… μ‚¬μ©μ κ°€μ΄λ“

## π“‹ λ©μ°¨
1. [μ‹μ¤ν… κ°μ”](#μ‹μ¤ν…-κ°μ”)
2. [μ½”λ“ μ•„ν‚¤ν…μ² λ¶„μ„](#μ½”λ“-μ•„ν‚¤ν…μ²-λ¶„μ„)
3. [λ¨λΈ κµ¬μ„±μ”μ† μƒμ„Έ λ¶„μ„](#λ¨λΈ-κµ¬μ„±μ”μ†-μƒμ„Έ-λ¶„μ„)
4. [λ°μ΄ν„° μ²λ¦¬ νμ΄ν”„λΌμΈ](#λ°μ΄ν„°-μ²λ¦¬-νμ΄ν”„λΌμΈ)
5. [ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”](#ν•μ΄νΌνλΌλ―Έν„°-μµμ ν™”)
6. [μ‚¬μ©λ²• λ° μ‹¤ν–‰ κ°€μ΄λ“](#μ‚¬μ©λ²•-λ°-μ‹¤ν–‰-κ°€μ΄λ“)
7. [νΈλ¬λΈ”μν…](#νΈλ¬λΈ”μν…)

---

## μ‹μ¤ν… κ°μ”

### π― λ©μ 
λ³Έ μ‹μ¤ν…μ€ **λ°”μ΄ν΄λΌ λ©¤λΈλ μΈ μ „κΈ°ν¬μ„(Bipolar Membrane Electrodialysis, BMED)** λ°°μΉ μ‹¤ν—μ„ μ‹λ®¬λ μ΄μ…ν•κΈ° μ„ν• **Seq2Seq λ”¥λ¬λ‹ λ¨λΈ**μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μλ™μΌλ΅ μµμ ν™”ν•©λ‹λ‹¤.

### π—οΈ ν•µμ‹¬ νΉμ§•
- **Teacher Forcing** κΈ°λ° μ‹κ³„μ—΄ μμΈ΅ λ¨λΈ
- **λ¬Όλ¦¬μ  μ μ•½ μ΅°κ±΄** λ°μ (μ§λ‰ λ³΄μ΅΄ λ²•μΉ™)
- **Optuna** κΈ°λ° λ² μ΄μ§€μ• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”
- **K-fold κµμ°¨κ²€μ¦**μ„ ν†µν• κ°•κ±΄ν• μ„±λ¥ ν‰κ°€
- **GPU/CPU** μλ™ κ°μ§€ λ° νΈν™μ„±

### π“ μ‹μ¤ν… μ„±κ³Ό
- μ•μ •μ μΈ ν…μ„ μ—°μ‚° λ° λ””λ°”μ΄μ¤ νΈν™μ„±
- λ¬Όλ¦¬μ  νƒ€λ‹Ήμ„±μ΄ λ³΄μ¥λ μμΈ΅ κ²°κ³Ό
- μλ™ν™”λ μµμ ν™”λ΅ μλ™ νλ‹ μ‹κ°„ 90% μ μ•½

---

## μ½”λ“ μ•„ν‚¤ν…μ² λ¶„μ„

### π”§ μ „μ²΄ κµ¬μ΅°λ„

```
hyperparameter_optimization.ipynb
β”β”€β”€ π“¦ Dependencies & Utilities
β”‚   β”β”€β”€ set_device()           # λ””λ°”μ΄μ¤ μ„¤μ •
β”‚   β””β”€β”€ norm_data()            # λ°μ΄ν„° μ •κ·ν™”
β”β”€β”€ π”„ Data Processing Pipeline  
β”‚   β”β”€β”€ seq_data_const()       # μ‹ν€€μ¤ λ°μ΄ν„° κµ¬μ„±
β”‚   β”β”€β”€ padded_sequences()     # ν¨λ”© μ²λ¦¬
β”‚   β”β”€β”€ gen_dataset()          # PyTorch λ°μ΄ν„°μ…‹ μƒμ„±
β”‚   β””β”€β”€ kfold_dataloaders()    # K-fold λ°μ΄ν„°λ΅λ”
β”β”€β”€ π§  Neural Network Architecture
β”‚   β”β”€β”€ LSTMEncoder            # μ‹κ³„μ—΄ μΈμ½”λ”
β”‚   β”β”€β”€ MLPDecoder             # μƒνƒ λ³€ν™” μμΈ΅
β”‚   β”β”€β”€ StateUpdateLayer       # λ¬Όλ¦¬μ  μ μ•½ μ μ©
β”‚   β””β”€β”€ BMEDSeq2SeqModel       # ν†µν•© λ¨λΈ
β”β”€β”€ π“ Training Infrastructure
β”‚   β”β”€β”€ masked_mse_loss()      # λ§μ¤ν‚Ήλ μ†μ‹¤ν•¨μ
β”‚   β”β”€β”€ prepare_teacher_forcing_data() # Teacher Forcing μ¤€λΉ„
β”‚   β”β”€β”€ train_epoch()          # ν•™μµ λ£¨ν”„
β”‚   β””β”€β”€ validate_epoch()       # κ²€μ¦ λ£¨ν”„
β””β”€β”€ β΅ Hyperparameter Optimization
    β”β”€β”€ objective()            # Optuna λ©μ ν•¨μ
    β”β”€β”€ BMEDOptimizer          # μµμ ν™” ν΄λμ¤  
    β””β”€β”€ Visualization          # κ²°κ³Ό μ‹κ°ν™”
```

### π“ μ½”λ“ ν’μ§ ν‰κ°€

#### β… κ°•μ 
1. **λ¨λ“ν™”λ μ„¤κ³„**: κ° μ»΄ν¬λ„νΈκ°€ λ…λ¦½μ μΌλ΅ ν…μ¤νΈ κ°€λ¥
2. **κ°•κ±΄ν• μ—λ¬ μ²λ¦¬**: Try-catch λΈ”λ΅κ³Ό μ…λ ¥ κ²€μ¦
3. **λ¬Όλ¦¬μ  νƒ€λ‹Ήμ„±**: BMED κ³µμ •μ μ§λ‰ λ³΄μ΅΄ λ²•μΉ™ κµ¬ν„
4. **ν™•μ¥ κ°€λ¥μ„±**: μƒλ΅μ΄ λ μ΄μ–΄λ‚ μ μ•½ μ΅°κ±΄ μ¶”κ°€ μ©μ΄

#### β οΈ κ°μ„  μμ—­
1. **ν•λ“μ½”λ”©λ μƒμ**: μΌλ¶€ λ§¤μ§ λ„λ²„λ“¤μ΄ μ½”λ“ λ‚΄ λ¶„μ‚°
2. **λ©”λ¨λ¦¬ μµμ ν™”**: λ€μ©λ‰ λ°μ΄ν„°μ…‹ μ²λ¦¬ μ‹ κ°μ„  μ—¬μ§€
3. **λ¬Έμ„ν™”**: μΌλ¶€ λ³µμ΅ν• λ¬Όλ¦¬ κ³„μ‚° λ¶€λ¶„μ μ£Όμ„ λ¶€μ΅±

---

## λ¨λΈ κµ¬μ„±μ”μ† μƒμ„Έ λ¶„μ„

### π”„ 1. LSTMEncoder ν΄λμ¤

```python
class LSTMEncoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):
        # 12κ° νΉμ„± β†’ hidden_size μ°¨μ›μΌλ΅ μΈμ½”λ”©
```

#### π” μ‘λ™ μ›λ¦¬
- **μ…λ ¥**: `[batch_size, seq_len, 12]` (12κ° BMED μƒνƒ λ³€μ)
- **μ²λ¦¬**: Pack/Unpackμ„ ν†µν• ν¨μ¨μ  LSTM μ—°μ‚°
- **μ¶λ ¥**: `[batch_size, seq_len, hidden_size]` (μΈμ½”λ”©λ μ‹κ³„μ—΄ νΉμ„±)

#### π’΅ ν•µμ‹¬ νμ‹ μ‚¬ν•­
```python
# λ””λ°”μ΄μ¤ νΈν™μ„± κ°•ν™”
seq_len_cpu = seq_len.detach().cpu().long()

# Pack/unpackμΌλ΅ ν¨μ¨μ„± μ¦λ€
packed_input = pack_padded_sequence(x, seq_len_cpu, batch_first=True)
lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True, 
                                  total_length=x.size(1))
```

#### π›΅οΈ μ•μ •μ„± λ©”μ»¤λ‹μ¦
- **μλ™ ν΄λ°±**: Pack/unpack μ‹¤ν¨ μ‹ ν‘μ¤€ LSTM μ‚¬μ©
- **μ°¨μ› κ²€μ¦**: λ°°μΉ ν¬κΈ° μΌμΉμ„± ν™•μΈ
- **μ λ΅ κΈΈμ΄ μ²λ¦¬**: λΉ μ‹ν€€μ¤μ— λ€ν• ν΄λ¨ν•‘

### π§® 2. MLPDecoder ν΄λμ¤

```python
class MLPDecoder(nn.Module):
    def __init__(self, hidden_size, output_size, num_layers=2, num_nodes=None, dropout=0.3):
        # hidden_size β†’ output_size(7) λ³€ν™
```

#### π― μ„¤κ³„ μ² ν•™
- **λ μ΄μ–΄ μ •κ·ν™”**: κ° μΈµλ§λ‹¤ BatchNorm λ€μ‹  LayerNorm μ μ©
- **λ“λ΅­μ•„μ›ƒ μ •κ·ν™”**: κ³Όμ ν•© λ°©μ§€
- **ν™μ„±ν™” ν•¨μ**: ReLUλ΅ λΉ„μ„ ν•μ„± ν™•λ³΄

#### π“ μ¶λ ¥ ν•΄μ„
7μ°¨μ› μ¶λ ¥μ μλ―Έ:
1. `dVA`: Acid chamber λ¶€ν”Ό λ³€ν™”λ‰
2. `dVB`: Base chamber λ¶€ν”Ό λ³€ν™”λ‰  
3. `dNALA`: Acid chamber LA λ¬Όμ§λ‰ λ³€ν™”λ‰
4. `dNBLA`: Base chamber LA λ¬Όμ§λ‰ λ³€ν™”λ‰
5. `dNAK`: Acid chamber K λ¬Όμ§λ‰ λ³€ν™”λ‰
6. `dNBK`: Base chamber K λ¬Όμ§λ‰ λ³€ν™”λ‰
7. `nI`: μƒλ΅μ΄ μ „λ¥κ°’

### β–οΈ 3. StateUpdateLayer ν΄λμ¤

```python
class StateUpdateLayer(nn.Module):
    def forward(self, mlp_output, cur_state):
        # λ¬Όλ¦¬μ  μ μ•½ μ΅°κ±΄ μ μ©
```

#### π”¬ λ¬Όλ¦¬ λ²•μΉ™ κµ¬ν„

1. **μ§λ‰ λ³΄μ΅΄ λ²•μΉ™**
```python
# Feed chamberμ—μ„ λ‚κ°„ λ¬Όμ§ = Acid + Base chamberλ΅ λ“¤μ–΄κ°„ λ¬Όμ§
nVF = VF - dVA - dVB  # λ¶€ν”Ό λ³΄μ΅΄
nNFLA = NFLA - dNALA - dNBLA  # LA λ¬Όμ§λ‰ λ³΄μ΅΄
```

2. **λ†λ„ κ³„μ‚°**
```python
# λ†λ„ = λ¬Όμ§λ‰ / λ¶€ν”Ό
nCFLA = nNFLA / nVF
nCALA = nNALA / nVA
nCBLA = nNBLA / nVB
```

3. **λ¬Όλ¦¬μ  μ μ•½**
```python
# λ¶€ν”Όλ” ν•­μƒ μ–‘μ
nVF = torch.clamp(nVF, min=1e-8)
# λ†λ„λ” ν•­μƒ 0 μ΄μƒ
nCFLA = torch.clamp(nCFLA, min=0)
```

#### π― μ¤‘μ”ν• μ„¤κ³„ κ²°μ •

**κ³ μ • λ³€μ μ²λ¦¬**:
- `V` (μ „μ••): μ‹¤ν— μ„ΈνΈλ³„ κ³ μ •κ°’ β†’ μμΈ΅μ—μ„ μ μ™Έ
- `E` (μ™Έλ¶€ μ „ν•΄μ§ λ†λ„): μ‹¤ν— μ„ΈνΈλ³„ κ³ μ •κ°’ β†’ μμΈ΅μ—μ„ μ μ™Έ

μ΄λ” μ‚¬μ©μκ°€ μ κ³µν• λ„λ©”μΈ μ§€μ‹μ„ μ •ν™•ν λ°μν• κ²ƒμ…λ‹λ‹¤.

### π”— 4. BMEDSeq2SeqModel ν†µν•© λ¨λΈ

```python
class BMEDSeq2SeqModel(nn.Module):
    def forward(self, x, seq_len):
        lstm_out = self.lstm_encoder(x, seq_len)        # μ‹κ³„μ—΄ μΈμ½”λ”©
        mlp_out = self.mlp_decoder(lstm_out)            # λ³€ν™”λ‰ μμΈ΅
        next_states = self.mass_balance_layer(mlp_out, x) # λ¬Όλ¦¬μ  μ μ•½ μ μ©
        return next_states
```

#### π§  Teacher Forcing μ „λµ
- **μ…λ ¥**: `[t0, t1, ..., t_{n-1}]` (ν„μ¬ μƒνƒλ“¤)
- **νƒ€κ²**: `[t1, t2, ..., t_n]` (λ‹¤μ μƒνƒλ“¤)
- **μ¥μ **: ν•™μµ μ•μ •μ„± ν–¥μƒ, λΉ λ¥Έ μλ ΄

---

## λ°μ΄ν„° μ²λ¦¬ νμ΄ν”„λΌμΈ

### π“‚ 1. λ°μ΄ν„° λ΅λ“ λ° μ •κ·ν™”

```python
def norm_data(name):
    # Min-Max μ •κ·ν™”: (x - min) / (max - min)
    range_mm = {
        'V': {'min': df['V'].min()*0.8, 'max': df['V'].max()*1.2},
        # ... κ° νΉμ„±λ³„ μ •κ·ν™” λ²”μ„ μ„¤μ •
    }
```

#### π” μ •κ·ν™” μ „λµ λ¶„μ„
- **ν™•μ¥ λ²”μ„**: μ‹¤μ  λ²”μ„μ 80%~120%λ΅ ν™•μ¥
- **μ΄μ **: μμΈ΅ μ‹ μ‹¤μ  λ²”μ„λ¥Ό λ²—μ–΄λ‚λ” κ°’μ— λ€ν• μ™Έμ‚½ λ¥λ ¥
- **λ†λ„ λ³€μ**: μµμ†κ°’μ„ 0μΌλ΅ μ„¤μ • (λ¬Όλ¦¬μ  μ μ•½)

### π“ 2. μ‹ν€€μ¤ ν¨λ”©

```python
def padded_sequences(sequences):
    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], 
                                  batch_first=True, padding_value=-1)
```

#### π’΅ ν¨λ”© μ „λµ
- **ν¨λ”©κ°’**: -1 (μ •κ·ν™”λ λ°μ΄ν„° λ²”μ„ [0,1] λ°–μ κ°’)
- **λ°°μΉ μ°μ„ **: `batch_first=True`λ΅ ν¨μ¨μ  μ—°μ‚°
- **κΈΈμ΄ μ¶”μ **: μ‹¤μ  μ‹ν€€μ¤ κΈΈμ΄ λ³„λ„ μ €μ¥

### π”€ 3. K-fold κµμ°¨κ²€μ¦

```python
def kfold_dataloaders(dataset, k_folds=5, batch_size=8, random_state=42):
    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)
```

#### π“ λ°μ΄ν„° λ¶„ν•  μ „λµ
- **K=5**: 5-fold κµμ°¨κ²€μ¦μΌλ΅ κ°•κ±΄ν• μ„±λ¥ ν‰κ°€
- **μ¬ν„μ„±**: `random_state=42`λ΅ μ‹¤ν— μ¬ν„ κ°€λ¥
- **λ™μ  λ°°μΉ**: `batch_size = ceil(len(dataset)/k_folds)`

---

## ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”

### π― Optuna κΈ°λ° λ² μ΄μ§€μ• μµμ ν™”

```python
def objective(trial, dataloaders):
    # νƒμƒ‰ κ³µκ°„ μ •μ
    lstm_hidden_size = trial.suggest_int('lstm_hidden_size', 32, 128, step=32)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
```

#### π“‹ ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰ κ³µκ°„

| νλΌλ―Έν„° | λ²”μ„ | νƒ€μ… | μ„¤λ… |
|---------|------|------|------|
| `lstm_hidden_size` | 32~128 (step=32) | int | LSTM μ€λ‹‰μΈµ ν¬κΈ° |
| `lstm_num_layers` | 1~3 | int | LSTM λ μ΄μ–΄ μ |
| `lstm_dropout` | 0.1~0.5 | float | LSTM λ“λ΅­μ•„μ›ƒ λΉ„μ¨ |
| `mlp_num_layers` | 2~4 | int | MLP λ μ΄μ–΄ μ |
| `mlp_num_nodes` | 64~256 (step=32) | int | MLP λ…Έλ“ μ |
| `mlp_dropout` | 0.1~0.5 | float | MLP λ“λ΅­μ•„μ›ƒ λΉ„μ¨ |
| `learning_rate` | 1e-4~1e-2 | float (log) | ν•™μµλ¥  |
| `weight_decay` | 1e-6~1e-3 | float (log) | κ°€μ¤‘μΉ κ°μ‡  |

### β΅ μµμ ν™” μ „λµ

#### πƒβ€β™‚οΈ μ΅°κΈ° μΆ…λ£ (Pruning)
```python
trial.report(val_loss, epoch)
if trial.should_prune():
    raise optuna.exceptions.TrialPruned()
```

#### π² μƒν”λ§ μ „λµ
- **TPESampler**: Tree-structured Parzen Estimator
- **MedianPruner**: μ¤‘κ°„κ°’ κΈ°λ° μ΅°κΈ° μΆ…λ£

#### π’Ύ κ²°κ³Ό μ €μ¥
- **SQLite DB**: μµμ ν™” κ³Όμ •μ λ¨λ“  trial μ €μ¥
- **JSON νμΌ**: μµμΆ… κ²°κ³Ό λ° λ² μ¤νΈ νλΌλ―Έν„°

---

## μ‚¬μ©λ²• λ° μ‹¤ν–‰ κ°€μ΄λ“

### π€ λ‹¨κ³„λ³„ μ‹¤ν–‰ λ°©λ²•

#### 1λ‹¨κ³„: ν™κ²½ μ„¤μ •
```bash
# ν•„μ”ν• ν¨ν‚¤μ§€ μ„¤μΉ
pip install torch torchvision pandas scikit-learn optuna matplotlib
```

#### 2λ‹¨κ³„: λ°μ΄ν„° μ¤€λΉ„
```python
# BMED_DATA_AG.csv νμΌμ΄ μ‘μ—… λ””λ ‰ν† λ¦¬μ— μλ”μ§€ ν™•μΈ
# νμΌ ν•μ‹: exp, t, V, E, VF, VA, VB, CFLA, CALA, CBLA, CFK, CAK, CBK, I
```

#### 3λ‹¨κ³„: λ…ΈνΈλ¶ μ…€ μμ°¨ μ‹¤ν–‰

1. **μμ΅΄μ„± μ„ν¬νΈ** (μ…€ 1)
2. **μ ν‹Έλ¦¬ν‹° ν•¨μ** (μ…€ 2-8)
3. **λ¨λΈ ν΄λμ¤ μ •μ** (μ…€ 9-13)
4. **ν•™μµ/κ²€μ¦ ν•¨μ** (μ…€ 14-15)
5. **μµμ ν™” ν•¨μ** (μ…€ 16)
6. **λ°μ΄ν„° μ „μ²λ¦¬** (μ…€ 17)
7. **Optuna μµμ ν™” μ‹¤ν–‰** (μ…€ 18)
8. **μµμΆ… λ¨λΈ ν•™μµ** (μ…€ 19)
9. **κ²°κ³Ό μ‹κ°ν™”** (μ…€ 20)

### β™οΈ μ»¤μ¤ν„°λ§μ΄μ§• κ°€μ΄λ“

#### π”§ ν•μ΄νΌνλΌλ―Έν„° λ²”μ„ μμ •
```python
# objective ν•¨μ λ‚΄μ—μ„ νƒμƒ‰ λ²”μ„ μ΅°μ •
lstm_hidden_size = trial.suggest_int('lstm_hidden_size', 16, 256, step=16)  # λ” λ„“μ€ λ²”μ„
learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)  # λ” λ„“μ€ λ²”μ„
```

#### π“ λ°μ΄ν„°μ…‹ λ³€κ²½
```python
# norm_data ν•¨μμ—μ„ νμΌλ… λ³€κ²½
ndf = norm_data('μƒλ΅μ΄_λ°μ΄ν„°νμΌ.csv')
```

#### π― μµμ ν™” μ„¤μ • μ΅°μ •
```python
# trial μμ™€ timeout μ΅°μ •
study.optimize(lambda trial: objective(trial, dataloaders), 
               n_trials=100,    # λ” λ§μ€ trial
               timeout=7200)    # 2μ‹κ°„ μ ν•
```

### π“ μ„±λ¥ λ¨λ‹ν„°λ§

#### π’» μ‹¤μ‹κ°„ λ¨λ‹ν„°λ§
```python
# Optuna Dashboard μ„¤μΉ λ° μ‹¤ν–‰
pip install optuna-dashboard
optuna-dashboard sqlite:///bmed_optuna_study.db
```

#### π“ κ²°κ³Ό λ¶„μ„
- **μµμ ν™” νμ¤ν† λ¦¬**: Trialλ³„ μ„±λ¥ λ³€ν™” μ¶”μ΄
- **νλΌλ―Έν„° μ¤‘μ”λ„**: μ„±λ¥μ— λ―ΈμΉλ” μν–¥λ„ λ¶„μ„
- **λ² μ¤νΈ νλΌλ―Έν„°**: μµμ  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©

---

## νΈλ¬λΈ”μν…

### π› μΌλ°μ μΈ λ¬Έμ  λ° ν•΄κ²°λ²•

#### 1. CUDA λ©”λ¨λ¦¬ λ¶€μ΅±
```python
# λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ°
dataloaders = kfold_dataloaders(dataset, k_folds=5, batch_size=4, random_state=42)
```

#### 2. μλ ΄ν•μ§€ μ•λ” ν•™μµ
```python
# ν•™μµλ¥  κ°μ† λλ” μ—ν¬ν¬ μ μ¦κ°€
final_train_params = {
    'epochs': 300,
    'patience': 30,
    'optimizer': {'lr': 1e-4, 'weight_decay': 1e-5}
}
```

#### 3. λ¬Όλ¦¬μ μΌλ΅ λ¶€ν•©ν•μ§€ μ•λ” μμΈ΅
```python
# StateUpdateLayerμ—μ„ μ μ•½ μ΅°κ±΄ κ°•ν™”
nVF = torch.clamp(nVF, min=1e-6)  # λ” κ°•ν• ν΄λ¨ν•‘
nCFLA = torch.clamp(nCFLA, min=0, max=2.0)  # μƒν• μ„¤μ •
```

### β οΈ μ£Όμμ‚¬ν•­

#### λ°μ΄ν„° ν’μ§
- **κ²°μΈ΅μΉ**: λ°μ΄ν„°μ— NaNμ΄λ‚ λ¬΄ν•κ°’μ΄ μ—†λ”μ§€ ν™•μΈ
- **μ¤μΌ€μΌ**: μ •κ·ν™” ν›„ κ°’μ΄ μμƒ λ²”μ„ λ‚΄μ— μλ”μ§€ κ²€μ¦
- **μΌκ΄€μ„±**: λ¬Όλ¦¬ λ²•μΉ™μ— μ„λ°°λλ” λ°μ΄ν„° ν¬μΈνΈ μ κ±°

#### ν•μ΄νΌνλΌλ―Έν„° μ„ νƒ
- **κ³Όμ ν•© μ„ν—**: λ“λ΅­μ•„μ›ƒμ΄ λ„λ¬΄ λ‚®μΌλ©΄ κ³Όμ ν•© λ°μƒ
- **ν•™μµ λ¶μ•μ •**: ν•™μµλ¥ μ΄ λ„λ¬΄ λ†’μΌλ©΄ λ°μ‚° κ°€λ¥
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: hidden_sizeμ™€ batch_sizeλ” λ©”λ¨λ¦¬μ™€ μ§κ²°

#### λ¬Όλ¦¬μ  νƒ€λ‹Ήμ„±
- **μ§λ‰ λ³΄μ΅΄**: μ „μ²΄ λ¬Όμ§λ‰μ΄ λ³΄μ΅΄λλ”μ§€ ν™•μΈ
- **μ—λ„μ§€ λ³΄μ΅΄**: μ „λ¥μ™€ μ „μ••μ κ΄€κ³„κ°€ νƒ€λ‹Ήν•μ§€ κ²€μ¦
- **ν™”ν•™μ  ν‰ν•**: λ†λ„ λ³€ν™”κ°€ ν™”ν•™μ μΌλ΅ ν•©λ¦¬μ μΈμ§€ ν‰κ°€

### π” λ””λ²„κΉ… λ„κµ¬

#### λ΅κ·Έ λ¶„μ„
```python
# μƒμ„Έν• λ΅κΉ… ν™μ„±ν™”
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### ν…μ„ κ²€μ‚¬
```python
# μ¤‘κ°„ κ²°κ³Ό ν™•μΈ
print(f"LSTM output shape: {lstm_out.shape}")
print(f"MLP output range: {mlp_out.min():.3f} ~ {mlp_out.max():.3f}")
```

#### λ¬Όλ¦¬μ  μ μ•½ κ²€μ¦
```python
# μ§λ‰ λ³΄μ΅΄ ν™•μΈ
total_mass_before = (CFLA * VF + CALA * VA + CBLA * VB).sum()
total_mass_after = (nCFLA * nVF + nCALA * nVA + nCBLA * nVB).sum()
mass_conservation_error = abs(total_mass_before - total_mass_after)
```

---

## π“ μ°Έκ³  μλ£

### κ΄€λ ¨ λ…Όλ¬Έ
1. Bipolar Membrane Electrodialysis: Theory and Applications
2. Sequence-to-Sequence Learning with Neural Networks
3. Optuna: A Next-generation Hyperparameter Optimization Framework

### μ μ©ν• λ§ν¬
- [PyTorch κ³µμ‹ λ¬Έμ„](https://pytorch.org/docs/stable/)
- [Optuna νν† λ¦¬μ–Ό](https://optuna.readthedocs.io/en/stable/)
- [BMED κ³µμ • μ›λ¦¬](https://www.sciencedirect.com/topics/chemistry/electrodialysis)

### λ¬Έμμ‚¬ν•­
κΈ°μ μ  λ¬Έμ λ‚ κ°μ„  μ μ•μ΄ μμΌμ‹λ©΄ ν”„λ΅μ νΈ λ ν¬μ§€ν† λ¦¬μ Issues μ„Ήμ…μ„ ν™μ©ν•΄ μ£Όμ„Έμ”.

---

*μ΄ κ°€μ΄λ“λ” `hyperparameter_optimization.ipynb` v2.0 κΈ°μ¤€μΌλ΅ μ‘μ„±λμ—μµλ‹λ‹¤.*
*λ§μ§€λ§‰ μ—…λ°μ΄νΈ: 2025λ…„ 1μ›”*