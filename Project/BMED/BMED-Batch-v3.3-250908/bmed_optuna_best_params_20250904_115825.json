{
  "lstm_hidden_size": 32,
  "lstm_n_layers": 3,
  "lstm_dropout": 0.4,
  "decoder_hidden_size": 96,
  "decoder_n_layers": 4,
  "decoder_dropout": 0.2,
  "current_hidden_size": 48,
  "current_n_layers": 3,
  "current_dropout": 0.1,
  "noam_factor": 0.6,
  "warmup_ratio": 0.25,
  "batch_size": 8
}